{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold_testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFold_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1r2dn6P2uq"
      },
      "source": [
        "### <center> <b> <font color='black'>  AlphaFold development suite</font></b> </center>\n",
        "\n",
        "<p><font color='green'> Instructions</p>\n",
        "\n",
        "<p>A. SETUP:  Run cells 1-3 to set up or hit <b><i>Runtime/Run all</i></b> to run everything (5 min.)</p>\n",
        "\n",
        "<p>B. DEVELOPMENT CYCLE:  </p>\n",
        "<li> 1a. Either edit files in the alphafold directory, or </li>\n",
        "\n",
        " <li> 1b. Edit files in github and hit \"Load current alphafold_working from github\"</li>\n",
        " <li> 2. Run cell B below to run Alphafold (2 min.)</li>\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "#@markdown 1. Set up imports and load dependencies...this is the slow step\n",
        "import os, sys\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "! echo \"Installing biopython ...\"\n",
        "!  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "\n",
        "! echo \"Downloading model parameters...\"\n",
        "!    rm -rf params\n",
        "!    mkdir params\n",
        "!    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar  | tar x -C params\n",
        "\n",
        "!echo \"Downloading jq curl zlib1g gawk...\"\n",
        "!    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "!    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Setting up conda...\"\n",
        "!    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "!    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "! echo \"Setting up template search methods...\"\n",
        "\n",
        "! conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Installing openmm...\"\n",
        "! conda install -qy -c omnia openmm 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Installing Mock...\"\n",
        "!  pip install mock\n",
        " \n",
        "! echo \"Setting paths to site-packages and dist-packages...\"\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages\")\n",
        "sys.path.append(\"/usr/local/lib/python3.7/dist-packages\")\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages/simtk/\")\n",
        "\n",
        "target_all_atom_positions = None # Initialize\n",
        "\n",
        "! echo \"Done with loading dependencies\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMuoZ9Kl7BoJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown 2. Load current alphafold_working from phenix-project github.  \n",
        "#@markdown  This is quick...\n",
        "#@markdown cycle back here after making a change in github alphafold_working\n",
        "! echo \"installing alphafold from https://github.com/phenix-project/af_development.git in /contents/alphafold\"\n",
        "\n",
        "! rm -rf af_development\n",
        "! rm -rf alphafold\n",
        "! rm -f colabfold.py\n",
        "\n",
        "\n",
        "! git clone https://github.com/phenix-project/af_development.git --quiet\n",
        "! (cd af_development; git checkout --quiet)\n",
        "! mv af_development/alphafold alphafold\n",
        "! mv af_development/colabfold.py .\n",
        "! mv af_development/run_alphafold.py .\n",
        "! mv alphafold/run_alphafold_test.py .\n",
        "\n",
        "\n",
        "\n",
        "!    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "!    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "!    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "\n",
        "\n",
        "! echo \"Ready with alphafold in /content/alphafold\"\n",
        "! ls -ltr /content/alphafold\n",
        "\n",
        "! echo \"clearing out /tmp/absl_testing/\"\n",
        "! rm -rf /tmp/absl_testing/\n",
        "! mkdir /tmp/absl_testing/\n",
        "\n",
        "! echo \" Clearing python caches ...\"\n",
        "for x in list(sys.modules.keys(  )) + list(globals()):\n",
        "  for key in ['alphafold','protein', 'Alphafold', 'Protein', 'colabfold']:\n",
        "    if x.find(key)>-1:\n",
        "      if x in list(sys.modules.keys()):\n",
        "        \n",
        "        del(sys.modules[x])\n",
        "      if x in list(globals().keys()):\n",
        "      \n",
        "        del globals()[x]\n",
        "        assert not x in list(globals().keys())\n",
        "        break\n",
        "\n",
        "\n",
        "if not os.environ['PYTHONPATH'].find(\":/opt/conda/bin\")>-1:\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/bin\"\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/lib/python3.7/site-packages\"\n",
        "  os.environ['PYTHONPATH']+=\":/usr/local/lib/python3.7/dist-packages\"\n",
        "! echo \"`grep Version run_alphafold_test.py|grep -v Apache`\"\n",
        "! echo \"Done loading current version.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlUDwt0lM0oc",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3a. Run ProteinTest (optional, local code)\n",
        "\n",
        "TEST_DATA_DIR = 'alphafold/common/testdata/'\n",
        "\n",
        "from absl.testing import parameterized\n",
        "from absl.testing import absltest\n",
        "from alphafold.common import protein\n",
        "import numpy as np\n",
        "class ProteinTest(parameterized.TestCase):\n",
        "\n",
        "  def _check_shapes(self, prot, num_res):\n",
        "    \"\"\"Check that the processed shapes are correct.\"\"\"\n",
        "    num_atoms = residue_constants.atom_type_num\n",
        "    self.assertEqual((num_res, num_atoms, 3), prot.atom_positions.shape)\n",
        "    self.assertEqual((num_res,), prot.aatype.shape)\n",
        "    self.assertEqual((num_res, num_atoms), prot.atom_mask.shape)\n",
        "    self.assertEqual((num_res,), prot.residue_index.shape)\n",
        "    self.assertEqual((num_res, num_atoms), prot.b_factors.shape)\n",
        "    print(\"Finished _check_shapes\")\n",
        "\n",
        "  @parameterized.parameters(('2rbg.pdb', 'A', 282),\n",
        "                            ('2rbg.pdb', 'B', 282))\n",
        "  def test_from_pdb_str(self, pdb_file, chain_id, num_res):\n",
        "    pdb_file = os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                            pdb_file)\n",
        "    with open(pdb_file) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id)\n",
        "    self._check_shapes(prot, num_res)\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "    self.assertGreaterEqual(prot.aatype.min(), 0)\n",
        "    # Allow equal since unknown restypes have index equal to restype_num.\n",
        "    self.assertLessEqual(prot.aatype.max(), residue_constants.restype_num)\n",
        "    print(\"Finished test_from_pdb_str\")\n",
        "\n",
        "  def test_to_pdb(self):\n",
        "    with open(\n",
        "        os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                     '2rbg.pdb')) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id='A')\n",
        "    pdb_string_reconstr = protein.to_pdb(prot)\n",
        "    prot_reconstr = protein.from_pdb_string(pdb_string_reconstr)\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "\n",
        "    np.testing.assert_array_equal(prot_reconstr.aatype, prot.aatype)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.atom_positions, prot.atom_positions)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.atom_mask, prot.atom_mask)\n",
        "    np.testing.assert_array_equal(\n",
        "        prot_reconstr.residue_index, prot.residue_index)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.b_factors, prot.b_factors)\n",
        "    print(\"Finished test_to_pdb\")\n",
        "\n",
        "  def test_ideal_atom_mask(self):\n",
        "    with open(\n",
        "        os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                     '2rbg.pdb')) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id='A')\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "    \n",
        "    ideal_mask = protein.ideal_atom_mask(prot)\n",
        "    non_ideal_residues = set([102] + list(range(127, 285)))\n",
        "    for i, (res, atom_mask) in enumerate(\n",
        "        zip(prot.residue_index, prot.atom_mask)):\n",
        "      if res in non_ideal_residues:\n",
        "        self.assertFalse(np.all(atom_mask == ideal_mask[i]), msg=f'{res}')\n",
        "      else:\n",
        "        self.assertTrue(np.all(atom_mask == ideal_mask[i]), msg=f'{res}')\n",
        "    print(\"Finished test_ideal_atom_mask\")\n",
        "    \n",
        "t = ProteinTest()\n",
        "\n",
        "t.test_to_pdb()\n",
        "t.test_ideal_atom_mask()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXfXFVK0L3_Q",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3b. load target as <b><i>target_all_atom_positions</i></b> (optional)\n",
        "text = \"\"\"ATOM      1  N   ALA A   1      -1.321 -17.567  -6.262  1.00  0.00           N  \n",
        "ATOM      2  CA  ALA A   1       0.048 -17.367  -5.793  1.00  0.00           C  \n",
        "ATOM      3  C   ALA A   1       0.898 -16.681  -6.858  1.00  0.00           C  \n",
        "ATOM      4  CB  ALA A   1       0.674 -18.701  -5.394  1.00  0.00           C  \n",
        "ATOM      5  O   ALA A   1       1.124 -17.238  -7.935  1.00  0.00           O  \n",
        "ATOM      6  N   SER A   2       0.450 -15.417  -7.370  1.00  0.00           N  \n",
        "ATOM      7  CA  SER A   2       0.999 -14.065  -7.391  1.00  0.00           C  \n",
        "ATOM      8  C   SER A   2       0.761 -13.392  -8.738  1.00  0.00           C  \n",
        "ATOM      9  CB  SER A   2       2.496 -14.088  -7.081  1.00  0.00           C  \n",
        "ATOM     10  O   SER A   2       1.301 -13.824  -9.758  1.00  0.00           O  \n",
        "ATOM     11  OG  SER A   2       3.251 -14.281  -8.264  1.00  0.00           O  \n",
        "ATOM     12  N   ASP A   3      -0.454 -12.939  -9.046  1.00  0.00           N  \n",
        "ATOM     13  CA  ASP A   3      -0.896 -11.874  -9.940  1.00  0.00           C  \n",
        "ATOM     14  C   ASP A   3      -0.189 -10.559  -9.620  1.00  0.00           C  \n",
        "ATOM     15  CB  ASP A   3      -2.412 -11.692  -9.851  1.00  0.00           C  \n",
        "ATOM     16  O   ASP A   3      -0.715  -9.732  -8.872  1.00  0.00           O  \n",
        "ATOM     17  CG  ASP A   3      -3.075 -11.557 -11.210  1.00  0.00           C  \n",
        "ATOM     18  OD1 ASP A   3      -2.735 -10.619 -11.964  1.00  0.00           O  \n",
        "ATOM     19  OD2 ASP A   3      -3.946 -12.394 -11.530  1.00  0.00           O  \n",
        "ATOM     20  N   PHE A   4       1.208 -10.569  -9.700  1.00  0.00           N  \n",
        "ATOM     21  CA  PHE A   4       2.002  -9.358  -9.529  1.00  0.00           C  \n",
        "ATOM     22  C   PHE A   4       1.723  -8.364 -10.650  1.00  0.00           C  \n",
        "ATOM     23  CB  PHE A   4       3.496  -9.694  -9.486  1.00  0.00           C  \n",
        "ATOM     24  O   PHE A   4       2.134  -8.578 -11.793  1.00  0.00           O  \n",
        "ATOM     25  CG  PHE A   4       4.104  -9.580  -8.114  1.00  0.00           C  \n",
        "ATOM     26  CD1 PHE A   4       4.905  -8.495  -7.782  1.00  0.00           C  \n",
        "ATOM     27  CD2 PHE A   4       3.874 -10.559  -7.156  1.00  0.00           C  \n",
        "ATOM     28  CE1 PHE A   4       5.470  -8.387  -6.513  1.00  0.00           C  \n",
        "ATOM     29  CE2 PHE A   4       4.435 -10.458  -5.886  1.00  0.00           C  \n",
        "ATOM     30  CZ  PHE A   4       5.233  -9.372  -5.567  1.00  0.00           C  \n",
        "ATOM     31  N   HIS A   5       0.493  -7.806 -10.795  1.00  0.00           N  \n",
        "ATOM     32  CA  HIS A   5       0.194  -6.689 -11.684  1.00  0.00           C  \n",
        "ATOM     33  C   HIS A   5       0.641  -5.364 -11.074  1.00  0.00           C  \n",
        "ATOM     34  CB  HIS A   5      -1.301  -6.642 -12.003  1.00  0.00           C  \n",
        "ATOM     35  O   HIS A   5       0.022  -4.871 -10.129  1.00  0.00           O  \n",
        "ATOM     36  CG  HIS A   5      -1.685  -7.451 -13.201  1.00  0.00           C  \n",
        "ATOM     37  CD2 HIS A   5      -1.326  -7.338 -14.502  1.00  0.00           C  \n",
        "ATOM     38  ND1 HIS A   5      -2.544  -8.526 -13.129  1.00  0.00           N  \n",
        "ATOM     39  CE1 HIS A   5      -2.697  -9.041 -14.338  1.00  0.00           C  \n",
        "ATOM     40  NE2 HIS A   5      -1.969  -8.339 -15.189  1.00  0.00           N  \n",
        "ATOM     41  N   ILE A   6       1.961  -5.098 -10.946  1.00  0.00           N  \n",
        "ATOM     42  CA  ILE A   6       2.360  -3.755 -10.539  1.00  0.00           C  \n",
        "ATOM     43  C   ILE A   6       2.589  -2.888 -11.775  1.00  0.00           C  \n",
        "ATOM     44  CB  ILE A   6       3.631  -3.784  -9.661  1.00  0.00           C  \n",
        "ATOM     45  O   ILE A   6       3.410  -3.222 -12.632  1.00  0.00           O  \n",
        "ATOM     46  CG1 ILE A   6       3.393  -4.628  -8.404  1.00  0.00           C  \n",
        "ATOM     47  CG2 ILE A   6       4.066  -2.363  -9.293  1.00  0.00           C  \n",
        "ATOM     48  CD1 ILE A   6       4.566  -5.522  -8.028  1.00  0.00           C  \n",
        "ATOM     49  N   GLU A   7       1.536  -2.336 -12.411  1.00  0.00           N  \n",
        "ATOM     50  CA  GLU A   7       1.427  -1.250 -13.380  1.00  0.00           C  \n",
        "ATOM     51  C   GLU A   7       2.077   0.027 -12.852  1.00  0.00           C  \n",
        "ATOM     52  CB  GLU A   7      -0.039  -0.987 -13.731  1.00  0.00           C  \n",
        "ATOM     53  O   GLU A   7       1.618   0.599 -11.862  1.00  0.00           O  \n",
        "ATOM     54  CG  GLU A   7      -0.240  -0.360 -15.103  1.00  0.00           C  \n",
        "ATOM     55  CD  GLU A   7      -1.695  -0.329 -15.542  1.00  0.00           C  \n",
        "ATOM     56  OE1 GLU A   7      -1.986   0.187 -16.645  1.00  0.00           O  \n",
        "ATOM     57  OE2 GLU A   7      -2.552  -0.825 -14.777  1.00  0.00           O  \n",
        "ATOM     58  N   ARG A   8       3.371   0.094 -12.817  1.00  0.00           N  \n",
        "ATOM     59  CA  ARG A   8       4.140   1.306 -12.555  1.00  0.00           C  \n",
        "ATOM     60  C   ARG A   8       3.975   2.314 -13.687  1.00  0.00           C  \n",
        "ATOM     61  CB  ARG A   8       5.621   0.973 -12.362  1.00  0.00           C  \n",
        "ATOM     62  O   ARG A   8       4.376   2.052 -14.823  1.00  0.00           O  \n",
        "ATOM     63  CG  ARG A   8       6.186   1.429 -11.026  1.00  0.00           C  \n",
        "ATOM     64  CD  ARG A   8       7.694   1.234 -10.956  1.00  0.00           C  \n",
        "ATOM     65  NE  ARG A   8       8.156   1.092  -9.578  1.00  0.00           N  \n",
        "ATOM     66  NH1 ARG A   8       9.448  -0.779  -9.985  1.00  0.00           N  \n",
        "ATOM     67  NH2 ARG A   8       9.334   0.104  -7.872  1.00  0.00           N  \n",
        "ATOM     68  CZ  ARG A   8       8.978   0.139  -9.148  1.00  0.00           C  \n",
        "ATOM     69  N   THR A   9       2.829   2.928 -13.808  1.00  0.00           N  \n",
        "ATOM     70  CA  THR A   9       2.600   4.094 -14.654  1.00  0.00           C  \n",
        "ATOM     71  C   THR A   9       3.114   5.362 -13.978  1.00  0.00           C  \n",
        "ATOM     72  CB  THR A   9       1.105   4.257 -14.987  1.00  0.00           C  \n",
        "ATOM     73  O   THR A   9       2.663   5.717 -12.887  1.00  0.00           O  \n",
        "ATOM     74  CG2 THR A   9       0.852   4.082 -16.481  1.00  0.00           C  \n",
        "ATOM     75  OG1 THR A   9       0.351   3.274 -14.267  1.00  0.00           O  \n",
        "ATOM     76  N   PRO A  10       4.375   5.707 -14.052  1.00  0.00           N  \n",
        "ATOM     77  CA  PRO A  10       4.529   7.162 -13.997  1.00  0.00           C  \n",
        "ATOM     78  C   PRO A  10       5.507   7.692 -15.043  1.00  0.00           C  \n",
        "ATOM     79  CB  PRO A  10       5.058   7.408 -12.581  1.00  0.00           C  \n",
        "ATOM     80  O   PRO A  10       6.678   7.306 -15.050  1.00  0.00           O  \n",
        "ATOM     81  CG  PRO A  10       5.735   6.131 -12.200  1.00  0.00           C  \n",
        "ATOM     82  CD  PRO A  10       5.263   5.046 -13.124  1.00  0.00           C  \n",
        "ATOM     83  N   TYR A  11       5.254   7.419 -16.347  1.00  0.00           N  \n",
        "ATOM     84  CA  TYR A  11       5.914   8.181 -17.401  1.00  0.00           C  \n",
        "ATOM     85  C   TYR A  11       5.569   9.662 -17.301  1.00  0.00           C  \n",
        "ATOM     86  CB  TYR A  11       5.520   7.645 -18.780  1.00  0.00           C  \n",
        "ATOM     87  O   TYR A  11       4.395  10.036 -17.350  1.00  0.00           O  \n",
        "ATOM     88  CG  TYR A  11       6.229   6.367 -19.160  1.00  0.00           C  \n",
        "ATOM     89  CD1 TYR A  11       7.301   6.381 -20.049  1.00  0.00           C  \n",
        "ATOM     90  CD2 TYR A  11       5.827   5.145 -18.633  1.00  0.00           C  \n",
        "ATOM     91  CE1 TYR A  11       7.957   5.207 -20.404  1.00  0.00           C  \n",
        "ATOM     92  CE2 TYR A  11       6.475   3.964 -18.981  1.00  0.00           C  \n",
        "ATOM     93  OH  TYR A  11       8.182   2.839 -20.213  1.00  0.00           O  \n",
        "ATOM     94  CZ  TYR A  11       7.537   4.005 -19.865  1.00  0.00           C  \n",
        "ATOM     95  N   MET A  12       6.033  10.379 -16.253  1.00  0.00           N  \n",
        "ATOM     96  CA  MET A  12       6.396  11.776 -16.030  1.00  0.00           C  \n",
        "ATOM     97  C   MET A  12       6.133  12.613 -17.277  1.00  0.00           C  \n",
        "ATOM     98  CB  MET A  12       7.866  11.892 -15.623  1.00  0.00           C  \n",
        "ATOM     99  O   MET A  12       6.530  12.233 -18.380  1.00  0.00           O  \n",
        "ATOM    100  CG  MET A  12       8.085  11.922 -14.119  1.00  0.00           C  \n",
        "ATOM    101  SD  MET A  12       9.830  12.272 -13.670  1.00  0.00           S  \n",
        "ATOM    102  CE  MET A  12      10.309  10.666 -12.975  1.00  0.00           C  \n",
        "ATOM    103  N   ASN A  13       4.840  12.977 -17.501  1.00  0.00           N  \n",
        "ATOM    104  CA  ASN A  13       4.332  14.138 -18.224  1.00  0.00           C  \n",
        "ATOM    105  C   ASN A  13       5.444  15.136 -18.535  1.00  0.00           C  \n",
        "ATOM    106  CB  ASN A  13       3.215  14.818 -17.430  1.00  0.00           C  \n",
        "ATOM    107  O   ASN A  13       6.142  15.596 -17.630  1.00  0.00           O  \n",
        "ATOM    108  CG  ASN A  13       2.216  15.531 -18.321  1.00  0.00           C  \n",
        "ATOM    109  ND2 ASN A  13       1.159  16.064 -17.719  1.00  0.00           N  \n",
        "ATOM    110  OD1 ASN A  13       2.393  15.602 -19.540  1.00  0.00           O  \n",
        "ATOM    111  N   ALA A  14       6.298  14.891 -19.661  1.00  0.00           N  \n",
        "ATOM    112  CA  ALA A  14       7.045  15.904 -20.401  1.00  0.00           C  \n",
        "ATOM    113  C   ALA A  14       7.947  16.709 -19.470  1.00  0.00           C  \n",
        "ATOM    114  CB  ALA A  14       6.088  16.833 -21.145  1.00  0.00           C  \n",
        "ATOM    115  O   ALA A  14       9.027  17.150 -19.870  1.00  0.00           O\n",
        "\"\"\"\n",
        "from alphafold.common import protein\n",
        "prot = protein.from_pdb_string(text)\n",
        "target_all_atom_positions = prot.atom_positions\n",
        "print(\"Target all_atom_positions:\", type(target_all_atom_positions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kLYlbIJ9ct8",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3c. test lddt (optional, code in repository)\n",
        "\n",
        "\n",
        "from alphafold.model.all_atom_test import AllAtomTest\n",
        "aat = AllAtomTest()\n",
        "\n",
        "aat.test_frame_aligned_point_error_perfect_on_global_transform_rot_174_trans_1()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLYglTWilJQL",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3d. Run run_alphafold_test (optional, code in repository)\n",
        "from run_alphafold_test import RunAlphafoldTest\n",
        "r= RunAlphafoldTest()\n",
        "r.test_end_to_end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ql38lZZ1SX",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3e. RunAlphafoldTest (optional, local code)\n",
        "import os\n",
        "\n",
        "from absl.testing import absltest\n",
        "from absl.testing import parameterized\n",
        "from run_alphafold import predict_structure\n",
        "from unittest import mock\n",
        "import numpy as np\n",
        "import run_alphafold\n",
        "# Internal import (7716).\n",
        "\n",
        "\n",
        "class RunAlphafoldTest(parameterized.TestCase):\n",
        "\n",
        "  def test_end_to_end(self):\n",
        "\n",
        "    data_pipeline_mock = mock.Mock()\n",
        "    model_runner_mock = mock.Mock()\n",
        "    amber_relaxer_mock = mock.Mock()\n",
        "\n",
        "    data_pipeline_mock.process.return_value = {}\n",
        "    model_runner_mock.process_features.return_value = {\n",
        "        'aatype': np.zeros((12, 10), dtype=np.int32),\n",
        "        'residue_index': np.tile(np.arange(10, dtype=np.int32)[None], (12, 1)),\n",
        "    }\n",
        "    model_runner_mock.predict.return_value = {\n",
        "        'structure_module': {\n",
        "            'final_atom_positions': np.zeros((10, 37, 3)),\n",
        "            'final_atom_mask': np.ones((10, 37)),\n",
        "        },\n",
        "        'predicted_lddt': {\n",
        "            'logits': np.ones((10, 50)),\n",
        "        },\n",
        "        'plddt': np.ones(10) * 42,\n",
        "        'ptm': np.array(0.),\n",
        "        'aligned_confidence_probs': np.zeros((10, 10, 50)),\n",
        "        'predicted_aligned_error': np.zeros((10, 10)),\n",
        "        'max_predicted_aligned_error': np.array(0.),\n",
        "    }\n",
        "    amber_relaxer_mock.process.return_value = ('RELAXED', None, None)\n",
        "\n",
        "    fasta_path = os.path.join(absltest.get_default_test_tmpdir(),\n",
        "                              'target.fasta')\n",
        "    with open(fasta_path, 'wt') as f:\n",
        "      f.write('>A\\nAAAAAAAAAAAAA')\n",
        "    fasta_name = 'test'\n",
        "\n",
        "    out_dir = absltest.get_default_test_tmpdir()\n",
        "\n",
        "    run_alphafold.predict_structure(\n",
        "        fasta_path=fasta_path,\n",
        "        fasta_name=fasta_name,\n",
        "        output_dir_base=out_dir,\n",
        "        data_pipeline=data_pipeline_mock,\n",
        "        model_runners={'model1': model_runner_mock},\n",
        "        amber_relaxer=amber_relaxer_mock,\n",
        "        benchmark=False,\n",
        "        random_seed=0)\n",
        "\n",
        "    base_output_files = os.listdir(out_dir)\n",
        "    self.assertIn('target.fasta', base_output_files)\n",
        "    self.assertIn('test', base_output_files)\n",
        "\n",
        "\n",
        "    target_output_files = os.listdir(os.path.join(out_dir, 'test'))\n",
        "    self.assertCountEqual(\n",
        "        ['features.pkl', 'msas', 'ranked_0.pdb', 'ranking_debug.json',\n",
        "         'relaxed_model1.pdb', 'result_model1.pkl', 'timings.json',\n",
        "         'unrelaxed_model1.pdb'], target_output_files)\n",
        "\n",
        "    # Check that pLDDT is set in the B-factor column.\n",
        "    with open(os.path.join(out_dir, 'test', 'unrelaxed_model1.pdb')) as f:\n",
        "      for line in f:\n",
        "        if line.startswith('ATOM'):\n",
        "          self.assertEqual(line[61:66], '42.00')\n",
        "    print(\"End-to-end completed successfully\")      \n",
        "\n",
        "r= RunAlphafoldTest()\n",
        "r.test_end_to_end()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YltwDZJRgMIf"
      },
      "source": [
        "#@markdown Upload files to /content/alphafold/model/ .\n",
        "#@markdown Check box to select and upload files.\n",
        "upload_files = False #@param {type:\"boolean\"}\n",
        "if upload_files:\n",
        "  uploaded = files.upload()\n",
        "  for filename,contents in uploaded.items():\n",
        "   filepath = Path(\"/content/alphafold/model\",filename)\n",
        "   with filepath.open(\"w\") as fh:\n",
        "        fh.write(contents.decode(\"UTF-8\"))\n",
        "   print(\"Uploaded to %s\" %(filepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "s5R7hIoEhrtN"
      },
      "source": [
        "#@markdown Optional code goes here. Type in anything, check box, and run cell\n",
        "run_optional_code = False #@param {type:\"boolean\"}\n",
        "\n",
        "! # use ! at start of line to indicate bash command\n",
        "#   use any python command as-is\n",
        "#   comment out commands you don't want to run\n",
        "! # mv /content/folding.py /content/alphafold/model/\n",
        "! # mv /content/modules.py /content/alphafold/model/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYUZJDD5ABjv",
        "cellView": "form"
      },
      "source": [
        "# USER INPUT SECTION\n",
        "\n",
        "# IMPORTS, STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "import os, sys\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "# Local methods\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "def clear_directories(all_dirs):\n",
        "\n",
        "  for d in all_dirs:\n",
        "    if d.exists():\n",
        "      shutil.rmtree(d)\n",
        "    d.mkdir(parents=True)\n",
        "\n",
        "\n",
        "def clean_query(query_sequence):\n",
        "  query_sequence = \"\".join(query_sequence.split())\n",
        "  query_sequence = re.sub(r'[^a-zA-Z]','', query_sequence).upper()\n",
        "  return query_sequence\n",
        "\n",
        "def clean_jobname(jobname):\n",
        "  jobname = \"\".join(jobname.split())\n",
        "  jobname = re.sub(r'\\W+', '', jobname)\n",
        "  if len(jobname.split(\"_\")) == 1:\n",
        "    jobname = add_hash(jobname, query_sequence)\n",
        "  return jobname\n",
        "\n",
        "def save_sequence(jobname, query_sequence):\n",
        "  # save sequence as text file\n",
        "  filename = f\"{jobname}.fasta\"\n",
        "  with open(filename, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)\n",
        "  print(\"Saved sequence in %s: %s\" %(filename, query_sequence))\n",
        "\n",
        "def upload_templates(cif_dir):\n",
        "  manual_templates_uploaded = []\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    uploaded = files.upload()\n",
        "    for filename,contents in uploaded.items():\n",
        "      if str(filename).endswith(\".pdb\"):\n",
        "        continue\n",
        "      filepath = Path(cif_dir,filename)\n",
        "      with filepath.open(\"w\") as fh:\n",
        "        fh.write(contents.decode(\"UTF-8\"))\n",
        "        manual_templates_uploaded.append(filepath)\n",
        "  print(\"Templates uploaded: %s\" %(manual_templates_uploaded))   \n",
        "  if not manual_templates_uploaded:\n",
        "    print(\"\\n*** WARNING: no templates uploaded...Please use only .cif files ***\\n\")\n",
        "  return manual_templates_uploaded\n",
        "\n",
        "def get_jobnames_sequences_from_file(\n",
        "    upload_manual_templates = None, cif_dir = None):\n",
        "  from io import StringIO\n",
        "  from google.colab import files\n",
        "  print(\"Upload file with one jobname, a space and one sequence on each line\")\n",
        "\n",
        "  uploaded = files.upload()\n",
        "  s = StringIO()\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  cif_filename_dict = {}\n",
        "  for filename,contents in uploaded.items():\n",
        "    print(contents.decode(\"UTF-8\"), file = s)\n",
        "    text = s.getvalue()\n",
        "    for line in text.splitlines():\n",
        "      spl = line.split()\n",
        "      if len(spl) < 2:\n",
        "        pass # empty line\n",
        "      else: # usual\n",
        "        jobname = spl[0]\n",
        "        query_sequence = \"\".join(spl[1:])\n",
        "        jobname = clean_jobname(jobname)\n",
        "        query_sequence = clean_query(query_sequence)\n",
        "\n",
        "        if jobname in jobnames:\n",
        "          pass # already there\n",
        "        else:\n",
        "          query_sequences.append(query_sequence)\n",
        "          jobnames.append(jobname)\n",
        "          if upload_manual_templates:\n",
        "            print(\"\\nPlease upload CIF template for %s\" %(jobname))\n",
        "            sys.stdout.flush()\n",
        "            cif_filename_dict[jobname] = upload_templates(cif_dir)\n",
        "  return jobnames, query_sequences, cif_filename_dict\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "# Clear out directories\n",
        "parent_dir = Path(\"/content/manual_templates\")\n",
        "cif_dir = Path(parent_dir,\"mmcif\")\n",
        "\n",
        "# GET INPUTS\n",
        "\n",
        "#@title A. Enter sequence and jobname. Load with <i><b>Run</b></i> button to left.\n",
        "#@markdown <b><i><font color=green>Protein sequence and job name</font></i></b>\n",
        "\n",
        "query_sequence = 'asdfhiertpymna' #@param {type:\"string\"}\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "\n",
        "templates_to_use = \"None\"  \n",
        "\n",
        "upload_manual_templates = False \n",
        "include_templates_from_pdb = False\n",
        "\n",
        "disable_jit = True #@param {type:\"boolean\"}\n",
        "number_of_ensembles = 1 #@param {type:\"integer\"}\n",
        "af_iterations = 0 #@param {type:\"integer\"}\n",
        "include_target_all_atom_positions_loaded_above = True #@param {type:\"boolean\"}\n",
        "if target_all_atom_positions is not None:\n",
        "  if include_target_all_atom_positions_loaded_above:\n",
        "    print(\"Including target_all_atom_positions as target\")\n",
        "  else:\n",
        "    target_all_atom_positions = None\n",
        "if upload_manual_templates:\n",
        "  print(\"Templates will be uploaded\")\n",
        "if include_templates_from_pdb:\n",
        "  print(\"Templates from the PDB will be included\")\n",
        "\n",
        "upload_file_with_jobname_space_sequence_lines = False \n",
        "maximum_templates_from_pdb = 20 \n",
        "clear_saved_sequences_and_jobnames = True \n",
        "\n",
        "# Initialize query_sequences so we can loop through input\n",
        "if clear_saved_sequences_and_jobnames or (\n",
        "     not locals().get('query_sequences', None)):\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  cif_filename_dict = {}\n",
        "  clear_directories([parent_dir,cif_dir])\n",
        "del locals()['clear_saved_sequences_and_jobnames'] # so it updates\n",
        "\n",
        "if upload_file_with_jobname_space_sequence_lines:\n",
        "  del locals()['upload_file_with_jobname_space_sequence_lines'] # so it updates\n",
        "  jobnames, query_sequences, cif_filename_dict = \\\n",
        "    get_jobnames_sequences_from_file(\n",
        "        upload_manual_templates = upload_manual_templates,\n",
        "        cif_dir = cif_dir)\n",
        "else: # usual\n",
        "  jobname = clean_jobname(jobname)\n",
        "  query_sequence = clean_query(query_sequence)\n",
        "  if query_sequence and not jobname:\n",
        "    print(\"Please enter a job name and rerun\")\n",
        "    raise AssertionError(\"Please enter a job name and rerun\")\n",
        "\n",
        "  if jobname and not query_sequence:\n",
        "    print(\"Please enter a query_sequence and rerun\")\n",
        "    raise AssertionError(\"Please enter a query_sequence rerun\")\n",
        "  \n",
        "  # Add sequence and jobname if new\n",
        "  if (jobname and query_sequence) and (\n",
        "       not query_sequence in query_sequences) and (\n",
        "       not jobname in jobnames):\n",
        "      query_sequences.append(query_sequence)\n",
        "      jobnames.append(jobname)\n",
        "      if upload_manual_templates:\n",
        "        print(\"\\nPlease upload template for %s\" %(jobname))\n",
        "        sys.stdout.flush()\n",
        "        cif_filename_dict[jobname] = upload_templates(cif_dir)\n",
        "\n",
        "# Save sequence\n",
        "for i in range(len(query_sequences)):\n",
        "  # save the sequence as a file with name jobname.fasta\n",
        "  save_sequence(jobnames[i], query_sequences[i])\n",
        "  \n",
        "print(\"\\nCurrent jobs, sequences, and templates:\")\n",
        "\n",
        "for qs,jn in zip(query_sequences, jobnames):\n",
        "  template_list = []\n",
        "  for t in cif_filename_dict.get(jn,[]):\n",
        "    template_list.append(os.path.split(str(t))[-1])\n",
        "  print(jn, qs, template_list)\n",
        "\n",
        "sys.stdout.flush()  # seems to overwrite otherwise\n",
        "\n",
        "\n",
        "if not query_sequences:\n",
        "  print(\"Please supply a query sequence and run again\")\n",
        "  raise AssertionError(\"Need a query sequence\")\n",
        "\n",
        "# STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "#standard values of parameters\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
        "num_models = 1 \n",
        "homooligomer = 1\n",
        "use_msa = True\n",
        "use_env = True\n",
        "use_custom_msa = False\n",
        "use_amber = False \n",
        "use_templates = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hQBtcKoAStg",
        "cellView": "form"
      },
      "source": [
        "#@title B. Create AlphaFold models with the <b><i>Run</i></b> button to the left\n",
        "\n",
        "! echo \"Clearing and re-importing python modules and tmp directories...\"\n",
        "! rm -rf /tmp/absl_testing/\n",
        "! mkdir /tmp/absl_testing/\n",
        "\n",
        "for x in list(sys.modules.keys(  )) + list(globals()):\n",
        "  for key in ['alphafold','protein', 'Alphafold', 'Protein','haiku',\"layer_norm\",\"base\",\"colabfold\",\"control_flow\",\"check_tree_and_avals\"]:\n",
        "    if x.find(key)>-1:\n",
        "      if x in list(sys.modules.keys()):\n",
        "        \n",
        "        del(sys.modules[x])\n",
        "      if x in list(globals().keys()):\n",
        "      \n",
        "        del globals()[x]\n",
        "        assert not x in list(globals().keys())\n",
        "        break\n",
        "    \n",
        "\n",
        "if not os.environ['PYTHONPATH'].find(\":/opt/conda/bin\")>-1:\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/bin\"\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/lib/python3.7/site-packages\"\n",
        "  os.environ['PYTHONPATH']+=\":/usr/local/lib/python3.7/dist-packages\"\n",
        "! echo \"VERSION:  `grep Version run_alphafold_test.py|grep -v Apache`\"\n",
        "\n",
        "\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from dataclasses import dataclass, replace\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "\n",
        "\n",
        "print(\"Setting up methods...\", end = \"\")\n",
        "import_alphafold_items = True\n",
        "# setup the model\n",
        "if import_alphafold_items:\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  import colabfold as cf\n",
        "\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "\n",
        "from alphafold.data import mmcif_parsing\n",
        "from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
        "                                      _process_single_hit,\n",
        "                                      _assess_hhsearch_hit,\n",
        "                                      _build_query_to_hit_index_mapping,\n",
        "                                      _extract_template_features,\n",
        "                                      SingleHitResult,\n",
        "                                      TEMPLATE_FEATURES)\n",
        "\n",
        "def mk_mock_template(query_sequence):\n",
        "  # since alphafold's model requires a template input\n",
        "  # we create a blank example w/ zero input, confidence -1\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"-\"*ln\n",
        "  output_confidence_scores = np.full(ln,-1)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
        "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
        "                       'template_sequence': [f'none'.encode()],\n",
        "                       'template_aatype': np.array(templates_aatype)[None],\n",
        "                       'template_confidence_scores': output_confidence_scores[None],\n",
        "                       'template_domain_names': [f'none'.encode()],\n",
        "                       'template_release_date': [f'none'.encode()]}\n",
        "  return template_features\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, model_params, \n",
        "  use_model,\n",
        "  model_runner_1,\n",
        "  model_runner_3,\n",
        "  do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  L_prev = 0\n",
        "  # Ls: number of residues in each chain\n",
        "  for L_i in Ls[:-1]:\n",
        "      idx_res[L_prev+L_i:] += 200\n",
        "      L_prev += L_i  \n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "  feature_dict['residue_index'] = idx_res\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "      \n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      prediction_result = model_runner.predict(processed_feature_dict)\n",
        "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'])\n",
        "      paes.append(prediction_result['predicted_aligned_error'])\n",
        "\n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  \n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
        "  \"\"\"\n",
        "  This is a hack to get hhsuite output strings to pass on\n",
        "  to the AlphaFold template featurizer. \n",
        "  \n",
        "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
        "  all the templates. Currently it builds a database with only one template at a time. Even \n",
        "  better would be to get an hhsuite alignment without using a database at all, just between\n",
        "  pairs of sequence files. However, I have not figured out how to do this.\n",
        "\n",
        "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
        "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
        "  \"\"\"\n",
        "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
        "  if hhDB_dir.exists():\n",
        "    shutil.rmtree(hhDB_dir)\n",
        "  \n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  msa_dir.mkdir(parents=True)\n",
        "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
        "  with template_seq_path.open(\"w\") as fh:\n",
        "    SeqIO.write([template_seq], fh, \"fasta\")\n",
        "\n",
        "  # make hhsuite DB\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    os.chdir(msa_dir)\n",
        "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
        "    os.chdir(hhDB_dir)\n",
        "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
        "    %shell rm DB_msa.ff{data,index}\n",
        "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
        "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
        "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
        "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
        "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
        "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
        "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
        "\n",
        "  # run hhsearch\n",
        "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
        "  with StringIO() as fh:\n",
        "    SeqIO.write([query_seq], fh, \"fasta\")\n",
        "    seq_fasta = fh.getvalue()\n",
        "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
        "\n",
        "  # process hits\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  if len(hhsearch_hits) >0:\n",
        "    hit = hhsearch_hits[0]\n",
        "    hit = replace(hit,**{\"name\":template_seq.id})\n",
        "  else:\n",
        "    hit = None\n",
        "  return hit\n",
        "\n",
        "def plot_plddt_legend():\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=100)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "def plot_confidence(outs, model_num=1):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  plt.figure(figsize=(10,3),dpi=100)\n",
        "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
        "  #########################################\n",
        "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
        "  plt.plot(outs[model_name][\"plddt\"])\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence))\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  #########################################\n",
        "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
        "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "  #########################################\n",
        "  return plt\n",
        "\n",
        "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  if use_amber:\n",
        "    pdb_filename = f\"{jobname}_relaxed_{model_name}.pdb\"\n",
        "  else:\n",
        "    pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
        "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "def run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_amber,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        number_of_ensembles,\n",
        "        af_iterations,\n",
        "        disable_jit,\n",
        "        target_all_atom_positions,):\n",
        "\n",
        "  #@title Get MSA and templates\n",
        "  print(\"Getting MSA and templates...\")\n",
        "  if (not include_templates_from_pdb):\n",
        "    template_paths = None # toss these ... get template_paths later\n",
        "  if use_templates:\n",
        "    a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "    if template_paths is None:\n",
        "      template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "    else:\n",
        "      template_features = mk_template(a3m_lines, template_paths)\n",
        "  elif use_msa:\n",
        "    a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  else:\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  \n",
        "  # File for a3m\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "\n",
        "  if use_msa:\n",
        "    with open(a3m_file, \"w\") as text_file:\n",
        "      text_file.write(a3m_lines)\n",
        "  else:\n",
        "    a3m_lines = \"\".join(open(a3m_file,\"r\").read())\n",
        "  \n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "  \n",
        "  print(\"Done with MSA and templates\")\n",
        "  \n",
        "  #Process templates\n",
        "  print(\"PROCESSING TEMPLATES\")\n",
        "  \n",
        "  os.chdir(\"/content/\")\n",
        "  \n",
        "  other_cif_dir = Path(\"/content/%s\" %(template_paths))\n",
        "  parent_dir = Path(\"/content/manual_templates\")\n",
        "  cif_dir = Path(parent_dir,\"mmcif\")\n",
        "  fasta_dir = Path(parent_dir,\"fasta\")\n",
        "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  clear_directories([fasta_dir,hhDB_dir,msa_dir])\n",
        "  \n",
        "  cif_files = list(cif_dir.glob(\"*\"))\n",
        "  number_of_supplied_templates = len(cif_files)\n",
        "  # Only include the cif_files in manual_templates_uploaded\n",
        "  manual_files_as_text = []\n",
        "  for f in manual_templates_uploaded:\n",
        "    manual_files_as_text.append(\n",
        "        os.path.split(str(f))[-1])\n",
        "  cif_files_to_include = []\n",
        "  for cif_file in cif_files:\n",
        "    text = os.path.split(str(cif_file))[-1]\n",
        "    if text in manual_files_as_text:\n",
        "      cif_files_to_include.append(cif_file)\n",
        "  cif_files = cif_files_to_include\n",
        "      \n",
        "  if include_templates_from_pdb:\n",
        "    other_cif_files = list(other_cif_dir.glob(\"*\"))\n",
        "    cif_files += other_cif_files\n",
        "  print(\"CIF files to include:\",cif_files)\n",
        "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",name=\"\",description=\"\")\n",
        "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
        "  with query_seq_path.open(\"w\") as fh:\n",
        "      SeqIO.write([query_seq], fh, \"fasta\")\n",
        "  \n",
        "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
        "  seqs = []\n",
        "  template_hit_list = []\n",
        "  \n",
        "  n_used = 0\n",
        "  for i,filepath in enumerate(cif_files):\n",
        "    if not str(filepath).endswith(\".cif\"): continue\n",
        "    if n_used >= maximum_templates_from_pdb + number_of_supplied_templates:\n",
        "      continue\n",
        "    n_used += 1\n",
        "    print(\"CIF file included:\",i+1,str(filepath))\n",
        "    with filepath.open(\"r\") as fh:\n",
        "      filestr = fh.read()\n",
        "      mmcif_obj = mmcif_parsing.parse(file_id=filepath.stem,mmcif_string=filestr)\n",
        "      mmcif = mmcif_obj.mmcif_object\n",
        "      if not mmcif: continue\n",
        "  \n",
        "      for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
        "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
        "        seq_name = filepath.stem.upper()+\"_\"+chain_id\n",
        "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
        "        seqs.append(seq)\n",
        "  \n",
        "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
        "          SeqIO.write([seq], fh, \"fasta\")\n",
        "  \n",
        "        \"\"\"\n",
        "        At this stage, we have a template sequence.\n",
        "        and a query sequence. \n",
        "        There are two options to generate template features:\n",
        "          1. Write new code to manually generate template features\n",
        "          2. Get an hhr alignment string, and pass that\n",
        "            to the existing template featurizer. \n",
        "            \n",
        "        I chose the second, implemented in hh_process_seq()\n",
        "        \"\"\"\n",
        "        SeqIO.write([seq], sys.stdout, \"fasta\")\n",
        "        SeqIO.write([query_seq], sys.stdout, \"fasta\")\n",
        "        try:\n",
        "          hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
        "        except Exception as e:\n",
        "          hit = None\n",
        "        if hit is not None:\n",
        "          template_hit_list.append(hit)\n",
        "  \n",
        "  if template_hit_list:\n",
        "    #process hits into template features\n",
        "    template_hit_list = [replace(hit,**{\"index\":i+1}) for i,hit in enumerate(template_hit_list)]\n",
        "  \n",
        "  if (len(manual_templates_uploaded) > 0) and upload_manual_templates and (not template_hit_list):\n",
        "    # check to make sure we got something\n",
        "    # need template and did not get any\n",
        "      print(\"\\n\",80*\"-\")\n",
        "      print(\"\\nNo templates obtained...please be sure to use a .cif file\")\n",
        "      print(\"Use this converter: https://mmcif.pdbj.org/converter/\")\n",
        "      print(\"\\nYou can hit the red run button and load a new file\")\n",
        "      print(\"\\nYou can then hit all the remaining red run buttons one by one\")\n",
        "      print(\"\\n ... or you can go up to Runtime and hit 'Run all' again to start over\")\n",
        "      print(\"\\n\",80*\"-\")\n",
        "      raise AssertionError(\"Failed to read template file\")\n",
        "  elif use_templates and template_hit_list:\n",
        "    # have new templates to work with\n",
        "  \n",
        "    template_features = {}\n",
        "    for template_feature_name in TEMPLATE_FEATURES:\n",
        "      template_features[template_feature_name] = []\n",
        "  \n",
        "    for i,hit in enumerate(sorted(template_hit_list, key=lambda x: x.sum_probs, reverse=True)):\n",
        "      # modifications to alphafold/data/templates.py _process_single_hit\n",
        "      hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
        "      mapping = _build_query_to_hit_index_mapping(\n",
        "      hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
        "      query_sequence)\n",
        "      template_sequence = hit.hit_sequence.replace('-', '')\n",
        "  \n",
        "      if 1:\n",
        "        features, realign_warning = _extract_template_features(\n",
        "          mmcif_object=mmcif,\n",
        "          pdb_id=hit_pdb_code,\n",
        "          mapping=mapping,\n",
        "          template_sequence=template_sequence,\n",
        "          query_sequence=query_sequence,\n",
        "          template_chain_id=hit_chain_id,\n",
        "          kalign_binary_path=\"kalign\")\n",
        "      if 0:\n",
        "        continue\n",
        "      features['template_sum_probs'] = [hit.sum_probs]\n",
        "  \n",
        "      single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
        "      for k in template_features:\n",
        "        template_features[k].append(features[k])\n",
        "  \n",
        "    for name in template_features:\n",
        "      template_features[name] = np.stack(\n",
        "          template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
        "      \n",
        "    #overwrite template data\n",
        "    template_paths = cif_dir.as_posix()\n",
        "\n",
        "\n",
        "    # Select only one chain from any cif file\n",
        "    unique_template_hits = []\n",
        "    pdb_text_list = []\n",
        "    for hit in template_hit_list:\n",
        "      pdb_text = hit.name.split()[0].split(\"_\")[0]\n",
        "      if not pdb_text in pdb_text_list:\n",
        "        pdb_text_list.append(pdb_text)\n",
        "        unique_template_hits.append(hit)\n",
        "    template_hit_list = unique_template_hits\n",
        "    template_hits = template_hit_list\n",
        "\n",
        "    print(\"\\nIncluding templates:\")\n",
        "    for hit in template_hit_list:\n",
        "      print(\"\\t\",hit.name.split()[0])\n",
        "    if len(template_hit_list) == 0:\n",
        "      print(\"No templates found...quitting\")\n",
        "      raise AssertionError(\"No templates found...quitting\")\n",
        "    os.chdir(\"/content/\")\n",
        "  \n",
        "    for key,value in template_features.items():\n",
        "      if np.all(value==0):\n",
        "        print(\"ERROR: Some template features are empty\")\n",
        "  else:  # no templates\n",
        "    print(\"Not using any templates\")\n",
        "  \n",
        "  print(\"\\nPREDICTING STRUCTURE\")\n",
        "\n",
        "  \n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  model_params = {}\n",
        "  model_runner_1 = None\n",
        "  model_runner_3 = None\n",
        "\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "\n",
        "    if model_name not in list(model_params.keys()):\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.model.num_recycle = af_iterations\n",
        "        model_config.data.common.num_recycle = af_iterations\n",
        "        print(\"Recycle iterations will be %s\" %(model_config.data.common.num_recycle))\n",
        "        model_config.data.eval.num_ensemble = number_of_ensembles\n",
        "        print(\"Number of ensembles will be %s\" %(model_config.data.eval.num_ensemble))\n",
        "        if disable_jit:\n",
        "          model_config.data.common.disable_jit = True\n",
        "          model_config.model.global_config.disable_jit = True\n",
        "\n",
        "        if target_all_atom_positions is not None:\n",
        "          print(\"Setting all_atom\",target_all_atom_positions.shape[0])\n",
        "          model_config.model.global_config.target_all_atom_positions = target_all_atom_positions\n",
        "          print(\"Set target_all_atom_positions\")\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "        print(\"Done running model.RunModel to get model_runner_1\")\n",
        "\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "\n",
        "\n",
        "  if homooligomer == 1:\n",
        "    msas = [msa]\n",
        "    deletion_matrices = [deletion_matrix]\n",
        "  else:\n",
        "    # make multiple copies of msa for each copy\n",
        "    # AAA------\n",
        "    # ---AAA---\n",
        "    # ------AAA\n",
        "    #\n",
        "    # note: if you concat the sequences (as below), it does NOT work\n",
        "    # AAAAAAAAA\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    Ln = len(query_sequence)\n",
        "    for o in range(homooligomer):\n",
        "      L = Ln * o\n",
        "      R = Ln * (homooligomer-(o+1))\n",
        "      msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
        "      deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
        "  \n",
        "  # gather features\n",
        "  feature_dict = {\n",
        "      **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
        "                                        description=\"none\",\n",
        "                                        num_res=len(query_sequence)*homooligomer),\n",
        "      **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "      **template_features\n",
        "  }\n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)]*homooligomer,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           model_runner_1=model_runner_1,\n",
        "                           model_runner_3=model_runner_3,\n",
        "                           do_relax=use_amber)\n",
        "  print(\"DONE WITH STRUCTURE\")\n",
        "  \n",
        "  #@title Making plots...\n",
        "  \n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "  ##################################################################\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Predicted Alignment Error\")\n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "  plt.savefig(jobname+\"_PAE.png\")\n",
        "  plt.show()\n",
        "  ##################################################################\n",
        "  #@title Displaying 3D structure... {run: \"auto\"}\n",
        "  model_num = 1 \n",
        "  color = \"lDDT\" \n",
        "  show_sidechains = False \n",
        "  show_mainchains = False \n",
        "  \n",
        "  \n",
        "  \n",
        "  show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
        "  if color == \"lDDT\": plot_plddt_legend().show()  \n",
        "  plot_confidence(outs, model_num).show()\n",
        "  #@title Packaging and downloading results...\n",
        "  \n",
        "  #@markdown When modeling is complete .zip files with results will be downloaded automatically.\n",
        "  \n",
        "  citations = {\n",
        "  \"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "  author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "  doi = {10.1101/2021.08.15.456425},\n",
        "  journal = {bioRxiv},\n",
        "  title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "  year = {2021},\n",
        "  comment = {ColabFold including MMseqs2 MSA server}\n",
        "  }\"\"\",\n",
        "    \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "  doi = {10.1093/nar/gkz1035},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "  year = {2019},\n",
        "  comment = {MGnify database}\n",
        "  }\"\"\",\n",
        "    \"Eastman2017\": \"\"\"@article{Eastman2017,\n",
        "  author = {Eastman, Peter and Swails, Jason and Chodera, John D. and McGibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and Wiewiora, Rafal P. and Brooks, Bernard R. and Pande, Vijay S.},\n",
        "  doi = {10.1371/journal.pcbi.1005659},\n",
        "  journal = {PLOS Comput. Biol.},\n",
        "  number = {7},\n",
        "  title = {{OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}},\n",
        "  volume = {13},\n",
        "  year = {2017},\n",
        "  comment = {Amber relaxation}\n",
        "  }\"\"\",\n",
        "    \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "  doi = {10.1038/s41586-021-03819-2},\n",
        "  journal = {Nature},\n",
        "  pmid = {34265844},\n",
        "  title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "  year = {2021},\n",
        "  comment = {AlphaFold2 + BFD Database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "  author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1093/bioinformatics/bty1057},\n",
        "  journal = {Bioinformatics},\n",
        "  number = {16},\n",
        "  pages = {2856--2858},\n",
        "  pmid = {30615063},\n",
        "  title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "  volume = {35},\n",
        "  year = {2019},\n",
        "  comment = {MMseqs2 search server}\n",
        "  }\"\"\",\n",
        "    \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "  author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1186/s12859-019-3019-7},\n",
        "  journal = {BMC Bioinform.},\n",
        "  number = {1},\n",
        "  pages = {473},\n",
        "  pmid = {31521110},\n",
        "  title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "  volume = {20},\n",
        "  year = {2019},\n",
        "  comment = {PDB70 database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "  author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "  doi = {10.1093/nar/gkw1081},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  number = {D1},\n",
        "  pages = {D170--D176},\n",
        "  pmid = {27899574},\n",
        "  title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "  volume = {45},\n",
        "  year = {2017},\n",
        "  comment = {Uniclust30/UniRef30 database},\n",
        "  }\"\"\",\n",
        "    \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "  booktitle = {Nat. Struct. Biol.},\n",
        "  doi = {10.1038/nsb1203-980},\n",
        "  number = {12},\n",
        "  pages = {980},\n",
        "  pmid = {14634627},\n",
        "  title = {{Announcing the worldwide Protein Data Bank}},\n",
        "  volume = {10},\n",
        "  year = {2003},\n",
        "  comment = {templates downloaded from wwPDB server}\n",
        "  }\"\"\",\n",
        "  }\n",
        "  \n",
        "  to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "  if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "  if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "  if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "  if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "  if use_templates: to_cite += [\"Berman2003\"]\n",
        "  if use_amber:     to_cite += [\"Eastman2017\"]\n",
        "  \n",
        "  with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
        "    for i in to_cite:\n",
        "      writer.write(citations[i])\n",
        "      writer.write(\"\\n\")\n",
        "  \n",
        "  print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
        "  if use_custom_msa:\n",
        "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "  \n",
        "  !echo 'FILES TO PACKAGE: $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\" '\n",
        "  try:\n",
        "    print(\"zipping files...\")\n",
        "    !zip -FSr $jobname\".result.zip\" $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\"\n",
        "  except Exception as e:\n",
        "    print(\"unable to zip files\")\n",
        "\n",
        "  filename = f\"{jobname}.result.zip\"\n",
        "  if os.path.isfile(filename):\n",
        "    print(\"About to download %s\" %(filename))\n",
        "  \n",
        "    try:\n",
        "      print(\"Downloading zip file %s\" %(filename))\n",
        "      files.download(filename)\n",
        "      print(\"Start of download successful (NOTE: if the download symbol does not go away it did not work. Download it manually using the folder icon to the left)\")\n",
        "      return filename\n",
        "    except Exception as e:\n",
        "      print(\"Unable to download zip file %s\" %(filename))\n",
        "      return None\n",
        "  else:\n",
        "    print(\"No .zip file %s created\" %(filename))\n",
        "    return None\n",
        "\n",
        "# RUN THE JOBS HERE\n",
        "\n",
        "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "  print(\"\\n\",\"****************************************\",\"\\n\",\n",
        "         \"RUNNING JOB %s with sequence %s\\n\" %(\n",
        "    jobname, query_sequence),\n",
        "    \"****************************************\",\"\\n\")\n",
        "  # GET TEMPLATES AND SET UP FILES\n",
        "\n",
        "\n",
        "\n",
        "  # User input of manual templates\n",
        "  manual_templates_uploaded = cif_filename_dict.get(\n",
        "      jobname,[])\n",
        "  if manual_templates_uploaded:\n",
        "    print(\"Using uploaded templates %s for this run\" %(\n",
        "        manual_templates_uploaded))\n",
        "\n",
        "  if 1:\n",
        "    filename = run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_amber,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        number_of_ensembles,\n",
        "        af_iterations,\n",
        "        disable_jit,\n",
        "        target_all_atom_positions)\n",
        "    if filename:\n",
        "      print(\"FINISHED JOB (%s) %s with sequence %s\\n\" %(\n",
        "        filename, jobname, query_sequence),\n",
        "        \"****************************************\",\"\\n\")\n",
        "    else:\n",
        "      print(\"NO RESULT FOR JOB %s with sequence %s\\n\" %(\n",
        "    jobname, query_sequence),\n",
        "    \"****************************************\",\"\\n\")\n",
        "\n",
        "  if 0:\n",
        "    print(\"FAILED: JOB %s with sequence %s\\n\\n%s\\n\" %(\n",
        "    jobname, query_sequence, str(e)),\n",
        "    \"****************************************\",\"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\nDOWNLOADING FILES NOW:\\n\")\n",
        "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "  filename = f\"{jobname}.result.zip\"\n",
        "  if os.path.isfile(filename):\n",
        "    print(filename)\n",
        "\n",
        "print(\"\\nALL DONE\\n\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
