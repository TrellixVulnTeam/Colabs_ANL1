{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold_testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFold_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1r2dn6P2uq"
      },
      "source": [
        "### <center> <b> <font color='black'>  AlphaFold development suite</font></b> </center>\n",
        "\n",
        "<p><font color='green'> Instructions</p>\n",
        "\n",
        "<p>A. SETUP:  Run cells 1-3 to set up or hit <b><i>Runtime/Run all</i></b> to run everything (5 min.)</p>\n",
        "\n",
        "<p>B. DEVELOPMENT CYCLE:  </p>\n",
        "<li> 1a. Either edit files in the alphafold directory, or </li>\n",
        "\n",
        " <li> 1b. Edit files in github and hit \"Load current alphafold_working from github\"</li>\n",
        " <li> 2. Run cell B below to run Alphafold (2 min.)</li>\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "#@markdown 1. Set up imports and load dependencies...this is the slow step\n",
        "import os, sys\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "! echo \"Installing biopython ...\"\n",
        "!  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "\n",
        "! echo \"Downloading model parameters...\"\n",
        "!    rm -rf params\n",
        "!    mkdir params\n",
        "!    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar  | tar x -C params\n",
        "\n",
        "!echo \"Downloading jq curl zlib1g gawk...\"\n",
        "!    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "!    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Setting up conda...\"\n",
        "!    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "!    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "! echo \"Setting up template search methods...\"\n",
        "\n",
        "! conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Installing openmm...\"\n",
        "! conda install -qy -c omnia openmm 2>&1 1>/dev/null\n",
        "\n",
        "! echo \"Installing Mock...\"\n",
        "!  pip install mock\n",
        " \n",
        "! echo \"Setting paths to site-packages and dist-packages...\"\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages\")\n",
        "sys.path.append(\"/usr/local/lib/python3.7/dist-packages\")\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages/simtk/\")\n",
        "\n",
        "target_all_atom_positions = None # Initialize\n",
        "target_prot = None # initialize\n",
        "\n",
        "\n",
        "! echo \"Done with loading dependencies\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMuoZ9Kl7BoJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown 2. Load current alphafold_working from phenix-project github.  \n",
        "#@markdown  This is quick...\n",
        "#@markdown cycle back here after making a change in github alphafold_working\n",
        "! echo \"installing alphafold from https://github.com/phenix-project/af_development.git in /contents/alphafold\"\n",
        "\n",
        "! rm -rf af_development\n",
        "! rm -rf alphafold\n",
        "! rm -f colabfold.py\n",
        "\n",
        "\n",
        "! git clone https://github.com/phenix-project/af_development.git --quiet\n",
        "! (cd af_development; git checkout --quiet)\n",
        "! mv af_development/alphafold alphafold\n",
        "! mv af_development/colabfold.py .\n",
        "! mv af_development/run_alphafold.py .\n",
        "! mv alphafold/run_alphafold_test.py .\n",
        "\n",
        "\n",
        "\n",
        "!    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "!    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "!    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "\n",
        "\n",
        "! echo \"Ready with alphafold in /content/alphafold\"\n",
        "! ls -ltr /content/alphafold\n",
        "\n",
        "! echo \"clearing out /tmp/absl_testing/\"\n",
        "! rm -rf /tmp/absl_testing/\n",
        "! mkdir /tmp/absl_testing/\n",
        "\n",
        "! echo \" Clearing python caches ...\"\n",
        "for x in list(sys.modules.keys(  )) + list(globals()):\n",
        "  for key in ['alphafold','protein', 'Alphafold', 'Protein', 'colabfold']:\n",
        "    if x.find(key)>-1:\n",
        "      if x in list(sys.modules.keys()):\n",
        "        \n",
        "        del(sys.modules[x])\n",
        "      if x in list(globals().keys()):\n",
        "      \n",
        "        del globals()[x]\n",
        "        assert not x in list(globals().keys())\n",
        "        break\n",
        "\n",
        "\n",
        "if not os.environ['PYTHONPATH'].find(\":/opt/conda/bin\")>-1:\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/bin\"\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/lib/python3.7/site-packages\"\n",
        "  os.environ['PYTHONPATH']+=\":/usr/local/lib/python3.7/dist-packages\"\n",
        "! echo \"`grep Version run_alphafold_test.py|grep -v Apache`\"\n",
        "! echo \"Done loading current version.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlUDwt0lM0oc",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3a. Run ProteinTest (optional, local code)\n",
        "\n",
        "TEST_DATA_DIR = 'alphafold/common/testdata/'\n",
        "\n",
        "from absl.testing import parameterized\n",
        "from absl.testing import absltest\n",
        "from alphafold.common import protein\n",
        "import numpy as np\n",
        "class ProteinTest(parameterized.TestCase):\n",
        "\n",
        "  def _check_shapes(self, prot, num_res):\n",
        "    \"\"\"Check that the processed shapes are correct.\"\"\"\n",
        "    num_atoms = residue_constants.atom_type_num\n",
        "    self.assertEqual((num_res, num_atoms, 3), prot.atom_positions.shape)\n",
        "    self.assertEqual((num_res,), prot.aatype.shape)\n",
        "    self.assertEqual((num_res, num_atoms), prot.atom_mask.shape)\n",
        "    self.assertEqual((num_res,), prot.residue_index.shape)\n",
        "    self.assertEqual((num_res, num_atoms), prot.b_factors.shape)\n",
        "    print(\"Finished _check_shapes\")\n",
        "\n",
        "  @parameterized.parameters(('2rbg.pdb', 'A', 282),\n",
        "                            ('2rbg.pdb', 'B', 282))\n",
        "  def test_from_pdb_str(self, pdb_file, chain_id, num_res):\n",
        "    pdb_file = os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                            pdb_file)\n",
        "    with open(pdb_file) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id)\n",
        "    self._check_shapes(prot, num_res)\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "    self.assertGreaterEqual(prot.aatype.min(), 0)\n",
        "    # Allow equal since unknown restypes have index equal to restype_num.\n",
        "    self.assertLessEqual(prot.aatype.max(), residue_constants.restype_num)\n",
        "    print(\"Finished test_from_pdb_str\")\n",
        "\n",
        "  def test_to_pdb(self):\n",
        "    with open(\n",
        "        os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                     '2rbg.pdb')) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id='A')\n",
        "    pdb_string_reconstr = protein.to_pdb(prot)\n",
        "    prot_reconstr = protein.from_pdb_string(pdb_string_reconstr)\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "\n",
        "    np.testing.assert_array_equal(prot_reconstr.aatype, prot.aatype)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.atom_positions, prot.atom_positions)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.atom_mask, prot.atom_mask)\n",
        "    np.testing.assert_array_equal(\n",
        "        prot_reconstr.residue_index, prot.residue_index)\n",
        "    np.testing.assert_array_almost_equal(\n",
        "        prot_reconstr.b_factors, prot.b_factors)\n",
        "    print(\"Finished test_to_pdb\")\n",
        "\n",
        "  def test_ideal_atom_mask(self):\n",
        "    with open(\n",
        "        os.path.join(absltest.get_default_test_srcdir(), TEST_DATA_DIR,\n",
        "                     '2rbg.pdb')) as f:\n",
        "      pdb_string = f.read()\n",
        "    prot = protein.from_pdb_string(pdb_string, chain_id='A')\n",
        "    print(\"Total residues: %s\" %(prot.aatype.shape))\n",
        "    \n",
        "    ideal_mask = protein.ideal_atom_mask(prot)\n",
        "    non_ideal_residues = set([102] + list(range(127, 285)))\n",
        "    for i, (res, atom_mask) in enumerate(\n",
        "        zip(prot.residue_index, prot.atom_mask)):\n",
        "      if res in non_ideal_residues:\n",
        "        self.assertFalse(np.all(atom_mask == ideal_mask[i]), msg=f'{res}')\n",
        "      else:\n",
        "        self.assertTrue(np.all(atom_mask == ideal_mask[i]), msg=f'{res}')\n",
        "    print(\"Finished test_ideal_atom_mask\")\n",
        "    \n",
        "t = ProteinTest()\n",
        "\n",
        "t.test_to_pdb()\n",
        "t.test_ideal_atom_mask()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kLYlbIJ9ct8",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3c. test lddt (optional, code in repository)\n",
        "\n",
        "\n",
        "from alphafold.model.all_atom_test import AllAtomTest\n",
        "aat = AllAtomTest()\n",
        "\n",
        "aat.test_frame_aligned_point_error_perfect_on_global_transform_rot_174_trans_1()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLYglTWilJQL",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3d. Run run_alphafold_test (optional, code in repository)\n",
        "from run_alphafold_test import RunAlphafoldTest\n",
        "r= RunAlphafoldTest()\n",
        "r.test_end_to_end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ql38lZZ1SX",
        "cellView": "form"
      },
      "source": [
        "#@markdown 3e. RunAlphafoldTest (optional, local code)\n",
        "import os\n",
        "\n",
        "from absl.testing import absltest\n",
        "from absl.testing import parameterized\n",
        "from run_alphafold import predict_structure\n",
        "from unittest import mock\n",
        "import numpy as np\n",
        "import run_alphafold\n",
        "# Internal import (7716).\n",
        "\n",
        "\n",
        "class RunAlphafoldTest(parameterized.TestCase):\n",
        "\n",
        "  def test_end_to_end(self):\n",
        "\n",
        "    data_pipeline_mock = mock.Mock()\n",
        "    model_runner_mock = mock.Mock()\n",
        "    amber_relaxer_mock = mock.Mock()\n",
        "\n",
        "    data_pipeline_mock.process.return_value = {}\n",
        "    model_runner_mock.process_features.return_value = {\n",
        "        'aatype': np.zeros((12, 10), dtype=np.int32),\n",
        "        'residue_index': np.tile(np.arange(10, dtype=np.int32)[None], (12, 1)),\n",
        "    }\n",
        "    model_runner_mock.predict.return_value = {\n",
        "        'structure_module': {\n",
        "            'final_atom_positions': np.zeros((10, 37, 3)),\n",
        "            'final_atom_mask': np.ones((10, 37)),\n",
        "        },\n",
        "        'predicted_lddt': {\n",
        "            'logits': np.ones((10, 50)),\n",
        "        },\n",
        "        'plddt': np.ones(10) * 42,\n",
        "        'ptm': np.array(0.),\n",
        "        'aligned_confidence_probs': np.zeros((10, 10, 50)),\n",
        "        'predicted_aligned_error': np.zeros((10, 10)),\n",
        "        'max_predicted_aligned_error': np.array(0.),\n",
        "    }\n",
        "    amber_relaxer_mock.process.return_value = ('RELAXED', None, None)\n",
        "\n",
        "    fasta_path = os.path.join(absltest.get_default_test_tmpdir(),\n",
        "                              'target.fasta')\n",
        "    with open(fasta_path, 'wt') as f:\n",
        "      f.write('>A\\nAAAAAAAAAAAAA')\n",
        "    fasta_name = 'test'\n",
        "\n",
        "    out_dir = absltest.get_default_test_tmpdir()\n",
        "\n",
        "    run_alphafold.predict_structure(\n",
        "        fasta_path=fasta_path,\n",
        "        fasta_name=fasta_name,\n",
        "        output_dir_base=out_dir,\n",
        "        data_pipeline=data_pipeline_mock,\n",
        "        model_runners={'model1': model_runner_mock},\n",
        "        amber_relaxer=amber_relaxer_mock,\n",
        "        benchmark=False,\n",
        "        random_seed=0)\n",
        "\n",
        "    base_output_files = os.listdir(out_dir)\n",
        "    self.assertIn('target.fasta', base_output_files)\n",
        "    self.assertIn('test', base_output_files)\n",
        "\n",
        "\n",
        "    target_output_files = os.listdir(os.path.join(out_dir, 'test'))\n",
        "    self.assertCountEqual(\n",
        "        ['features.pkl', 'msas', 'ranked_0.pdb', 'ranking_debug.json',\n",
        "         'relaxed_model1.pdb', 'result_model1.pkl', 'timings.json',\n",
        "         'unrelaxed_model1.pdb'], target_output_files)\n",
        "\n",
        "    # Check that pLDDT is set in the B-factor column.\n",
        "    with open(os.path.join(out_dir, 'test', 'unrelaxed_model1.pdb')) as f:\n",
        "      for line in f:\n",
        "        if line.startswith('ATOM'):\n",
        "          self.assertEqual(line[61:66], '42.00')\n",
        "    print(\"End-to-end completed successfully\")      \n",
        "\n",
        "r= RunAlphafoldTest()\n",
        "r.test_end_to_end()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YltwDZJRgMIf",
        "cellView": "form"
      },
      "source": [
        "#@markdown Upload files to /content/alphafold/model/ .\n",
        "#@markdown Check box to select and upload files.\n",
        "upload_files = False #@param {type:\"boolean\"}\n",
        "if upload_files:\n",
        "  uploaded = files.upload()\n",
        "  for filename,contents in uploaded.items():\n",
        "   if filename == 'protein.py':\n",
        "     filepath = Path(\"/content/alphafold/common\",filename)\n",
        "   else:\n",
        "     filepath = Path(\"/content/alphafold/model\",filename)\n",
        "   with filepath.open(\"w\") as fh:\n",
        "        fh.write(contents.decode(\"UTF-8\"))\n",
        "   print(\"Uploaded to %s\" %(filepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "s5R7hIoEhrtN"
      },
      "source": [
        "#@markdown Optional code goes here. Type in anything, check box, and run cell\n",
        "run_optional_code = False #@param {type:\"boolean\"}\n",
        "\n",
        "! # use ! at start of line to indicate bash command\n",
        "#   use any python command as-is\n",
        "#   comment out commands you don't want to run\n",
        "! # mv /content/folding.py /content/alphafold/model/\n",
        "! # mv /content/modules.py /content/alphafold/model/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYUZJDD5ABjv",
        "cellView": "form"
      },
      "source": [
        "# USER INPUT SECTION\n",
        "\n",
        "# IMPORTS, STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "import os, sys\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "# Local methods\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "def clear_directories(all_dirs):\n",
        "\n",
        "  for d in all_dirs:\n",
        "    if d.exists():\n",
        "      shutil.rmtree(d)\n",
        "    d.mkdir(parents=True)\n",
        "\n",
        "\n",
        "def clean_query(query_sequence):\n",
        "  query_sequence = \"\".join(query_sequence.split())\n",
        "  query_sequence = re.sub(r'[^a-zA-Z]','', query_sequence).upper()\n",
        "  return query_sequence\n",
        "\n",
        "def clean_jobname(jobname):\n",
        "  jobname = \"\".join(jobname.split())\n",
        "  jobname = re.sub(r'\\W+', '', jobname)\n",
        "  if len(jobname.split(\"_\")) == 1:\n",
        "    jobname = add_hash(jobname, query_sequence)\n",
        "  return jobname\n",
        "\n",
        "def save_sequence(jobname, query_sequence):\n",
        "  # save sequence as text file\n",
        "  filename = f\"{jobname}.fasta\"\n",
        "  with open(filename, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)\n",
        "  print(\"Saved sequence in %s: %s\" %(filename, query_sequence))\n",
        "\n",
        "def upload_target():\n",
        "  target_dict = {}\n",
        "  from alphafold.common import protein\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    uploaded = files.upload()\n",
        "    for filename,contents in uploaded.items():\n",
        "      if not str(filename).endswith(\".cif\"):\n",
        "        continue\n",
        "      filepath = Path(cif_dir,filename)\n",
        "      text_as_mmcif = contents.decode(\"UTF-8\")\n",
        "      text_as_bio_pdb = protein.mmcif_text_to_BioPDB(text_as_mmcif)\n",
        "      text_as_pdb = protein.BioPDB_to_pdb_text(text_as_bio_pdb)\n",
        "      target_dict[filename] = {}\n",
        "      target_dict[filename]['prot'] = prot = protein.from_pdb_string(text_as_pdb)\n",
        "      target_dict[filename]['mmcif_text'] = text_as_mmcif\n",
        "\n",
        "  print(\"Targets uploaded: %s\" %(target_dict.keys())) \n",
        "  if len( list(target_dict.keys())) < 1:\n",
        "    print(\"\\n*** WARNING: no templates uploaded...Please use only .cif files ***\\n\")\n",
        "  return target_dict\n",
        "\n",
        "def upload_msa():\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "  print(\"\\nPlease select MSA file...\")\n",
        "  sys.stdout.flush()\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    uploaded = files.upload()\n",
        "  for filename,contents in uploaded.items():\n",
        "      a3m_lines=contents.decode(\"UTF-8\")\n",
        "      break\n",
        "  print(\"MSA file %s with %s lines uploaded \" %(\n",
        "            filename, len(a3m_lines.splitlines())))\n",
        "  sys.stdout.flush()\n",
        "  return a3m_lines\n",
        "\n",
        "def get_jobnames_sequences_from_file(\n",
        "    upload_manual_templates = None, cif_dir = None):\n",
        "  from io import StringIO\n",
        "  from google.colab import files\n",
        "  print(\"Upload file with one jobname, a space and one sequence on each line\")\n",
        "\n",
        "  uploaded = files.upload()\n",
        "  s = StringIO()\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  cif_filename_dict = {}\n",
        "  for filename,contents in uploaded.items():\n",
        "    print(contents.decode(\"UTF-8\"), file = s)\n",
        "    text = s.getvalue()\n",
        "    for line in text.splitlines():\n",
        "      spl = line.split()\n",
        "      if len(spl) < 2:\n",
        "        pass # empty line\n",
        "      else: # usual\n",
        "        jobname = spl[0]\n",
        "        query_sequence = \"\".join(spl[1:])\n",
        "        jobname = clean_jobname(jobname)\n",
        "        query_sequence = clean_query(query_sequence)\n",
        "\n",
        "        if jobname in jobnames:\n",
        "          pass # already there\n",
        "        else:\n",
        "          query_sequences.append(query_sequence)\n",
        "          jobnames.append(jobname)\n",
        "          if upload_manual_templates:\n",
        "            print(\"\\nPlease upload CIF template for %s\" %(jobname))\n",
        "            sys.stdout.flush()\n",
        "            cif_filename_dict[jobname] = upload_templates(cif_dir)\n",
        "  return jobnames, query_sequences, cif_filename_dict\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "# Clear out directories\n",
        "parent_dir = Path(\"/content/manual_templates\")\n",
        "cif_dir = Path(parent_dir,\"mmcif\")\n",
        "\n",
        "# GET INPUTS\n",
        "\n",
        "#@title A. Enter sequence and jobname. Load with <i><b>Run</b></i> button to left.\n",
        "#@markdown <b><i><font color=green>Protein sequence and job name</font></i></b>\n",
        "\n",
        "query_sequence = 'PSSGTSFHTASPSFSSRYRY' #@param {type:\"string\"}\n",
        "jobname = 'test' #@param {type:\"string\"}\n",
        "\n",
        "templates_to_use = \"None\"  \n",
        "\n",
        "upload_manual_templates = False \n",
        "include_templates_from_pdb = False\n",
        "\n",
        "disable_jit = False #@param {type:\"boolean\"}\n",
        "number_of_ensembles = 1 #@param {type:\"integer\"}\n",
        "af_iterations = 1 #@param {type:\"integer\"}\n",
        "upload_target_pdb_file = False #@param {type:\"boolean\"}\n",
        "use_target_pdb_as_template = False #@param {type:\"boolean\"}\n",
        "use_target_pdb_as_recycle = False #@param {type:\"boolean\"}\n",
        "use_custom_msa = True #@param {type:\"boolean\"}\n",
        "\n",
        "if upload_manual_templates:\n",
        "  print(\"Templates will be uploaded\")\n",
        "if include_templates_from_pdb:\n",
        "  print(\"Templates from the PDB will be included\")\n",
        "upload_file_with_jobname_space_sequence_lines = False \n",
        "maximum_templates_from_pdb = 20 \n",
        "clear_saved_sequences_and_jobnames = True \n",
        "\n",
        "# Initialize query_sequences so we can loop through input\n",
        "if clear_saved_sequences_and_jobnames or (\n",
        "     not locals().get('query_sequences', None)):\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  cif_filename_dict = {}\n",
        "  clear_directories([parent_dir,cif_dir])\n",
        "del locals()['clear_saved_sequences_and_jobnames'] # so it updates\n",
        "\n",
        "target_dict = {} # initialize\n",
        "\n",
        "if upload_file_with_jobname_space_sequence_lines:\n",
        "  del locals()['upload_file_with_jobname_space_sequence_lines'] # so it updates\n",
        "  jobnames, query_sequences, cif_filename_dict = \\\n",
        "    get_jobnames_sequences_from_file(\n",
        "        upload_manual_templates = upload_manual_templates,\n",
        "        cif_dir = cif_dir)\n",
        "else: # usual\n",
        "  jobname = clean_jobname(jobname)\n",
        "  query_sequence = clean_query(query_sequence)\n",
        "  if query_sequence and not jobname:\n",
        "    print(\"Please enter a job name and rerun\")\n",
        "    raise AssertionError(\"Please enter a job name and rerun\")\n",
        "\n",
        "  if jobname and not query_sequence:\n",
        "    print(\"Please enter a query_sequence and rerun\")\n",
        "    raise AssertionError(\"Please enter a query_sequence rerun\")\n",
        "  \n",
        "  # Add sequence and jobname if new\n",
        "  if (jobname and query_sequence) and (\n",
        "       not query_sequence in query_sequences) and (\n",
        "       not jobname in jobnames):\n",
        "      query_sequences.append(query_sequence)\n",
        "      jobnames.append(jobname)\n",
        "      if upload_target_pdb_file:\n",
        "        print(\"\\nPlease upload template for %s (must match exactly in sequence)\" %(jobname))\n",
        "        sys.stdout.flush()\n",
        "        target_dict = upload_target()\n",
        "        cif_filename_dict[jobname] = list(target_dict.keys())\n",
        "  \n",
        "        target_dict['jobname'] = list(target_dict.keys())[0]\n",
        "  \n",
        "        assert len(jobnames) == 1 # need different code if multiple\n",
        "\n",
        "\n",
        "# Save sequence\n",
        "for i in range(len(query_sequences)):\n",
        "  # save the sequence as a file with name jobname.fasta\n",
        "  save_sequence(jobnames[i], query_sequences[i])\n",
        "  \n",
        "print(\"\\nCurrent jobs, sequences, and templates:\")\n",
        "\n",
        "for qs,jn in zip(query_sequences, jobnames):\n",
        "  template_list = []\n",
        "  for t in cif_filename_dict.get(jn,[]):\n",
        "    template_list.append(os.path.split(str(t))[-1])\n",
        "  print(jn, qs, template_list)\n",
        "\n",
        "  target_dict['use_target_pdb_as_template'] = False\n",
        "  target_dict['use_target_pdb_as_recycle'] = False\n",
        "if use_target_pdb_as_template:\n",
        "  print(\"Using target_pdb as a template\")\n",
        "  target_dict['use_target_pdb_as_template'] = True\n",
        "if use_target_pdb_as_recycle:\n",
        "  print(\"Using target_pdb as if it were recycled model\")\n",
        "  target_dict['use_target_pdb_as_recycle'] = True\n",
        "\n",
        "\n",
        "sys.stdout.flush()  # seems to overwrite otherwise\n",
        "\n",
        "\n",
        "if not query_sequences:\n",
        "  print(\"Please supply a query sequence and run again\")\n",
        "  raise AssertionError(\"Need a query sequence\")\n",
        "\n",
        "# STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "#standard values of parameters\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
        "num_models = 1 \n",
        "homooligomer = 1\n",
        "use_msa = True\n",
        "use_env = True\n",
        "\n",
        "use_amber = False \n",
        "use_templates = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hQBtcKoAStg",
        "cellView": "form"
      },
      "source": [
        "#@title B. Create AlphaFold models with the <b><i>Run</i></b> button to the left\n",
        "\n",
        "! echo \"Clearing and re-importing python modules and tmp directories...\"\n",
        "! rm -rf /tmp/absl_testing/\n",
        "! mkdir /tmp/absl_testing/\n",
        "\n",
        "for x in list(sys.modules.keys(  )) + list(globals()):\n",
        "  for key in ['alphafold','protein', 'Alphafold', 'Protein','haiku',\"layer_norm\",\"base\",\"colabfold\",\"control_flow\",\"check_tree_and_avals\"]:\n",
        "    if x.find(key)>-1:\n",
        "      if x in list(sys.modules.keys()):\n",
        "        \n",
        "        del(sys.modules[x])\n",
        "      if x in list(globals().keys()):\n",
        "      \n",
        "        del globals()[x]\n",
        "        assert not x in list(globals().keys())\n",
        "        break\n",
        "    \n",
        "\n",
        "if not os.environ['PYTHONPATH'].find(\":/opt/conda/bin\")>-1:\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/bin\"\n",
        "  os.environ['PYTHONPATH']+=\":/opt/conda/lib/python3.7/site-packages\"\n",
        "  os.environ['PYTHONPATH']+=\":/usr/local/lib/python3.7/dist-packages\"\n",
        "! echo \"VERSION:  `grep Version run_alphafold_test.py|grep -v Apache`\"\n",
        "\n",
        "\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from dataclasses import dataclass, replace\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "\n",
        "\n",
        "print(\"Setting up methods...\", end = \"\")\n",
        "import_alphafold_items = True\n",
        "# setup the model\n",
        "if import_alphafold_items:\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  import colabfold as cf\n",
        "\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "\n",
        "from alphafold.data import mmcif_parsing\n",
        "from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
        "                                      _process_single_hit,\n",
        "                                      _assess_hhsearch_hit,\n",
        "                                      _build_query_to_hit_index_mapping,\n",
        "                                      _extract_template_features,\n",
        "                                      SingleHitResult,\n",
        "                                      TEMPLATE_FEATURES)\n",
        "\n",
        "def mk_mock_template(query_sequence):\n",
        "  # since alphafold's model requires a template input\n",
        "  # we create a blank example w/ zero input, confidence -1\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"-\"*ln\n",
        "  output_confidence_scores = np.full(ln,-1)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
        "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
        "                       'template_sequence': [f'none'.encode()],\n",
        "                       'template_aatype': np.array(templates_aatype)[None],\n",
        "                       'template_confidence_scores': output_confidence_scores[None],\n",
        "                       'template_domain_names': [f'none'.encode()],\n",
        "                       'template_release_date': [f'none'.encode()]}\n",
        "  return template_features\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, model_params, \n",
        "  use_model,\n",
        "  model_runner_1,\n",
        "  model_runner_3,\n",
        "  do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  L_prev = 0\n",
        "  # Ls: number of residues in each chain\n",
        "  for L_i in Ls[:-1]:\n",
        "      idx_res[L_prev+L_i:] += 200\n",
        "      L_prev += L_i  \n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "  feature_dict['residue_index'] = idx_res\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "      \n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      prediction_result = model_runner.predict(processed_feature_dict)\n",
        "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'])\n",
        "      paes.append(prediction_result['predicted_aligned_error'])\n",
        "\n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  \n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
        "  \"\"\"\n",
        "  This is a hack to get hhsuite output strings to pass on\n",
        "  to the AlphaFold template featurizer. \n",
        "  \n",
        "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
        "  all the templates. Currently it builds a database with only one template at a time. Even \n",
        "  better would be to get an hhsuite alignment without using a database at all, just between\n",
        "  pairs of sequence files. However, I have not figured out how to do this.\n",
        "\n",
        "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
        "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
        "  \"\"\"\n",
        "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
        "  if hhDB_dir.exists():\n",
        "    shutil.rmtree(hhDB_dir)\n",
        "  \n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  msa_dir.mkdir(parents=True)\n",
        "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
        "  with template_seq_path.open(\"w\") as fh:\n",
        "    SeqIO.write([template_seq], fh, \"fasta\")\n",
        "\n",
        "  # make hhsuite DB\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    os.chdir(msa_dir)\n",
        "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
        "    os.chdir(hhDB_dir)\n",
        "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
        "    %shell rm DB_msa.ff{data,index}\n",
        "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
        "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
        "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
        "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
        "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
        "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
        "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
        "\n",
        "  # run hhsearch\n",
        "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
        "  with StringIO() as fh:\n",
        "    SeqIO.write([query_seq], fh, \"fasta\")\n",
        "    seq_fasta = fh.getvalue()\n",
        "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
        "\n",
        "  # process hits\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  if len(hhsearch_hits) >0:\n",
        "    hit = hhsearch_hits[0]\n",
        "    hit = replace(hit,**{\"name\":template_seq.id})\n",
        "  else:\n",
        "    hit = None\n",
        "  return hit\n",
        "\n",
        "def plot_plddt_legend():\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=100)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "def plot_confidence(outs, model_num=1):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  plt.figure(figsize=(10,3),dpi=100)\n",
        "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
        "  #########################################\n",
        "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
        "  plt.plot(outs[model_name][\"plddt\"])\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence))\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  #########################################\n",
        "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
        "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "  #########################################\n",
        "  return plt\n",
        "\n",
        "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  if use_amber:\n",
        "    pdb_filename = f\"{jobname}_relaxed_{model_name}.pdb\"\n",
        "  else:\n",
        "    pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
        "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "def run_job(query_sequence,\n",
        "        jobname,\n",
        "        target_dict,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_amber,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        number_of_ensembles,\n",
        "        af_iterations,\n",
        "        disable_jit,):\n",
        "\n",
        "\n",
        "  key = target_dict.get('jobname',None)\n",
        "  prot = target_dict.get(key,{}).get('prot',None)\n",
        "  if key is not None:\n",
        "    use_target_pdb_as_recycle = target_dict['use_target_pdb_as_recycle']\n",
        "    use_target_pdb_as_template = target_dict['use_target_pdb_as_template']\n",
        "  else:\n",
        "    use_target_pdb_as_recycle = False\n",
        "    use_target_pdb_as_template = False\n",
        "\n",
        "  if prot is not None and use_target_pdb_as_recycle:\n",
        "    target_all_atom_positions = prot.atom_positions\n",
        "  else:\n",
        "    target_all_atom_positions = None\n",
        "  if prot is not None and use_target_pdb_as_template:\n",
        "    mmcif_text = target_dict[key]['mmcif_text']\n",
        "    print(\"Using target model...\",jobname, key)\n",
        "  else:\n",
        "    mmcif_text = \"\"\n",
        "\n",
        "\n",
        "  #@title Get MSA and templates\n",
        "  print(\"Getting MSA and templates...\")\n",
        "\n",
        "  template_paths = None # toss these ... get template_paths later\n",
        "  if use_custom_msa:\n",
        "    a3m_lines = upload_msa()\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  elif use_msa:\n",
        "    a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  else:\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  \n",
        "  # File for a3m\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "\n",
        "  if use_msa:\n",
        "    with open(a3m_file, \"w\") as text_file:\n",
        "      text_file.write(a3m_lines)\n",
        "  else:\n",
        "    a3m_lines = \"\".join(open(a3m_file,\"r\").read())\n",
        "  \n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "  \n",
        "  print(\"Done with MSA and templates\")\n",
        "  \n",
        "  #Process templates\n",
        "  print(\"PROCESSING TEMPLATES\")\n",
        "  \n",
        "  os.chdir(\"/content/\")\n",
        "  \n",
        "  other_cif_dir = Path(\"/content/%s\" %(template_paths))\n",
        "  parent_dir = Path(\"/content/manual_templates\")\n",
        "  cif_dir = Path(parent_dir,\"mmcif\")\n",
        "  fasta_dir = Path(parent_dir,\"fasta\")\n",
        "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  clear_directories([fasta_dir,hhDB_dir,msa_dir])\n",
        "  \n",
        "  if use_target_pdb_as_template:\n",
        "    cif_files = ['text_a.cif']\n",
        "  else:\n",
        "    cif_files = []\n",
        "  print(\"CIF files to include:\",cif_files)\n",
        "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",name=\"\",description=\"\")\n",
        "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
        "  with query_seq_path.open(\"w\") as fh:\n",
        "      SeqIO.write([query_seq], fh, \"fasta\")\n",
        "  \n",
        "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
        "  seqs = []\n",
        "  template_hit_list = []\n",
        "  \n",
        "  n_used = 0\n",
        "  for i,filepath in enumerate(cif_files):\n",
        "    n_used += 1\n",
        "    assert filepath == 'text_a.cif'\n",
        "    filestr = mmcif_text\n",
        "\n",
        "    mmcif_obj = mmcif_parsing.parse(file_id='text_a',mmcif_string=filestr)\n",
        "    mmcif = mmcif_obj.mmcif_object\n",
        "    if not mmcif: continue\n",
        "    print(\"CIF file included:\",i+1,str(filepath))\n",
        "  \n",
        "    for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
        "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
        "        seq_name = filepath.upper()+\"_\"+chain_id\n",
        "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
        "        seqs.append(seq)\n",
        "  \n",
        "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
        "          SeqIO.write([seq], fh, \"fasta\")\n",
        "  \n",
        "        \"\"\"\n",
        "        At this stage, we have a template sequence.\n",
        "        and a query sequence. \n",
        "        There are two options to generate template features:\n",
        "          1. Write new code to manually generate template features\n",
        "          2. Get an hhr alignment string, and pass that\n",
        "            to the existing template featurizer. \n",
        "            \n",
        "        I chose the second, implemented in hh_process_seq()\n",
        "        \"\"\"\n",
        "        SeqIO.write([seq], sys.stdout, \"fasta\")\n",
        "        SeqIO.write([query_seq], sys.stdout, \"fasta\")\n",
        "        try:\n",
        "          hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
        "        except Exception as e:\n",
        "          hit = None\n",
        "        if hit is not None:\n",
        "          template_hit_list.append(hit)\n",
        "  \n",
        "  if template_hit_list:\n",
        "    #process hits into template features\n",
        "    template_hit_list = [replace(hit,**{\"index\":i+1}) for i,hit in enumerate(template_hit_list)]\n",
        "  \n",
        "  if (len(manual_templates_uploaded) > 0) and upload_manual_templates and (not template_hit_list):\n",
        "    # check to make sure we got something\n",
        "    # need template and did not get any\n",
        "      print(\"\\n\",80*\"-\")\n",
        "      print(\"\\nNo templates obtained...\")\n",
        "      print(\"\\n\",80*\"-\")\n",
        "      raise AssertionError(\"Failed to read template file\")\n",
        "  elif use_templates and template_hit_list:\n",
        "    # have new templates to work with\n",
        "  \n",
        "    template_features = {}\n",
        "    for template_feature_name in TEMPLATE_FEATURES:\n",
        "      template_features[template_feature_name] = []\n",
        "  \n",
        "    for i,hit in enumerate(sorted(template_hit_list, key=lambda x: x.sum_probs, reverse=True)):\n",
        "      # modifications to alphafold/data/templates.py _process_single_hit\n",
        "      hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
        "      mapping = _build_query_to_hit_index_mapping(\n",
        "      hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
        "      query_sequence)\n",
        "      template_sequence = hit.hit_sequence.replace('-', '')\n",
        "  \n",
        "\n",
        "      features, realign_warning = _extract_template_features(\n",
        "          mmcif_object=mmcif,\n",
        "          pdb_id=hit_pdb_code,\n",
        "          mapping=mapping,\n",
        "          template_sequence=template_sequence,\n",
        "          query_sequence=query_sequence,\n",
        "          template_chain_id=hit_chain_id,\n",
        "          kalign_binary_path=\"kalign\")\n",
        "\n",
        "      features['template_sum_probs'] = [hit.sum_probs]\n",
        "  \n",
        "      single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
        "      for k in template_features:\n",
        "        template_features[k].append(features[k])\n",
        "  \n",
        "    for name in template_features:\n",
        "      template_features[name] = np.stack(\n",
        "          template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
        "      \n",
        "    #overwrite template data\n",
        "    template_paths = cif_dir.as_posix()\n",
        "\n",
        "\n",
        "    # Select only one chain from any cif file\n",
        "    unique_template_hits = []\n",
        "    pdb_text_list = []\n",
        "    for hit in template_hit_list:\n",
        "      pdb_text = hit.name.split()[0].split(\"_\")[0]\n",
        "      if not pdb_text in pdb_text_list:\n",
        "        pdb_text_list.append(pdb_text)\n",
        "        unique_template_hits.append(hit)\n",
        "    template_hit_list = unique_template_hits\n",
        "    template_hits = template_hit_list\n",
        "\n",
        "    print(\"\\nIncluding templates:\")\n",
        "    for hit in template_hit_list:\n",
        "      print(\"\\t\",hit.name.split()[0])\n",
        "    if len(template_hit_list) == 0:\n",
        "      print(\"No templates found...quitting\")\n",
        "      raise AssertionError(\"No templates found...quitting\")\n",
        "    os.chdir(\"/content/\")\n",
        "  \n",
        "    for key,value in template_features.items():\n",
        "      if np.all(value==0):\n",
        "        print(\"ERROR: Some template features are empty\")\n",
        "  else:  # no templates\n",
        "    print(\"Not using any templates\")\n",
        "  \n",
        "  print(\"\\nPREDICTING STRUCTURE\")\n",
        "\n",
        "  \n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  model_params = {}\n",
        "  model_runner_1 = None\n",
        "  model_runner_3 = None\n",
        "\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "\n",
        "    if model_name not in list(model_params.keys()):\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.model.num_recycle = af_iterations\n",
        "        model_config.data.common.num_recycle = af_iterations\n",
        "        print(\"Recycle iterations will be %s\" %(model_config.data.common.num_recycle))\n",
        "        model_config.data.eval.num_ensemble = number_of_ensembles\n",
        "        print(\"Number of ensembles will be %s\" %(model_config.data.eval.num_ensemble))\n",
        "        if disable_jit:\n",
        "          model_config.data.common.disable_jit = True\n",
        "          model_config.model.global_config.disable_jit = True\n",
        "\n",
        "        if use_target_pdb_as_recycle and target_all_atom_positions is not None:\n",
        "          print(\"Setting all_atom\",target_all_atom_positions.shape[0])\n",
        "          model_config.model.global_config.target_all_atom_positions = target_all_atom_positions\n",
        "          model_config.model.global_config.target_prot = prot\n",
        "          print(\"Set target_all_atom_positions\")\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "        print(\"Done running model.RunModel to get model_runner_1\")\n",
        "\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "\n",
        "\n",
        "  if homooligomer == 1:\n",
        "    msas = [msa]\n",
        "    deletion_matrices = [deletion_matrix]\n",
        "  else:\n",
        "    # make multiple copies of msa for each copy\n",
        "    # AAA------\n",
        "    # ---AAA---\n",
        "    # ------AAA\n",
        "    #\n",
        "    # note: if you concat the sequences (as below), it does NOT work\n",
        "    # AAAAAAAAA\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    Ln = len(query_sequence)\n",
        "    for o in range(homooligomer):\n",
        "      L = Ln * o\n",
        "      R = Ln * (homooligomer-(o+1))\n",
        "      msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
        "      deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
        "  \n",
        "  # gather features\n",
        "  feature_dict = {\n",
        "      **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
        "                                        description=\"none\",\n",
        "                                        num_res=len(query_sequence)*homooligomer),\n",
        "      **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "      **template_features\n",
        "  }\n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)]*homooligomer,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           model_runner_1=model_runner_1,\n",
        "                           model_runner_3=model_runner_3,\n",
        "                           do_relax=use_amber)\n",
        "  print(\"DONE WITH STRUCTURE\")\n",
        "  \n",
        "  #@title Making plots...\n",
        "  \n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "  ##################################################################\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Predicted Alignment Error\")\n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "  plt.savefig(jobname+\"_PAE.png\")\n",
        "  plt.show()\n",
        "  ##################################################################\n",
        "  #@title Displaying 3D structure... {run: \"auto\"}\n",
        "  model_num = 1 \n",
        "  color = \"lDDT\" \n",
        "  show_sidechains = False \n",
        "  show_mainchains = False \n",
        "  \n",
        "  \n",
        "  \n",
        "  show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
        "  if color == \"lDDT\": plot_plddt_legend().show()  \n",
        "  plot_confidence(outs, model_num).show()\n",
        "  #@title Packaging and downloading results...\n",
        "  \n",
        "  #@markdown When modeling is complete .zip files with results will be downloaded automatically.\n",
        "  \n",
        "  citations = {\n",
        "  \"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "  author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "  doi = {10.1101/2021.08.15.456425},\n",
        "  journal = {bioRxiv},\n",
        "  title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "  year = {2021},\n",
        "  comment = {ColabFold including MMseqs2 MSA server}\n",
        "  }\"\"\",\n",
        "    \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "  doi = {10.1093/nar/gkz1035},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "  year = {2019},\n",
        "  comment = {MGnify database}\n",
        "  }\"\"\",\n",
        "    \"Eastman2017\": \"\"\"@article{Eastman2017,\n",
        "  author = {Eastman, Peter and Swails, Jason and Chodera, John D. and McGibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and Wiewiora, Rafal P. and Brooks, Bernard R. and Pande, Vijay S.},\n",
        "  doi = {10.1371/journal.pcbi.1005659},\n",
        "  journal = {PLOS Comput. Biol.},\n",
        "  number = {7},\n",
        "  title = {{OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}},\n",
        "  volume = {13},\n",
        "  year = {2017},\n",
        "  comment = {Amber relaxation}\n",
        "  }\"\"\",\n",
        "    \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "  doi = {10.1038/s41586-021-03819-2},\n",
        "  journal = {Nature},\n",
        "  pmid = {34265844},\n",
        "  title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "  year = {2021},\n",
        "  comment = {AlphaFold2 + BFD Database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "  author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1093/bioinformatics/bty1057},\n",
        "  journal = {Bioinformatics},\n",
        "  number = {16},\n",
        "  pages = {2856--2858},\n",
        "  pmid = {30615063},\n",
        "  title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "  volume = {35},\n",
        "  year = {2019},\n",
        "  comment = {MMseqs2 search server}\n",
        "  }\"\"\",\n",
        "    \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "  author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1186/s12859-019-3019-7},\n",
        "  journal = {BMC Bioinform.},\n",
        "  number = {1},\n",
        "  pages = {473},\n",
        "  pmid = {31521110},\n",
        "  title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "  volume = {20},\n",
        "  year = {2019},\n",
        "  comment = {PDB70 database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "  author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "  doi = {10.1093/nar/gkw1081},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  number = {D1},\n",
        "  pages = {D170--D176},\n",
        "  pmid = {27899574},\n",
        "  title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "  volume = {45},\n",
        "  year = {2017},\n",
        "  comment = {Uniclust30/UniRef30 database},\n",
        "  }\"\"\",\n",
        "    \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "  booktitle = {Nat. Struct. Biol.},\n",
        "  doi = {10.1038/nsb1203-980},\n",
        "  number = {12},\n",
        "  pages = {980},\n",
        "  pmid = {14634627},\n",
        "  title = {{Announcing the worldwide Protein Data Bank}},\n",
        "  volume = {10},\n",
        "  year = {2003},\n",
        "  comment = {templates downloaded from wwPDB server}\n",
        "  }\"\"\",\n",
        "  }\n",
        "  \n",
        "  to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "  if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "  if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "  if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "  if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "  if use_templates: to_cite += [\"Berman2003\"]\n",
        "  if use_amber:     to_cite += [\"Eastman2017\"]\n",
        "  \n",
        "  with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
        "    for i in to_cite:\n",
        "      writer.write(citations[i])\n",
        "      writer.write(\"\\n\")\n",
        "  \n",
        "  print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
        "  if use_custom_msa:\n",
        "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "  \n",
        "  !echo 'FILES TO PACKAGE: $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\" '\n",
        "  try:\n",
        "    print(\"zipping files...\")\n",
        "    !zip -FSr $jobname\".result.zip\" $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\"\n",
        "  except Exception as e:\n",
        "    print(\"unable to zip files\")\n",
        "\n",
        "  filename = f\"{jobname}.result.zip\"\n",
        "  if os.path.isfile(filename):\n",
        "    print(\"About to download %s\" %(filename))\n",
        "  \n",
        "    try:\n",
        "      print(\"Downloading zip file %s\" %(filename))\n",
        "      files.download(filename)\n",
        "      print(\"Start of download successful (NOTE: if the download symbol does not go away it did not work. Download it manually using the folder icon to the left)\")\n",
        "      return filename\n",
        "    except Exception as e:\n",
        "      print(\"Unable to download zip file %s\" %(filename))\n",
        "      return None\n",
        "  else:\n",
        "    print(\"No .zip file %s created\" %(filename))\n",
        "    return None\n",
        "\n",
        "# RUN THE JOBS HERE\n",
        "\n",
        "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "  print(\"\\n\",\"****************************************\",\"\\n\",\n",
        "         \"RUNNING JOB %s with sequence %s\\n\" %(\n",
        "    jobname, query_sequence),\n",
        "    \"****************************************\",\"\\n\")\n",
        "  # GET TEMPLATES AND SET UP FILES\n",
        "\n",
        "\n",
        "\n",
        "  # User input of manual templates\n",
        "  manual_templates_uploaded = cif_filename_dict.get(\n",
        "      jobname,[])\n",
        "  if manual_templates_uploaded:\n",
        "    print(\"Using uploaded templates %s for this run\" %(\n",
        "        manual_templates_uploaded))\n",
        "\n",
        "  if 1:\n",
        "    filename = run_job(query_sequence,\n",
        "        jobname,\n",
        "        target_dict,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_amber,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        number_of_ensembles,\n",
        "        af_iterations,\n",
        "        disable_jit,\n",
        "        )\n",
        "    if filename:\n",
        "      print(\"FINISHED JOB (%s) %s with sequence %s\\n\" %(\n",
        "        filename, jobname, query_sequence),\n",
        "        \"****************************************\",\"\\n\")\n",
        "    else:\n",
        "      print(\"NO RESULT FOR JOB %s with sequence %s\\n\" %(\n",
        "    jobname, query_sequence),\n",
        "    \"****************************************\",\"\\n\")\n",
        "\n",
        "  if 0:\n",
        "    print(\"FAILED: JOB %s with sequence %s\\n\\n%s\\n\" %(\n",
        "    jobname, query_sequence, str(e)),\n",
        "    \"****************************************\",\"\\n\")\n",
        "\n",
        "\n",
        "print(\"\\nDOWNLOADING FILES NOW:\\n\")\n",
        "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "  filename = f\"{jobname}.result.zip\"\n",
        "  if os.path.isfile(filename):\n",
        "    print(filename)\n",
        "\n",
        "print(\"\\nALL DONE\\n\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}