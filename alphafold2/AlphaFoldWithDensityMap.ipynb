{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphaFoldWithDensityMap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFoldWithDensityMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1r2dn6P2uq"
      },
      "source": [
        "### <center> <b> <font color='black'>  AlphaFold with a density map </font></b> </center>\n",
        "\n",
        "<font color='green'>This notebook integrates Phenix model rebuilding with AlphaFold to improve AlphaFold modeling.  You upload a sequence and a density map (ccp4/mrc format) and it carries out cycles of AlphaFold modeling, rebuilding with the density map, and AlphaFold modeling with the rebuilt model as a template. In each cycle you get a new AlphaFold model and a rebuilt model.\n",
        "\n",
        "To understand how this all works see the Phenix tutorial video [\"AlphaFold changes everything\"](https://youtu.be/9IExeA_A8Xs) and the [BioRxiv preprint](https://www.biorxiv.org/content/10.1101/2022.01.07.475350v2) on using AlphaFold with a density map.\n",
        "\n",
        "This notebook is derived from [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) and the DeepMind [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb).\n",
        "</font>\n",
        "\n",
        "-----------------\n",
        "<b> <font color='black'> <center>Instructions for a simple run:</center>\n",
        "</font></b> \n",
        "\n",
        "1. Run the first cell to install condacolab and reboot the virtual machine. You need to do this <b><i>before</i></b> using <b><i>Run all</i></b> in step 3.\n",
        "\n",
        "2.  Select the \"Basic Inputs\" cell, type in a sequence, resolution, jobname, and Phenix download password in the form in the first cell. You can also edit the Options in the next cell if you want.\n",
        "\n",
        "3. Start your run by going up to the <b><i>Runtime</i></b> pulldown menu and selecting <b><i>Run all</i></b>\n",
        "\n",
        "4. Scroll down the page and follow what is going on.  If necessary, upload your map file when the Upload button appears below the \"Setting up input files\" form\n",
        "\n",
        "5. See the helpful hints at the bottom of the page for more details and advanced notes.\n",
        "\n",
        "\n",
        "</font>\n",
        "-----------------\n",
        "<b> <font color='black'> <center>Please cite the ColabFold and AlphaFold2 papers if you use this notebook:</center>\n",
        "</font></b> \n",
        "\n",
        "- <font color='green'>[Mirdita, M.,  Ovchinnikov, S., Steinegger, M.(2021). ColabFold - Making protein folding accessible to all *bioRxiv*, 2021.08.15.456425](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v2)</font> \n",
        "\n",
        "- <font color='green'> [Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583â€“589 (2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font>\n",
        "-----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Hit the triangle <b>Run</b> button to the left to install condacolab and reboot the virtual machine.  \n",
        "#@markdown Hit \"OK\" when asked if it is ok to run this notebook\n",
        "\n",
        "#@markdown  You can edit the forms below while it is rebooting.\n",
        "\n",
        "#@markdown In 30 sec you get 3 messages about a crash (because of the reboot).\n",
        "#@markdown Close the last one and you are ready to go with <b><i>Runtime</i></b> / <b><i>Run all</i></b>\n",
        "\n",
        "# https://github.com/conda-incubator/condacolab\n",
        "\n",
        "# Get the helper python files phenix_alphafold_utils and phenix_colab_utils\n",
        "def get_helper_files():\n",
        "  import os\n",
        "  for file_name in [\"phenix_alphafold_utils.py\",\"phenix_colab_utils.py\"]:\n",
        "    if os.path.isfile(file_name):\n",
        "      os.remove(file_name)\n",
        "    os.environ['file_name'] = file_name\n",
        "    result = os.system(\"wget -qnc https://raw.githubusercontent.com/phenix-project/Colabs/main/alphafold2/$file_name\")\n",
        "get_helper_files()\n",
        "\n",
        "print(\"About to install condacolab...ignore crash messages\")\n",
        "import phenix_colab_utils as cu\n",
        "cu.clear_python_caches()\n",
        "cu.install_condacolab()\n",
        "print(\"Ready with condacolab installed...close the crash message\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A90Q1WxQ83iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Basic inputs (Required)\n",
        "#@markdown Select this cell, then enter sequence of chain to predict (at least 20 residues), resolution, name of this job, and Phenix download password \n",
        "\n",
        "\n",
        "sequence = 'EVQLVESGGGLVQPGGSLRLSCAASGFNIYSSSIHWVRQAPGKGLEWVAYIYSYSGYTSYADSVKGRFTISADTSKNTAY LQMNSLRAEDTAVYYCARSLEYLYSSGYQYKWATGLDYWGQGTLVTVSSAST' #@param {type:\"string\"}\n",
        "resolution =  3#@param {type:\"number\"}\n",
        "jobname = '7mjs' #@param {type:\"string\"}\n",
        "phenix_download_password='' #@param {type:\"string\"}\n",
        "query_sequence = sequence\n",
        "password = phenix_download_password\n",
        "\n",
        "# Save all parameters in a dictionary\n",
        "params = {}\n",
        "for p in ['resolution','jobname', 'password', 'query_sequence']:\n",
        "  params[p] = locals().get(p,None)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uJjLhYW0v1_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCr54KqwlmA",
        "cellView": "form"
      },
      "source": [
        "#@title 3. Options (Run without changes for a simple job)\n",
        "\n",
        "#@markdown Check if you want your ouputs saved to the directory <b>ColabOuputs</b> on Google drive\n",
        "save_outputs_in_google_drive = True #@param {type:\"boolean\" }\n",
        "\n",
        "#@markdown If your maps and models are uploaded, fill in name of directory containing just these files here\n",
        "#@markdown (usually put them in <b>ColabInputs</b>). Skip parts of the file name like /content/ or MyDrive/). Leave blank to upload directly</i></b>\n",
        "input_directory = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Choose what templates to include (those from the PDB are based on sequence \n",
        "#@markdown similarity):\n",
        "include_templates_from_pdb = False #@param {type:\"boolean\" }\n",
        "maximum_templates_from_pdb =  20#@param {type:\"integer\"}\n",
        "upload_manual_templates = False #@param {type:\"boolean\" }\n",
        "\n",
        "#@markdown Specify whether any uploaded templates have the correct sequence \n",
        "#@markdown (if not checked, only used as suggestions for rebuilding and not as AlphaFold templates)</font></i></b>\n",
        "uploaded_templates_have_exact_sequence = True #@param {type:\"boolean\" }\n",
        "uploaded_templates_are_map_to_model = (not uploaded_templates_have_exact_sequence)\n",
        "\n",
        "\n",
        "#@markdown Cycles to run:\n",
        "maximum_cycles =  10#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Version of Phenix to use:\n",
        "phenix_version ='dev-4497' #@param {type:\"string\"}\n",
        "version = phenix_version  # rename variable\n",
        "\n",
        "#@markdown Specify if you want to run a series of jobs by uploading a file with one jobname, resolution and sequence per line</i></b>\n",
        "upload_file_with_jobname_resolution_sequence_lines = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Specify how to use multiple sequence alignment information</i></b>\n",
        "msa_use = 'Use MSA throughout' #@param [\"Use MSA throughout\", \"Use MSA in first cycle\",\"Skip all MSA\"]\n",
        "if msa_use == \"Use MSA throughout\":\n",
        "  skip_all_msa = False \n",
        "  skip_all_msa_after_first_cycle = False\n",
        "elif msa_use == \"Use MSA in first cycle\":\n",
        "  skip_all_msa = False \n",
        "  skip_all_msa_after_first_cycle = True\n",
        "else:\n",
        "  skip_all_msa = True \n",
        "  skip_all_msa_after_first_cycle = True\n",
        "\n",
        "upload_maps = True  # This version expects a map\n",
        "\n",
        "# Save parameters\n",
        "for p in ['save_outputs_in_google_drive','input_directory',\n",
        "    'include_templates_from_pdb','maximum_templates_from_pdb',\n",
        "    'upload_manual_templates','uploaded_templates_are_map_to_model',\n",
        "    'maximum_cycles','version',\n",
        "    'upload_file_with_jobname_resolution_sequence_lines',\n",
        "    'skip_all_msa','skip_all_msa_after_first_cycle',\n",
        "    'upload_maps']:\n",
        "  params[p] = locals().get(p,None)\n",
        "\n",
        "print(\"Values of parameters:\")\n",
        "keys = sorted(list(params.keys()))\n",
        "for key in keys:\n",
        "  print(key,params[key])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Setting up input files...\n",
        "#@markdown You will be asked for permission to use your Google drive if needed.\n",
        "\n",
        "#@markdown The upload button will appear below this cell if needed\n",
        "\n",
        "# Set up the inputs using the helper python files\n",
        "from phenix_alphafold_utils import set_up_input_files\n",
        "params = set_up_input_files(params)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CKLhKPFjQI1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Installing Phenix, Alphafold and utilities...\n",
        "#@markdown This step takes 8 minutes\n",
        "\n",
        "import phenix_colab_utils as cu\n",
        "\n",
        "# Get tensorflow import before installation\n",
        "if not locals().get('tf'):\n",
        "  tf = cu.import_tensorflow()\n",
        "\n",
        "# Install selected software\n",
        "cu.install_software(\n",
        "  bioconda = True,\n",
        "  phenix = True,\n",
        "    phenix_version = params.get('version'),\n",
        "    phenix_password = params.get('password'),\n",
        "  alphafold = True,\n",
        "  pdb_to_cif = True\n",
        "    )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cXQNJKwyOqqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFNCnjI9_DUG",
        "cellView": "form"
      },
      "source": [
        "#@title 6. Creating AlphaFold models\n",
        "\n",
        "\n",
        "# STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "#standard values of parameters\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
        "num_models = 1 \n",
        "homooligomer = 1\n",
        "use_msa = (not skip_all_msa)\n",
        "use_env = True\n",
        "use_custom_msa = False\n",
        "use_amber = False \n",
        "use_templates = True\n",
        "\n",
        "# set locals from params\n",
        "for p in params.keys():\n",
        "  locals()[p] = params[p]\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "from phenix_alphafold_utils import clear_directories\n",
        "\n",
        "for x in ('mmcif_parsing','mmcif_parsing.parse'):\n",
        "  if x in locals():\n",
        "    del locals()[x]\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "import os, sys\n",
        "ready = True\n",
        "if not os.path.isfile('AF2_READY'):\n",
        "  print(\"Please load AlphaFold2 before running this cell...\")\n",
        "  ready = False\n",
        "\n",
        "\n",
        "if ready:\n",
        "  from contextlib import redirect_stderr, redirect_stdout\n",
        "  from dataclasses import dataclass, replace\n",
        "  from Bio.Seq import Seq\n",
        "  from Bio.SeqRecord import SeqRecord\n",
        "  from Bio import SeqIO\n",
        "\n",
        "\n",
        "  print(\"Setting up methods...\", end = \"\")\n",
        "# setup the model\n",
        "if ready and \"model\" not in dir():\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  import colabfold as cf\n",
        "\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "if ready:\n",
        "  from alphafold.data import mmcif_parsing\n",
        "  from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
        "                                      _process_single_hit,\n",
        "                                      _assess_hhsearch_hit,\n",
        "                                      _build_query_to_hit_index_mapping,\n",
        "                                      _extract_template_features,\n",
        "                                      SingleHitResult,\n",
        "                                      TEMPLATE_FEATURES)\n",
        "\n",
        "def mk_mock_template(query_sequence):\n",
        "  # since alphafold's model requires a template input\n",
        "  # we create a blank example w/ zero input, confidence -1\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"-\"*ln\n",
        "  output_confidence_scores = np.full(ln,-1)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
        "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
        "                       'template_sequence': [f'none'.encode()],\n",
        "                       'template_aatype': np.array(templates_aatype)[None],\n",
        "                       'template_confidence_scores': output_confidence_scores[None],\n",
        "                       'template_domain_names': [f'none'.encode()],\n",
        "                       'template_release_date': [f'none'.encode()]}\n",
        "  return template_features\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, model_params, \n",
        "  use_model,\n",
        "  model_runner_1,\n",
        "  model_runner_3,\n",
        "  do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  L_prev = 0\n",
        "  # Ls: number of residues in each chain\n",
        "  for L_i in Ls[:-1]:\n",
        "      idx_res[L_prev+L_i:] += 200\n",
        "      L_prev += L_i  \n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "  feature_dict['residue_index'] = idx_res\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "      \n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      prediction_result = model_runner.predict(processed_feature_dict)\n",
        "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'])\n",
        "      paes.append(prediction_result['predicted_aligned_error'])\n",
        "\n",
        "    \n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  print(\"reranking models based on avg. predicted lDDT\")\n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    if do_relax:\n",
        "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
        "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
        "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
        "  \"\"\"\n",
        "  This is a hack to get hhsuite output strings to pass on\n",
        "  to the AlphaFold template featurizer. \n",
        "  \n",
        "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
        "  all the templates. Currently it builds a database with only one template at a time. Even \n",
        "  better would be to get an hhsuite alignment without using a database at all, just between\n",
        "  pairs of sequence files. However, I have not figured out how to do this.\n",
        "\n",
        "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
        "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
        "  \"\"\"\n",
        "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
        "  if hhDB_dir.exists():\n",
        "    shutil.rmtree(hhDB_dir)\n",
        "  \n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  msa_dir.mkdir(parents=True)\n",
        "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
        "  with template_seq_path.open(\"w\") as fh:\n",
        "    SeqIO.write([template_seq], fh, \"fasta\")\n",
        "  print(\"MSA DIR\",msa_dir)\n",
        "  # make hhsuite DB\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    os.chdir(msa_dir)\n",
        "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
        "    os.chdir(hhDB_dir)\n",
        "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
        "    %shell rm DB_msa.ff{data,index}\n",
        "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
        "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
        "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
        "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
        "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
        "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
        "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
        "    os.chdir(\"/content/\")\n",
        "\n",
        "  # run hhsearch\n",
        "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\",\n",
        "      databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
        "  with StringIO() as fh:\n",
        "    SeqIO.write([query_seq], fh, \"fasta\")\n",
        "    seq_fasta = fh.getvalue()\n",
        "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
        "\n",
        "  # process hits\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  if len(hhsearch_hits) >0:\n",
        "    hit = hhsearch_hits[0]\n",
        "    hit = replace(hit,**{\"name\":template_seq.id})\n",
        "  else:\n",
        "    hit = None\n",
        "  return hit\n",
        "\n",
        "def plot_plddt_legend():\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=100)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "def plot_confidence(outs, model_num=1):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  plt.figure(figsize=(10,3),dpi=100)\n",
        "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
        "  #########################################\n",
        "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
        "  plt.plot(outs[model_name][\"plddt\"])\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence))\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  #########################################\n",
        "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
        "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "  #########################################\n",
        "  return plt\n",
        "\n",
        "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "\n",
        "  pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
        "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "def write_pae_file(pae_matrix, file_name):\n",
        "  shape=tuple(pae_matrix.shape)\n",
        "  n,n = shape\n",
        "  # Write out array to text file as json\n",
        "  residues_1 = []\n",
        "  residues_2 = []\n",
        "  distances = []\n",
        "  for i in range(n):\n",
        "    ii = i + 1\n",
        "    for j in range(n):\n",
        "      jj= j + 1\n",
        "      residues_1.append(ii)\n",
        "      residues_2.append(jj)\n",
        "      distances.append(float(\"%.2f\" %(pae_matrix[i][j])))\n",
        "\n",
        "  residue_dict = {\"residue1\":residues_1,\n",
        "                   \"residue2\":residues_2,\n",
        "                  \"distance\":distances,\n",
        "                  \"max_predicted_aligned_error\":0}\n",
        "  values = [residue_dict]\n",
        "  text = str(values).replace(\" \",\"\").replace(\"'\",'\"')\n",
        "\n",
        "  f = open(file_name, 'w')\n",
        "  print(text, file = f)\n",
        "  f.close()\n",
        "  print(\"Wrote pae file to %s\" %(file_name))\n",
        "\n",
        "\n",
        "def get_msa(\n",
        "      query_sequence, jobname, use_env,\n",
        "      use_templates,\n",
        "      homooligomer,\n",
        "      use_msa):\n",
        "  template_paths = None # initialize\n",
        "\n",
        "  #@title Get MSA and templates\n",
        "  print(\"Getting MSA and templates...\")\n",
        "  if use_templates:\n",
        "    a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "  elif use_msa:\n",
        "    a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "\n",
        "\n",
        "\n",
        "  if (not use_msa):\n",
        "    a3m_lines = \">query sequence \\n%s\" %(query_sequence)\n",
        "    print(\"Not using any MSA information\")\n",
        "  \n",
        "  # File for a3m\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "      text_file.write(a3m_lines)\n",
        "\n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "      \n",
        "  print(\"Done with MSA and templates\")\n",
        "  return msa, deletion_matrix, template_paths\n",
        "\n",
        "def get_cif_file_list(include_pdb = None,\n",
        "    manual_templates_uploaded = None,\n",
        "    cif_dir = None,\n",
        "    other_cif_dir = None):\n",
        "  \n",
        "  if cif_dir is not None:\n",
        "    cif_files = list(cif_dir.glob(\"*\"))\n",
        "  else:\n",
        "    cif_files = []\n",
        "  # Only include the cif_files in manual_templates_uploaded\n",
        "  manual_files_as_text = []\n",
        "  if not manual_templates_uploaded:\n",
        "    manual_templates_uploaded = []\n",
        "  for f in manual_templates_uploaded:\n",
        "    manual_files_as_text.append(\n",
        "        os.path.split(str(f))[-1])\n",
        "  cif_files_to_include = []\n",
        "  for cif_file in cif_files:\n",
        "    text = os.path.split(str(cif_file))[-1]\n",
        "    if text in manual_files_as_text:\n",
        "      cif_files_to_include.append(cif_file)\n",
        "  cif_files = cif_files_to_include\n",
        "      \n",
        "  if include_templates_from_pdb and other_cif_dir is not None:\n",
        "    other_cif_files = []\n",
        "    for file_name in list(other_cif_dir.glob(\"*\")):\n",
        "      if str(file_name).endswith(\".cif\"):\n",
        "        other_cif_files.append(file_name)\n",
        "    cif_files += other_cif_files\n",
        "\n",
        "  return cif_files\n",
        "\n",
        "def get_template_hit_list(cif_files = None, fasta_dir = None,\n",
        "    query_seq = None,\n",
        "    hhDB_dir = None):\n",
        "  template_hit_list = []\n",
        "  for i,filepath in enumerate(cif_files):\n",
        "    if not str(filepath).endswith(\".cif\"): continue\n",
        "    print(\"CIF file included:\",i+1,str(filepath))\n",
        "    with filepath.open(\"r\") as fh:\n",
        "      filestr = fh.read()\n",
        "      mmcif_obj = mmcif_parsing.parse(file_id=filepath.stem,mmcif_string=filestr)\n",
        "      mmcif = mmcif_obj.mmcif_object\n",
        "      if not mmcif: \n",
        "        print(\"...No CIF object obtained...skipping...\")\n",
        "        continue\n",
        "  \n",
        "      for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
        "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
        "        seq_name = filepath.stem.upper()+\"_\"+chain_id\n",
        "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
        "  \n",
        "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
        "          SeqIO.write([seq], fh, \"fasta\")\n",
        "  \n",
        "        \"\"\"\n",
        "        At this stage, we have a template sequence.\n",
        "        and a query sequence. \n",
        "        There are two options to generate template features:\n",
        "          1. Write new code to manually generate template features\n",
        "          2. Get an hhr alignment string, and pass that\n",
        "            to the existing template featurizer. \n",
        "            \n",
        "        I chose the second, implemented in hh_process_seq()\n",
        "        \"\"\"\n",
        "        SeqIO.write([seq], sys.stdout, \"fasta\")\n",
        "        SeqIO.write([query_seq], sys.stdout, \"fasta\")\n",
        "        try:\n",
        "          hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
        "        except Exception as e:\n",
        "          print(\"Failed to process %s\" %(filepath),e)\n",
        "          hit = None\n",
        "        if hit is not None:\n",
        "          template_hit_list.append([hit,mmcif])\n",
        "          print(\"Template %s included\" %(filepath))\n",
        "        else:\n",
        "          print(\"Template %s not included (failed to process)\" %(filepath))\n",
        "\n",
        "  return template_hit_list\n",
        "  \n",
        "def run_one_cycle(cycle, template_hit_list,\n",
        "        query_sequence,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        num_models,\n",
        "        msa, deletion_matrix, template_paths,\n",
        "        mtm_file_name,\n",
        "        cif_dir,\n",
        "        output_directory):\n",
        "\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  if template_hit_list:\n",
        "    #process hits into template features\n",
        "    template_hit_list = [[replace(hit,**{\"index\":i+1}),mmcif] for i,[hit,mmcif] in enumerate(template_hit_list)]\n",
        "  \n",
        "    template_features = {}\n",
        "    for template_feature_name in TEMPLATE_FEATURES:\n",
        "      template_features[template_feature_name] = []\n",
        "  \n",
        "    for i,[hit,mmcif] in enumerate(sorted(template_hit_list, key=lambda xx: xx[0].sum_probs, reverse=True)):\n",
        "      # modifications to alphafold/data/templates.py _process_single_hit\n",
        "      hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
        "      mapping = _build_query_to_hit_index_mapping(\n",
        "      hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
        "      query_sequence)\n",
        "      template_sequence = hit.hit_sequence.replace('-', '')\n",
        "  \n",
        "      try:\n",
        "        features, realign_warning = _extract_template_features(\n",
        "          mmcif_object=mmcif,\n",
        "          pdb_id=hit_pdb_code,\n",
        "          mapping=mapping,\n",
        "          template_sequence=template_sequence,\n",
        "          query_sequence=query_sequence,\n",
        "          template_chain_id=hit_chain_id,\n",
        "          kalign_binary_path=\"kalign\")\n",
        "      except Exception as e:\n",
        "        continue\n",
        "      features['template_sum_probs'] = [hit.sum_probs]\n",
        "      single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
        "      for k in template_features:\n",
        "        template_features[k].append(features[k])\n",
        "\n",
        "    for name in template_features:\n",
        "      template_features[name] = np.stack(\n",
        "          template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
        "    #overwrite template data\n",
        "    template_paths = cif_dir.as_posix()\n",
        "\n",
        "\n",
        "    # Select only one chain from any cif file\n",
        "    unique_template_hits = []\n",
        "    pdb_text_list = []\n",
        "    for hit, mmcif in template_hit_list:\n",
        "      pdb_text = hit.name.split()[0].split(\"_\")[0]\n",
        "      if not pdb_text in pdb_text_list:\n",
        "        pdb_text_list.append(pdb_text)\n",
        "        unique_template_hits.append(hit)\n",
        "    template_hits = unique_template_hits\n",
        "\n",
        "    print(\"\\nIncluding templates:\")\n",
        "    for hit,mmcif in template_hit_list:\n",
        "      print(\"\\t\",hit.name.split()[0])\n",
        "    if len(template_hit_list) == 0:\n",
        "      print(\"No templates found...quitting\")\n",
        "      raise AssertionError(\"No templates found...quitting\")\n",
        "\n",
        "  \n",
        "    for key,value in template_features.items():\n",
        "      if np.all(value==0):\n",
        "        print(\"ERROR: Some template features are empty\")\n",
        "  else:  # no templates\n",
        "    print(\"Not using any templates\")\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "\n",
        "  print(\"\\nPREDICTING STRUCTURE\")\n",
        "  \n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  model_params = {}\n",
        "  model_runner_1 = None\n",
        "  model_runner_3 = None\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "    if model_name not in list(model_params.keys()):\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "  if homooligomer == 1:\n",
        "    msas = [msa]\n",
        "    deletion_matrices = [deletion_matrix]\n",
        "  else:\n",
        "    # make multiple copies of msa for each copy\n",
        "    # AAA------\n",
        "    # ---AAA---\n",
        "    # ------AAA\n",
        "    #\n",
        "    # note: if you concat the sequences (as below), it does NOT work\n",
        "    # AAAAAAAAA\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    Ln = len(query_sequence)\n",
        "    for o in range(homooligomer):\n",
        "      L = Ln * o\n",
        "      R = Ln * (homooligomer-(o+1))\n",
        "      msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
        "      deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
        "  \n",
        "  # gather features\n",
        "  feature_dict = {\n",
        "      **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
        "                                        description=\"none\",\n",
        "                                        num_res=len(query_sequence)*homooligomer),\n",
        "      **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "      **template_features\n",
        "  }\n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)]*homooligomer,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           model_runner_1=model_runner_1,\n",
        "                           model_runner_3=model_runner_3,\n",
        "                           do_relax=False)\n",
        "  print(\"DONE WITH STRUCTURE in\",os.getcwd())\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  print(os.listdir(\".\"))\n",
        "  model_file_name = \"%s_unrelaxed_model_1.pdb\" %(jobname)\n",
        "  if os.path.isfile(model_file_name):\n",
        "    print(\"Model file is in %s\" %(model_file_name))\n",
        "    cycle_model_file_name = \"%s_unrelaxed_model_1_%s.pdb\" %(jobname, cycle)\n",
        "    shutil.copyfile(model_file_name,cycle_model_file_name)\n",
        "    if output_directory is not None:\n",
        "      shutil.copyfile(model_file_name,os.path.join(output_directory, cycle_model_file_name))\n",
        "  else:\n",
        "    print(\"No model file %s found for job %s\" %(model_file_name, jobname))\n",
        "    cycle_model_file_name = None\n",
        "  \n",
        "  #@title Making plots...\n",
        "  \n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "  ##################################################################\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Predicted Alignment Error\")\n",
        "  ##################################################################\n",
        "  pae_file_list = []\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "    # Write pae file\n",
        "    pae_file = jobname+\"_\"+model_name+\"_PAE.jsn\"\n",
        "    write_pae_file(value[\"pae\"], pae_file)\n",
        "    pae_file_list.append(pae_file)\n",
        "  plt.savefig(jobname+\"_PAE.png\")\n",
        "  plt.show()\n",
        "  ##################################################################\n",
        "  #@title Displaying 3D structure... {run: \"auto\"}\n",
        "  model_num = 1 \n",
        "  color = \"lDDT\" \n",
        "  show_sidechains = False \n",
        "  show_mainchains = False \n",
        "  \n",
        "  \n",
        "  \n",
        "  show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
        "  if color == \"lDDT\": plot_plddt_legend().show()  \n",
        "  plot_confidence(outs, model_num).show()\n",
        "  #@title Packaging and downloading results...\n",
        "  \n",
        "  #@markdown When modeling is complete .zip files with results will be downloaded automatically.\n",
        "  \n",
        "  citations = {\n",
        "  \"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "  author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "  doi = {10.1101/2021.08.15.456425},\n",
        "  journal = {bioRxiv},\n",
        "  title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "  year = {2021},\n",
        "  comment = {ColabFold including MMseqs2 MSA server}\n",
        "  }\"\"\",\n",
        "    \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "  doi = {10.1093/nar/gkz1035},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "  year = {2019},\n",
        "  comment = {MGnify database}\n",
        "  }\"\"\",\n",
        "    \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "  doi = {10.1038/s41586-021-03819-2},\n",
        "  journal = {Nature},\n",
        "  pmid = {34265844},\n",
        "  title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "  year = {2021},\n",
        "  comment = {AlphaFold2 + BFD Database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "  author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1093/bioinformatics/bty1057},\n",
        "  journal = {Bioinformatics},\n",
        "  number = {16},\n",
        "  pages = {2856--2858},\n",
        "  pmid = {30615063},\n",
        "  title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "  volume = {35},\n",
        "  year = {2019},\n",
        "  comment = {MMseqs2 search server}\n",
        "  }\"\"\",\n",
        "    \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "  author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1186/s12859-019-3019-7},\n",
        "  journal = {BMC Bioinform.},\n",
        "  number = {1},\n",
        "  pages = {473},\n",
        "  pmid = {31521110},\n",
        "  title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "  volume = {20},\n",
        "  year = {2019},\n",
        "  comment = {PDB70 database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "  author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "  doi = {10.1093/nar/gkw1081},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  number = {D1},\n",
        "  pages = {D170--D176},\n",
        "  pmid = {27899574},\n",
        "  title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "  volume = {45},\n",
        "  year = {2017},\n",
        "  comment = {Uniclust30/UniRef30 database},\n",
        "  }\"\"\",\n",
        "    \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "  booktitle = {Nat. Struct. Biol.},\n",
        "  doi = {10.1038/nsb1203-980},\n",
        "  number = {12},\n",
        "  pages = {980},\n",
        "  pmid = {14634627},\n",
        "  title = {{Announcing the worldwide Protein Data Bank}},\n",
        "  volume = {10},\n",
        "  year = {2003},\n",
        "  comment = {templates downloaded from wwPDB server}\n",
        "  }\"\"\",\n",
        "  }\n",
        "  \n",
        "  to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "  if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "  if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "  if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "  if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "  if use_templates: to_cite += [\"Berman2003\"]\n",
        "  \n",
        "  with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
        "    for i in to_cite:\n",
        "      writer.write(citations[i])\n",
        "      writer.write(\"\\n\")\n",
        "  \n",
        "  print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
        "  if use_custom_msa:\n",
        "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "  \n",
        "\n",
        "\n",
        "  return cycle_model_file_name\n",
        "\n",
        "def get_map_to_model(map_file_name,\n",
        "    resolution,\n",
        "    seq_file,\n",
        "    output_file_name = None,\n",
        "    nproc = 4):\n",
        "    \n",
        "\n",
        "  !phenix.map_to_model nproc=$nproc seq_file=$seq_file resolution=$resolution $map_file_name pdb_out=$output_file_name\n",
        "  return output_file_name\n",
        "\n",
        "def run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        uploaded_templates_are_map_to_model,\n",
        "        output_directory,\n",
        "        skip_all_msa_after_first_cycle):\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  \n",
        "  #Get the MSA\n",
        "  msa, deletion_matrix, template_paths = get_msa(\n",
        "      query_sequence, jobname, use_env,\n",
        "      use_templates,\n",
        "      homooligomer,\n",
        "      use_msa)\n",
        "  \n",
        "  #Process templates\n",
        "  print(\"PROCESSING TEMPLATES\")\n",
        "  \n",
        "  other_cif_dir = Path(\"/content/%s\" %(template_paths))\n",
        "  parent_dir = Path(\"/content/manual_templates\")\n",
        "  cif_dir = Path(parent_dir,\"mmcif\")\n",
        "  fasta_dir = Path(parent_dir,\"fasta\")\n",
        "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  clear_directories([fasta_dir,hhDB_dir,msa_dir])\n",
        "  \n",
        "  if uploaded_templates_are_map_to_model and \\\n",
        "      manual_templates_uploaded: # mtm\n",
        "    print(\"Uploaded tempates are map to model\")\n",
        "    mtm_file_name = manual_templates_uploaded[0]\n",
        "    manual_templates_uploaded = []\n",
        "  else:\n",
        "    print(\"Uploaded templates are actual templates\")\n",
        "    mtm_file_name = \"None\"\n",
        "\n",
        "  pdb_cif_file_list = get_cif_file_list(\n",
        "    include_pdb = True,\n",
        "    manual_templates_uploaded = None,\n",
        "    cif_dir = cif_dir,\n",
        "    other_cif_dir = other_cif_dir)\n",
        "  print(\"CIF files from PDB to include:\",pdb_cif_file_list)\n",
        "  \n",
        "  manual_cif_file_list = get_cif_file_list(\n",
        "    include_pdb = False,\n",
        "    manual_templates_uploaded = manual_templates_uploaded,\n",
        "    cif_dir = cif_dir)\n",
        "  print(\"Uploaded CIF files to include:\",manual_cif_file_list)\n",
        "    \n",
        "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",\n",
        "    name=\"\",description=\"\")\n",
        "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
        "  with query_seq_path.open(\"w\") as fh:\n",
        "      SeqIO.write([query_seq], fh, \"fasta\")\n",
        "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
        "\n",
        "  previous_final_model_name = manual_templates_uploaded[0] if \\\n",
        "      manual_templates_uploaded else None\n",
        "\n",
        "  assert len(maps_uploaded) == 1  # just one map\n",
        "  map_file_name = maps_uploaded[0]\n",
        "  \n",
        "  seq_file = \"%s.seq\" %(jobname)\n",
        "  ff = open(seq_file,'w')\n",
        "  print(query_sequence, file = ff)\n",
        "  ff.close()\n",
        "\n",
        "  # Run first cycle\n",
        "\n",
        "  for cycle in range(1, maximum_cycles + 1):\n",
        "    print(\"\\nStarting cycle %s\" %(cycle))\n",
        "\n",
        "    if cycle == 2 and skip_all_msa_after_first_cycle:\n",
        "      print(\"Getting dummy msa for cycles after the first\")\n",
        "      #Get dummy msa\n",
        "      use_msa = False\n",
        "      msa, deletion_matrix, template_paths = get_msa(\n",
        "        query_sequence, jobname, use_env,\n",
        "        use_templates,\n",
        "        homooligomer,\n",
        "        use_msa)\n",
        "      \n",
        "\n",
        "    working_cif_file_list = \\\n",
        "     list(manual_cif_file_list) + \\\n",
        "     list(pdb_cif_file_list)[:maximum_templates_from_pdb]\n",
        "\n",
        "    print(\"Templates used in this cycle: %s\" %(\n",
        "        \" \".join([w.as_posix() for w in working_cif_file_list])))\n",
        "\n",
        "    template_hit_list = get_template_hit_list(\n",
        "      cif_files = working_cif_file_list,\n",
        "      fasta_dir = fasta_dir,\n",
        "      query_seq = query_seq,\n",
        "      hhDB_dir = hhDB_dir)  \n",
        "\n",
        "    os.chdir(\"/content/\")\n",
        "    \n",
        "    cycle_model_file_name = run_one_cycle(\n",
        "        cycle, template_hit_list,\n",
        "        query_sequence,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        num_models,\n",
        "        msa, deletion_matrix, template_paths,\n",
        "        mtm_file_name,\n",
        "        cif_dir,\n",
        "        output_directory)\n",
        "\n",
        "    cycle_model_file_name = Path(cycle_model_file_name)\n",
        "\n",
        "   \n",
        "    print(\"\\nFinished with cycle %s of AlphaFold model generation\" %(cycle))\n",
        "    if not os.path.isfile(cycle_model_file_name.as_posix()):\n",
        "      print(\"No AlphaFold model obtained...quitting\")\n",
        "      return None\n",
        "    print(\"Current AlphaFold model is in %s\" %(\n",
        "        cycle_model_file_name.as_posix()))\n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "    print(\"\\nGetting a new rebuilt model at a resolution of %.2f A\" %(\n",
        "        resolution))\n",
        "    # Now get a new rebuilt model\n",
        "    final_model_file_name = rebuild_model(\n",
        "        cycle_model_file_name,\n",
        "        previous_final_model_name,\n",
        "        mtm_file_name,\n",
        "        cycle,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        resolution)\n",
        "    \n",
        "    try:\n",
        "      !zip -FSr $jobname\".result.zip\"  $jobname*.pdb $jobname*.j* $jobname*.png $jobname*.bibtex $jobname*.jsn\n",
        "      zip_file_name = f\"{jobname}.result.zip\"\n",
        "    except Exception as e:\n",
        "      zip_file_name = None\n",
        "\n",
        "    if not final_model_file_name:\n",
        "      print(\"\\nEnding cycles as no rebuilt model obtained\")\n",
        "      break\n",
        "\n",
        "    # now update template_hit_list\n",
        "    final_model_file_name = Path(final_model_file_name)\n",
        "\n",
        "    final_model_file_name_in_cif_dir = Path(\n",
        "        os.path.join(cif_dir,final_model_file_name.name))\n",
        "    shutil.copyfile(\n",
        "      final_model_file_name,\n",
        "      final_model_file_name_in_cif_dir)\n",
        "    \n",
        "    if output_directory is not None:\n",
        "      final_model_file_name_in_output_dir = Path(\n",
        "        os.path.join(output_directory,final_model_file_name.name))\n",
        "      shutil.copyfile(\n",
        "        final_model_file_name,\n",
        "        final_model_file_name_in_output_dir)\n",
        "      print(\"Copied rebuilt model to %s\" %(\n",
        "          final_model_file_name_in_output_dir))\n",
        "    from phenix_colab_utils import run_pdb_to_cif\n",
        "    final_model_file_name_as_cif_in_cif_dir = run_pdb_to_cif(\n",
        "       final_model_file_name_in_cif_dir)\n",
        "    manual_cif_file_list = get_cif_file_list(\n",
        "      include_pdb = False,\n",
        "      manual_templates_uploaded = [final_model_file_name_as_cif_in_cif_dir.name],\n",
        "      cif_dir = cif_dir)\n",
        "    previous_final_model_name = final_model_file_name\n",
        "\n",
        "  filename = zip_file_name\n",
        "  if filename and os.path.isfile(filename):\n",
        "    print(\"About to download %s\" %(filename))\n",
        "  \n",
        "    try:\n",
        "      print(\"Downloading zip file %s\" %(filename))\n",
        "      files.download(filename)\n",
        "      print(\"Start of download successful (NOTE: if the download symbol does not go away it did not work. Download it manually using the folder icon to the left)\")\n",
        "      return filename\n",
        "    except Exception as e:\n",
        "      print(\"Unable to download zip file %s\" %(filename))\n",
        "      return None\n",
        "  else:\n",
        "    print(\"No .zip file %s created\" %(filename))\n",
        "    return None\n",
        "\n",
        "def rebuild_model(\n",
        "        cycle_model_file_name,\n",
        "        previous_final_model_name,\n",
        "        mtm_file_name,\n",
        "        cycle,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        resolution,\n",
        "        nproc = 4):\n",
        "  assert len(maps_uploaded) == 1  # just one map\n",
        "  map_file_name = maps_uploaded[0]\n",
        "  af_model_file = os.path.abspath(\n",
        "      cycle_model_file_name.as_posix())\n",
        "  if previous_final_model_name:\n",
        "    previous_model_file = os.path.abspath(\n",
        "      previous_final_model_name.as_posix())\n",
        "  else:\n",
        "    previous_model_file = None\n",
        "  output_file_name = os.path.abspath(\n",
        "      \"%s_rebuilt_%s.pdb\" %(jobname,cycle))\n",
        "  print(\"Rebuilding %s %s with map in %s at resolution of %.2f\" %(\n",
        "       af_model_file,\n",
        "        \" with previous model of %s\" %(previous_model_file) \\\n",
        "        if previous_model_file else \"\",\n",
        "       map_file_name,\n",
        "      resolution,))\n",
        "\n",
        "  for ff in (map_file_name,af_model_file, previous_model_file):\n",
        "    if ff and not os.path.isfile(ff):\n",
        "      print(\"\\nMissing the file: %s\" %(ff))\n",
        "      return None\n",
        "\n",
        "  rebuilt_model_name = af_model_file.replace(\".pdb\",\"_rebuilt.pdb\")\n",
        "  rebuilt_model_stem = rebuilt_model_name.replace(\".pdb\",\"\")\n",
        "\n",
        "  # run phenix dock_and_rebuild here\n",
        "  !phenix.dock_and_rebuild fragments_model_file=$mtm_file_name nproc=$nproc resolution=$resolution previous_model_file=$previous_model_file model=$af_model_file full_map=$map_file_name output_model_prefix=$rebuilt_model_stem\n",
        "\n",
        "  if os.path.isfile(rebuilt_model_name):\n",
        "    print(\"Rebuilding successful\")\n",
        "    return rebuilt_model_name\n",
        "  else:\n",
        "    print(\"Rebuilding not successful\")\n",
        "    return None\n",
        "\n",
        "if ready:\n",
        "  # RUN THE JOBS HERE\n",
        "\n",
        "  for query_sequence, jobname, resolution in zip(query_sequences, jobnames, resolutions):\n",
        "    print(\"\\n\",\"****************************************\",\"\\n\",\n",
        "         \"RUNNING JOB %s with sequence %s at resolution of %s\\n\" %(\n",
        "      jobname, query_sequence, resolution),\n",
        "      \"****************************************\",\"\\n\")\n",
        "    # GET TEMPLATES AND SET UP FILES\n",
        "\n",
        "    # User input of manual templates\n",
        "    manual_templates_uploaded = cif_filename_dict.get(\n",
        "      jobname,[])\n",
        "    if manual_templates_uploaded:\n",
        "      print(\"Using uploaded templates %s for this run\" %(\n",
        "          manual_templates_uploaded))\n",
        "    maps_uploaded = map_filename_dict.get(\n",
        "      jobname,[])\n",
        "    if maps_uploaded:\n",
        "      print(\"Using uploaded maps %s for this run\" %(\n",
        "          maps_uploaded))\n",
        "      assert len(maps_uploaded) == 1\n",
        "\n",
        "    try:\n",
        "      filename = run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb,\n",
        "        uploaded_templates_are_map_to_model,\n",
        "        output_directory,\n",
        "        skip_all_msa_after_first_cycle)\n",
        "      if filename:\n",
        "        print(\"FINISHED JOB (%s) %s with sequence %s\\n\" %(\n",
        "        filename, jobname, query_sequence),\n",
        "        \"****************************************\",\"\\n\")\n",
        "      else:\n",
        "        print(\"NO RESULT FOR JOB %s with sequence %s\\n\" %(\n",
        "      jobname, query_sequence),\n",
        "      \"****************************************\",\"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"FAILED: JOB %s with sequence %s\\n\\n%s\\n\" %(\n",
        "      jobname, query_sequence, str(e)),\n",
        "      \"****************************************\",\"\\n\")\n",
        "\n",
        "\n",
        "  print(\"\\nDOWNLOADING FILES NOW:\\n\")\n",
        "  for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "    filename = f\"{jobname}.result.zip\"\n",
        "    if os.path.isfile(filename):\n",
        "      print(filename)\n",
        "\n",
        "  print(\"\\nALL DONE\\n\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities (skipped unless checked)\n",
        "\n",
        "# Put whatever utilities you want here. They will be run if checked\n",
        "clear_caches = False #@param {type:\"boolean\" }\n",
        "if clear_caches:\n",
        "  from phenix_colab_utils import clear_python_caches\n",
        "  clear_python_caches(modules = ['get_templates_from_drive','phenix_alphafold_utils','phenix_colab_utils','clear_python_caches'])\n",
        "  from phenix_colab_utils import clear_python_caches\n",
        "  clear_caches = False\n",
        "  clear_python_caches()\n",
        "\n",
        "\n",
        "crash_deliberately_and_restart = False #@param {type:\"boolean\" }\n",
        "if crash_deliberately_and_restart:\n",
        "  print(\"Crashing by using all memory.  Results in restart, losing everything\")\n",
        "    crash_deliberately_and_restart = False\n",
        "    [1]*10**10\n",
        "\n"
      ],
      "metadata": {
        "id": "cBbp-KJ507K0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "**Helpful hints**\n",
        "\n",
        "**Password**\n",
        "* Your Phenix download password is the password you get from <a href = \"https://phenix-online.org/download\" target=\"_blank\"> phenix_online.org/download </a> and that you (or someone from your institution) used to download Phenix. It is updated weekly so you may need to request a new one rather frequently.\n",
        "\n",
        "**Saving your results**\n",
        "\n",
        "* You might want to download your results as they appear.  Go to the Folder icon on the left, click on the 3 dots to the right of your file and select \"Download\".\n",
        "\n",
        "* If you specify a Google drive input_directory (maybe \"ColabInputs\"), then your output files will be saved as they are created in a directory called ColabOutputs in your Google drive.\n",
        "\n",
        "**Sequence format**\n",
        "\n",
        "* Your sequence should contain only the 1-letter code of one protein chain. It can contain spaces if you want.\n",
        "\n",
        "**File names and jobname must match**\n",
        "* Your AlphaFold predictions will be named yyyy_unrelaxed_model_1_x.pdb\n",
        "and your rebuilt models yyyy_unrelaxed_model_1_x_rebuilt.pdb, where yyyy is your jobname and x is the cycle number.\n",
        "\n",
        "* All model file names must start with 4 characters, optionally followed by \"_\" and more characters, and must end in \".pdb\" or \".cif\",  Valid file names are abcd.pdb, abcd.cif, abcd_other.pdb.  Non-valid names are abc.pdb, abcde.cif.\n",
        "\n",
        "* Your jobname must match the beginnings of your map file names and model file names.  If your jobname is joba then your map file name must look like: joba_xxx.mrc or joba_yyy.ccp4.  Your model file name must look like: joba_mymodel.pdb or joba.cif.  This correspondence is used to match map and model files with jobnames.\n",
        "\n",
        "**Options for uploading your map file**\n",
        "\n",
        "* (A) Upload when the Upload button appears at the bottom of the cell after you hit Runtime / Run all in step 3\n",
        "* (B) Upload in advance to a unique folder in your Google Drive and specify this directory in the entry form.\n",
        "* (C) as in B but upload to a unique new folder in /content/.  Note that C requires using the command-line tool at the bottom left of the page to create a new directory like MyFiles, uploading with the upload button near the top left of the page, and moving the uploaded file from /content/my_file.mrc to /content/MyFiles/my_file.mrc.\n",
        "\n",
        "**Uploading a file with all your file information**\n",
        "\n",
        "* To upload\n",
        "a file with a jobname, resolution, and sequence on each line, \n",
        "check ***upload_file_with_jobname_resolution_sequence_lines*** and hit\n",
        "the ***Run*** button to the left of the first cell.\n",
        "\n",
        "* If you upload a file with multiple sequences, each line of the file should have exactly one job name, a space, resolution, and a sequence, like this:\n",
        "\n",
        "7n8i_24237 2.3 VIWMTQSPSSLSASVGDRVTITCQASQDIRFYLNWYQQKPGKAPKLLISDASNMETGVPSRFSGS\n",
        "\n",
        "7lvr_23541 3 MRECISIHVGQAGVQIGNACWELYCLEHGIQPDGQMPSDKTIGGGDDSFNTFFSETG\n",
        "\n",
        "\n",
        "**Try turning off MSA's after first cycle**\n",
        "\n",
        "* You can encourage AlphaFold to use your rebuilt templates by specifying skip_all_msa_after_first_cycle. This will just use your template information and intrinsic structural information in AlphaFold for all cycles except the first.\n",
        "\n",
        "**Running cells in this Colab notebook**\n",
        "* You can step through this notebook one part at a time\n",
        "by hitting the ***Run*** buttons to the left one at a time. \n",
        "\n",
        "* The cell that is active is indicated by a ***Run*** button that has turned into a black circle with a moving black arc\n",
        "\n",
        "* When execution is done, the ***Run*** button will go back \n",
        "to its original white triangle inside a black circle\n",
        "\n",
        "* You can stop execution of the active cell by hitting its ***Run*** button. It will turn red to indicate it has stopped.\n",
        "\n",
        "* You can rerun any cell any time that nothing is running.  That means you can go all the way through, then go back to the first cell and enter another sequence and redo the procedure.\n",
        "\n",
        "* If something goes wrong, the Colab Notebook will print out\n",
        "an error message.  Usually this will be something telling you\n",
        "how to change your inputs.  You enter your new inputs and\n",
        "hit the ***Run*** button again to carry on.\n",
        "\n",
        "**Possible problems**\n",
        "\n",
        "* The automatic download may not always work. Normally the\n",
        "file download starts when the .zip files are created,\n",
        "but the actual download happens when all the AlphaFold\n",
        "models are completed.\n",
        "You can click on the \n",
        "folder icon to the left of the window and download your\n",
        "jobname.zip file manually.  Open and close the file\n",
        "broswer to show recently-added files.\n",
        "\n",
        "* Your Colab connection may time out if you go away and\n",
        "leave it, or if you run for a long time (more than an hour).\n",
        "If your connection times out you lose everything that\n",
        "is not yet downloaded. So you might want to download as you go or specify a Google drive input directory.\n",
        "\n",
        "* The zip file or files will not be automatically downloaded until the very end of the job. \n",
        "\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.  \n",
        "\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structure\n",
        "2. Plot of the model quality (IDDT).\n",
        "3. Plots of the MSA coverage.\n",
        "4. A3M formatted input MSA.\n",
        "5. BibTeX file with citations for all used tools and databases.\n",
        "6. JSN file with predicted error matrix (PAE matrix)\n",
        "\n",
        "At the end of the job the `jobname.result.zip` file or files will be downloaded automatically.\n",
        "\n",
        "\n",
        "**Colab limitations**\n",
        "* While Colab is free, it is designed for interactive work and not-unlimited memory and GPU usage. It will time-out after a few hours and it may check that you are not a robot at random times.  On a time-out you may lose your work. You can increase your allowed time with Colab+\n",
        "\n",
        "* AlphaFold can crash if it requires too much memory. On a crash you may lose all your work that is not yet downloaded. You can have more memory accessible if you have Colab+. If you are familiar with Colab scripts you can try this [hack](https://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-efa636e646ff ) with the <b>crash_deliberately_and_restart</b> check-off in the Utilities section to increase your memory allowance.\n",
        "\n",
        "\n",
        "**Description of the plots**\n",
        "\n",
        "*   **Number of sequences per position** - Look for at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Updates**\n",
        "\n",
        "- <b> <font color='green'>2022-01-25 Includes integrated rebuilding and AlphaFold2 modeling\n",
        "\n",
        "\n",
        "**Acknowledgments**\n",
        "\n",
        "- <b> <font color='green'>This notebook is based on the very nice notebook from ColabFold ([Mirdita et al., *bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1), https://github.com/sokrypton/ColabFold)</font></b> \n",
        "\n",
        "- <b><font color='green'>ColabFold is based on AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font></b>"
      ]
    }
  ]
}