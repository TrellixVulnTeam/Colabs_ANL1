{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Working.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/Working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn1r2dn6P2uq"
   },
   "source": [
    "### <center> <b> <font color='black'>  AlphaFold with choice of templates (Phenix version) </font></b> </center>\n",
    "\n",
    "<font color='green'>AlphaFold can use templates as hypotheses about the structure. This version allows you run AlphaFold with models from the PDB or models you upload. You can run multiple sequences as well.\n",
    "This notebook is derived from [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) and the DeepMind [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb).\n",
    "</font>\n",
    "\n",
    "-----------------\n",
    "<b> <font color='black'> <center>Instructions:</center>\n",
    "</font></b> \n",
    "\n",
    "1.  Type in a sequence and jobname in the form below and hit the <b><i>Run</i></b> button next to it (little white triangle in black circle) to load this info. You can choose whether and how you want to supply templates\n",
    "\n",
    "2. Load AlphaFold with the <b><i>Run</i></b> button in the next cell down...takes a couple minutes\n",
    "\n",
    "3. Run AlphaFold with the <b><i>Run</i></b> button in the last cell...takes 10-20 minutes\n",
    "</font>\n",
    "\n",
    "\n",
    "Your model information should download automatically as a zip file at the very end of the third step (when all jobs are done).\n",
    "</font>\n",
    "-----------------\n",
    "<b> <font color='black'> <center>Please cite the ColabFold and AlphaFold2 papers if you use this notebook:</center>\n",
    "</font></b> \n",
    "\n",
    "- <font color='green'>[Mirdita, M.,  Ovchinnikov, S., Steinegger, M.(2021). ColabFold - Making protein folding accessible to all *bioRxiv*, 2021.08.15.456425](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1)</font> \n",
    "\n",
    "- <font color='green'> [Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583â€“589 (2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
    "</font>\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-VCr54KqwlmA"
   },
   "outputs": [],
   "source": [
    "# USER INPUT SECTION\n",
    "\n",
    "# IMPORTS, STANDARD PARAMETERS AND METHODS\n",
    "\n",
    "import os, sys\n",
    "import os.path\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from io import StringIO\n",
    "from google.colab import files\n",
    "import shutil\n",
    "from string import ascii_uppercase\n",
    "\n",
    "# Local methods\n",
    "\n",
    "def add_hash(x,y):\n",
    "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
    "\n",
    "def clear_directories(all_dirs):\n",
    "\n",
    "  for d in all_dirs:\n",
    "    if d.exists():\n",
    "      shutil.rmtree(d)\n",
    "    d.mkdir(parents=True)\n",
    "\n",
    "\n",
    "def clean_query(query_sequence):\n",
    "  query_sequence = \"\".join(query_sequence.split())\n",
    "  query_sequence = re.sub(r'[^a-zA-Z]','', query_sequence).upper()\n",
    "  return query_sequence\n",
    "\n",
    "def clean_jobname(jobname):\n",
    "  jobname = \"\".join(jobname.split())\n",
    "  jobname = re.sub(r'\\W+', '', jobname)\n",
    "  if len(jobname.split(\"_\")) == 1:\n",
    "    jobname = add_hash(jobname, query_sequence)\n",
    "  return jobname\n",
    "\n",
    "def save_sequence(jobname, query_sequence):\n",
    "  # save sequence as text file\n",
    "  filename = f\"{jobname}.fasta\"\n",
    "  with open(filename, \"w\") as text_file:\n",
    "    text_file.write(\">1\\n%s\" % query_sequence)\n",
    "  print(\"Saved sequence in %s: %s\" %(filename, query_sequence))\n",
    "\n",
    "def upload_templates(cif_dir):\n",
    "  manual_templates_uploaded = []\n",
    "  with redirect_stdout(StringIO()) as out:\n",
    "    uploaded = files.upload()\n",
    "    for filename,contents in uploaded.items():\n",
    "      filepath = Path(cif_dir,filename)\n",
    "      with filepath.open(\"w\") as fh:\n",
    "        fh.write(contents.decode(\"UTF-8\"))\n",
    "        manual_templates_uploaded.append(filepath)\n",
    "  print(\"Templates uploaded: %s\" %(manual_templates_uploaded))   \n",
    "  return manual_templates_uploaded\n",
    "\n",
    "def get_jobnames_sequences_from_file(\n",
    "    upload_manual_templates = None, cif_dir = None):\n",
    "  from io import StringIO\n",
    "  from google.colab import files\n",
    "  print(\"Upload file with one jobname, a space and one sequence on each line\")\n",
    "  #with redirect_stdout(StringIO()) as out:\n",
    "  uploaded = files.upload()\n",
    "  s = StringIO()\n",
    "  query_sequences = []\n",
    "  jobnames = []\n",
    "  cif_filename_dict = {}\n",
    "  for filename,contents in uploaded.items():\n",
    "    print(contents.decode(\"UTF-8\"), file = s)\n",
    "    text = s.getvalue()\n",
    "    for line in text.splitlines():\n",
    "      spl = line.split()\n",
    "      if len(spl) < 2:\n",
    "        pass # empty line\n",
    "      else: # usual\n",
    "        jobname = spl[0]\n",
    "        query_sequence = \"\".join(spl[1:])\n",
    "        if jobname in jobnames:\n",
    "          pass # already there\n",
    "        else:\n",
    "          query_sequences.append(query_sequence)\n",
    "          jobnames.append(jobname)\n",
    "          if upload_manual_templates:\n",
    "            print(\"\\nPlease upload CIF template for %s\" %(jobname))\n",
    "            sys.stdout.flush()\n",
    "            cif_filename_dict[jobname] = upload_templates(cif_dir)\n",
    "  return jobnames, query_sequences, cif_filename_dict\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"/content/\")\n",
    "\n",
    "# Clear out directories\n",
    "parent_dir = Path(\"/content/manual_templates\")\n",
    "cif_dir = Path(parent_dir,\"mmcif\")\n",
    "\n",
    "# GET INPUTS\n",
    "\n",
    "#@title 1. Enter sequence and jobname. Load with <i><b>Run</b></i> button to left. Repeat to enter any additional sequences.\n",
    "\n",
    "#@markdown <b><i><font color=green>Protein sequence and job name</font></i></b>\n",
    "\n",
    "query_sequence = '' #@param {type:\"string\"}\n",
    "jobname = '' #@param {type:\"string\"}\n",
    "\n",
    "#@markdown <b><i><font color=green>Options (Use PDB templates/upload templates, upload button will show up below if uploading)</font></i></b>\n",
    "templates_to_use = 'From_PDB' #@param [\"From_PDB\", \"Upload_files\", \"From_PDB_and_upload_files\", \"None\"]\n",
    "if templates_to_use == \"From_PDB\":\n",
    "  upload_manual_templates = False \n",
    "  include_templates_from_pdb = True\n",
    "elif templates_to_use == \"Upload_files\":\n",
    "  include_templates_from_pdb = False\n",
    "  upload_manual_templates = True\n",
    "elif templates_to_use == \"From_PDB_and_Upload_files\":\n",
    "  include_templates_from_pdb = True\n",
    "  upload_manual_templates = True\n",
    "else:\n",
    "  upload_manual_templates = False \n",
    "  include_templates_from_pdb = False\n",
    "\n",
    "if upload_manual_templates:\n",
    "  print(\"Templates will be uploaded\")\n",
    "if include_templates_from_pdb:\n",
    "  print(\"Templates from the PDB will be included\")\n",
    "\n",
    "upload_file_with_jobname_space_sequence_lines = False #@param {type:\"boolean\"}\n",
    "clear_saved_sequences_and_jobnames = False #@param {type:\"boolean\"}\n",
    "maximum_templates_from_pdb = 20 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "# Initialize query_sequences so we can loop through input\n",
    "if clear_saved_sequences_and_jobnames or (\n",
    "     not locals().get('query_sequences', None)):\n",
    "  query_sequences = []\n",
    "  jobnames = []\n",
    "  cif_filename_dict = {}\n",
    "  clear_directories([parent_dir,cif_dir])\n",
    "\n",
    "del locals()['clear_saved_sequences_and_jobnames'] # so it updates\n",
    "\n",
    "if upload_file_with_jobname_space_sequence_lines:\n",
    "  del locals()['upload_file_with_jobname_space_sequence_lines'] # so it updates\n",
    "  jobnames, query_sequences, cif_filename_dict = get_jobnames_sequences_from_file()\n",
    "\n",
    "else: # usual\n",
    "  if query_sequence and not jobname:\n",
    "    print(\"Please enter a job name and rerun\")\n",
    "    raise AssertionError(\"Please enter a job name and rerun\")\n",
    "\n",
    "  if jobname and not query_sequence:\n",
    "    print(\"Please enter a query_sequence and rerun\")\n",
    "    raise AssertionError(\"Please enter a query_sequence rerun\")\n",
    "\n",
    "  # Add sequence and jobname if new\n",
    "  if (jobname and query_sequence) and (\n",
    "       not query_sequence in query_sequences) and (\n",
    "       not jobname in jobnames):\n",
    "      query_sequences.append(query_sequence)\n",
    "      jobnames.append(jobname)\n",
    "      if upload_manual_templates:\n",
    "        print(\"\\nPlease upload template for %s\" %(jobname))\n",
    "        sys.stdout.flush()\n",
    "        cif_filename_dict[jobname] = upload_templates(cif_dir)\n",
    "\n",
    "\n",
    "# remove whitespaces\n",
    "for i in range(len(query_sequences)):\n",
    "  query_sequences[i] = clean_query(query_sequences[i])\n",
    "  jobnames[i] = clean_jobname(jobnames[i])\n",
    "\n",
    "  # save the sequence as a file with name jobname.fasta\n",
    "  save_sequence(jobnames[i], query_sequences[i])\n",
    "  \n",
    "print(\"\\nCurrent jobs, sequences, and templates:\")\n",
    "\n",
    "for qs,jn in zip(query_sequences, jobnames):\n",
    "  template_list = []\n",
    "  for t in cif_filename_dict.get(jobname,[]):\n",
    "    template_list.append(os.path.split(str(t))[-1])\n",
    "  print(jn, qs, template_list)\n",
    "import sys\n",
    "sys.stdout.flush()  # seems to overwrite otherwise\n",
    "\n",
    "\n",
    "if not query_sequences:\n",
    "  print(\"Please supply a query sequence and run again\")\n",
    "  raise AssertionError(\"Need a query sequence\")\n",
    "\n",
    "# STANDARD PARAMETERS AND METHODS\n",
    "\n",
    "#standard values of parameters\n",
    "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
    "num_models = 1 \n",
    "homooligomer = 1\n",
    "use_msa = True\n",
    "use_env = True\n",
    "use_custom_msa = False\n",
    "use_amber = False \n",
    "use_templates = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iccGdbe_Pmt9"
   },
   "outputs": [],
   "source": [
    "#@title 2. Load AlphaFold2 with the <b><i>Run</i></b> button to the left\n",
    "%%bash -s $use_amber $use_msa $use_templates\n",
    "echo \"Installing dependencies...\"\n",
    "USE_AMBER=$1\n",
    "USE_MSA=$2\n",
    "USE_TEMPLATES=$3\n",
    "\n",
    "if [ ! -f AF2_READY ]; then\n",
    "  # install dependencies\n",
    "  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
    "  wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
    "\n",
    "  # download model\n",
    "  if [ ! -d \"alphafold/\" ]; then\n",
    "    git clone https://github.com/deepmind/alphafold.git --quiet\n",
    "    (cd alphafold; git checkout 0bab1bf84d9d887aba5cfb6d09af1e8c3ecbc408 --quiet)\n",
    "    mv alphafold alphafold_\n",
    "    mv alphafold_/alphafold .\n",
    "    # remove \"END\" from PDBs, otherwise biopython complains\n",
    "    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
    "    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
    "  fi\n",
    "\n",
    "  # download model params (~1 min)\n",
    "  if [ ! -d \"params/\" ]; then\n",
    "    mkdir params\n",
    "    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \\\n",
    "    | tar x -C params\n",
    "  fi\n",
    "  touch AF2_READY\n",
    "fi\n",
    "# download libraries for interfacing with MMseqs2 API\n",
    "if [ ${USE_MSA} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
    "  if [ ! -f MMSEQ2_READY ]; then\n",
    "    apt-get -qq -y update 2>&1 1>/dev/null\n",
    "    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
    "    touch MMSEQ2_READY\n",
    "  fi\n",
    "fi\n",
    "# setup conda\n",
    "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
    "  if [ ! -f CONDA_READY ]; then\n",
    "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
    "    rm Miniconda3-latest-Linux-x86_64.sh\n",
    "    touch CONDA_READY\n",
    "  fi\n",
    "fi\n",
    "# setup template search\n",
    "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
    "  conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
    "  touch HH_READY\n",
    "fi\n",
    "# setup openmm for amber refinement\n",
    "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
    "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
    "  (cd /usr/local/lib/python3.7/site-packages; patch -s -p0 < /content/alphafold_/docker/openmm.patch)\n",
    "  wget -qnc https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
    "  mv stereo_chemical_props.txt alphafold/common/\n",
    "  touch AMBER_READY\n",
    "fi\n",
    "echo \"Done with dependencies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jFNCnjI9_DUG"
   },
   "outputs": [],
   "source": [
    "#@title 3. Create AlphaFold models with the <b><i>Run</i></b> button to the left\n",
    "\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from dataclasses import dataclass, replace\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "print(\"Setting up methods...\", end = \"\")\n",
    "# setup the model\n",
    "if \"model\" not in dir():\n",
    "\n",
    "  # hiding warning messages\n",
    "  import warnings\n",
    "  from absl import logging\n",
    "  import os\n",
    "  import tensorflow as tf\n",
    "  warnings.filterwarnings('ignore')\n",
    "  logging.set_verbosity(\"error\")\n",
    "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "  tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "  import sys\n",
    "  import numpy as np\n",
    "  import pickle\n",
    "  from alphafold.common import protein\n",
    "  from alphafold.data import pipeline\n",
    "  from alphafold.data import templates\n",
    "  from alphafold.model import data\n",
    "  from alphafold.model import config\n",
    "  from alphafold.model import model\n",
    "  from alphafold.data.tools import hhsearch\n",
    "  import colabfold as cf\n",
    "\n",
    "  # plotting libraries\n",
    "  import py3Dmol\n",
    "  import matplotlib.pyplot as plt\n",
    "  import ipywidgets\n",
    "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
    "\n",
    "\n",
    "if use_amber and \"relax\" not in dir():\n",
    "  sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
    "  from alphafold.relax import relax\n",
    "\n",
    "from alphafold.data import mmcif_parsing\n",
    "from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
    "                                      _process_single_hit,\n",
    "                                      _assess_hhsearch_hit,\n",
    "                                      _build_query_to_hit_index_mapping,\n",
    "                                      _extract_template_features,\n",
    "                                      SingleHitResult,\n",
    "                                      TEMPLATE_FEATURES)\n",
    "\n",
    "def mk_mock_template(query_sequence):\n",
    "  # since alphafold's model requires a template input\n",
    "  # we create a blank example w/ zero input, confidence -1\n",
    "  ln = len(query_sequence)\n",
    "  output_templates_sequence = \"-\"*ln\n",
    "  output_confidence_scores = np.full(ln,-1)\n",
    "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
    "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
    "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
    "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
    "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
    "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
    "                       'template_sequence': [f'none'.encode()],\n",
    "                       'template_aatype': np.array(templates_aatype)[None],\n",
    "                       'template_confidence_scores': output_confidence_scores[None],\n",
    "                       'template_domain_names': [f'none'.encode()],\n",
    "                       'template_release_date': [f'none'.encode()]}\n",
    "  return template_features\n",
    "\n",
    "def mk_template(a3m_lines, template_paths):\n",
    "  template_featurizer = templates.TemplateHitFeaturizer(\n",
    "      mmcif_dir=template_paths,\n",
    "      max_template_date=\"2100-01-01\",\n",
    "      max_hits=20,\n",
    "      kalign_binary_path=\"kalign\",\n",
    "      release_dates_path=None,\n",
    "      obsolete_pdbs_path=None)\n",
    "\n",
    "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
    "\n",
    "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
    "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
    "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
    "                                                       query_pdb_code=None,\n",
    "                                                       query_release_date=None,\n",
    "                                                       hits=hhsearch_hits)\n",
    "  return templates_result.features\n",
    "\n",
    "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
    "  I = open(pdb_filename,\"r\").readlines()\n",
    "  O = open(pdb_filename,\"w\")\n",
    "  for line in I:\n",
    "    if line[0:6] == \"ATOM  \":\n",
    "      seq_id = int(line[22:26].strip()) - 1\n",
    "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
    "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
    "  O.close()\n",
    "\n",
    "def predict_structure(prefix, feature_dict, Ls, model_params, \n",
    "  use_model,\n",
    "  model_runner_1,\n",
    "  model_runner_3,\n",
    "  do_relax=False, random_seed=0):  \n",
    "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
    "\n",
    "  # Minkyung's code\n",
    "  # add big enough number to residue index to indicate chain breaks\n",
    "  idx_res = feature_dict['residue_index']\n",
    "  L_prev = 0\n",
    "  # Ls: number of residues in each chain\n",
    "  for L_i in Ls[:-1]:\n",
    "      idx_res[L_prev+L_i:] += 200\n",
    "      L_prev += L_i  \n",
    "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
    "  feature_dict['residue_index'] = idx_res\n",
    "\n",
    "  # Run the models.\n",
    "  plddts,paes = [],[]\n",
    "  unrelaxed_pdb_lines = []\n",
    "  relaxed_pdb_lines = []\n",
    "\n",
    "  for model_name, params in model_params.items():\n",
    "    if model_name in use_model:\n",
    "      print(f\"running {model_name}\")\n",
    "      # swap params to avoid recompiling\n",
    "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
    "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
    "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
    "      model_runner.params = params\n",
    "      \n",
    "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
    "      prediction_result = model_runner.predict(processed_feature_dict)\n",
    "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
    "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
    "      plddts.append(prediction_result['plddt'])\n",
    "      paes.append(prediction_result['predicted_aligned_error'])\n",
    "\n",
    "      if do_relax:\n",
    "        # Relax the prediction.\n",
    "        amber_relaxer = relax.AmberRelaxation(max_iterations=0,tolerance=2.39,\n",
    "                                              stiffness=10.0,exclude_residues=[],\n",
    "                                              max_outer_iterations=20)      \n",
    "        relaxed_pdb_str, _, _ = amber_relaxer.process(prot=unrelaxed_protein)\n",
    "        relaxed_pdb_lines.append(relaxed_pdb_str)\n",
    "\n",
    "  # rerank models based on predicted lddt\n",
    "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
    "  out = {}\n",
    "  print(\"reranking models based on avg. predicted lDDT\")\n",
    "  for n,r in enumerate(lddt_rank):\n",
    "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
    "\n",
    "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
    "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
    "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
    "\n",
    "    if do_relax:\n",
    "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
    "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
    "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
    "\n",
    "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
    "  \"\"\"\n",
    "  This is a hack to get hhsuite output strings to pass on\n",
    "  to the AlphaFold template featurizer. \n",
    "  \n",
    "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
    "  all the templates. Currently it builds a database with only one template at a time. Even \n",
    "  better would be to get an hhsuite alignment without using a database at all, just between\n",
    "  pairs of sequence files. However, I have not figured out how to do this.\n",
    "\n",
    "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
    "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
    "  \"\"\"\n",
    "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
    "  if hhDB_dir.exists():\n",
    "    shutil.rmtree(hhDB_dir)\n",
    "  \n",
    "  msa_dir = Path(hhDB_dir,\"msa\")\n",
    "  msa_dir.mkdir(parents=True)\n",
    "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
    "  with template_seq_path.open(\"w\") as fh:\n",
    "    SeqIO.write([template_seq], fh, \"fasta\")\n",
    "\n",
    "  # make hhsuite DB\n",
    "  with redirect_stdout(StringIO()) as out:\n",
    "    os.chdir(msa_dir)\n",
    "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
    "    os.chdir(hhDB_dir)\n",
    "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
    "    %shell rm DB_msa.ff{data,index}\n",
    "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
    "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
    "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
    "\n",
    "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
    "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
    "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
    "\n",
    "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
    "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
    "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
    "\n",
    "  # run hhsearch\n",
    "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
    "  with StringIO() as fh:\n",
    "    SeqIO.write([query_seq], fh, \"fasta\")\n",
    "    seq_fasta = fh.getvalue()\n",
    "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
    "\n",
    "  # process hits\n",
    "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
    "  if len(hhsearch_hits) >0:\n",
    "    hit = hhsearch_hits[0]\n",
    "    hit = replace(hit,**{\"name\":template_seq.id})\n",
    "  else:\n",
    "    hit = None\n",
    "  return hit\n",
    "\n",
    "def plot_plddt_legend():\n",
    "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
    "  plt.figure(figsize=(1,0.1),dpi=100)\n",
    "  ########################################\n",
    "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
    "    plt.bar(0, 0, color=c)\n",
    "  plt.legend(thresh, frameon=False,\n",
    "             loc='center', ncol=6,\n",
    "             handletextpad=1,\n",
    "             columnspacing=1,\n",
    "             markerscale=0.5,)\n",
    "  plt.axis(False)\n",
    "  return plt\n",
    "\n",
    "def plot_confidence(outs, model_num=1):\n",
    "  model_name = f\"model_{model_num}\"\n",
    "  plt.figure(figsize=(10,3),dpi=100)\n",
    "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
    "  #########################################\n",
    "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
    "  plt.plot(outs[model_name][\"plddt\"])\n",
    "  for n in range(homooligomer+1):\n",
    "    x = n*(len(query_sequence))\n",
    "    plt.plot([x,x],[0,100],color=\"black\")\n",
    "  plt.ylabel('plDDT')\n",
    "  plt.xlabel('position')\n",
    "  #########################################\n",
    "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
    "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
    "  plt.colorbar()\n",
    "  plt.xlabel('Scored residue')\n",
    "  plt.ylabel('Aligned residue')\n",
    "  #########################################\n",
    "  return plt\n",
    "\n",
    "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
    "  model_name = f\"model_{model_num}\"\n",
    "  if use_amber:\n",
    "    pdb_filename = f\"{jobname}_relaxed_{model_name}.pdb\"\n",
    "  else:\n",
    "    pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
    "\n",
    "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
    "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
    "\n",
    "  if color == \"lDDT\":\n",
    "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
    "  elif color == \"rainbow\":\n",
    "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
    "  elif color == \"chain\":\n",
    "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
    "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
    "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
    "  if show_sidechains:\n",
    "    BB = ['C','O','N']\n",
    "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
    "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
    "  if show_mainchains:\n",
    "    BB = ['C','O','N','CA']\n",
    "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "\n",
    "  view.zoomTo()\n",
    "  return view\n",
    "\n",
    "def run_job(query_sequence,\n",
    "        jobname,\n",
    "        upload_manual_templates,\n",
    "        manual_templates_uploaded,\n",
    "        maximum_templates_from_pdb,\n",
    "        num_models,\n",
    "        homooligomer,\n",
    "        use_msa,\n",
    "        use_env,\n",
    "        use_custom_msa,\n",
    "        use_amber,\n",
    "        use_templates,\n",
    "        include_templates_from_pdb):\n",
    "\n",
    "  #@title Get MSA and templates\n",
    "  print(\"Getting MSA and templates...\")\n",
    "  if (not include_templates_from_pdb):\n",
    "    template_paths = None # toss these ... get template_paths later\n",
    "  if use_templates:\n",
    "    a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
    "    if template_paths is None:\n",
    "      template_features = mk_mock_template(query_sequence * homooligomer)\n",
    "    else:\n",
    "      template_features = mk_template(a3m_lines, template_paths)\n",
    "  elif use_msa:\n",
    "    a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
    "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
    "  else:\n",
    "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
    "  \n",
    "  # File for a3m\n",
    "  a3m_file = f\"{jobname}.a3m\"\n",
    "\n",
    "  if use_msa:\n",
    "    with open(a3m_file, \"w\") as text_file:\n",
    "      text_file.write(a3m_lines)\n",
    "  else:\n",
    "    a3m_lines = \"\".join(open(a3m_file,\"r\").read())\n",
    "  \n",
    "  # parse MSA\n",
    "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
    "  \n",
    "  print(\"Done with MSA and templates\")\n",
    "  \n",
    "  #Process templates\n",
    "  print(\"PROCESSING TEMPLATES\")\n",
    "  \n",
    "  os.chdir(\"/content/\")\n",
    "  \n",
    "  other_cif_dir = Path(\"/content/%s\" %(template_paths))\n",
    "  parent_dir = Path(\"/content/manual_templates\")\n",
    "  cif_dir = Path(parent_dir,\"mmcif\")\n",
    "  fasta_dir = Path(parent_dir,\"fasta\")\n",
    "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
    "  msa_dir = Path(hhDB_dir,\"msa\")\n",
    "  clear_directories([fasta_dir,hhDB_dir,msa_dir])\n",
    "  \n",
    "  cif_files = list(cif_dir.glob(\"*\"))\n",
    "  number_of_supplied_templates = len(cif_files)\n",
    "  # Only include the cif_files in manual_templates_uploaded\n",
    "  manual_files_as_text = []\n",
    "  for f in manual_templates_uploaded:\n",
    "    manual_files_as_text.append(\n",
    "        os.path.split(str(f))[-1])\n",
    "  cif_files_to_include = []\n",
    "  for cif_file in cif_files:\n",
    "    text = os.path.split(str(cif_file))[-1]\n",
    "    if text in manual_files_as_text:\n",
    "      cif_files_to_include.append(cif_file)\n",
    "  cif_files = cif_files_to_include\n",
    "      \n",
    "  if include_templates_from_pdb:\n",
    "    other_cif_files = list(other_cif_dir.glob(\"*\"))\n",
    "    cif_files += other_cif_files\n",
    "  print(\"CIF files to include:\",cif_files)\n",
    "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",name=\"\",description=\"\")\n",
    "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
    "  with query_seq_path.open(\"w\") as fh:\n",
    "      SeqIO.write([query_seq], fh, \"fasta\")\n",
    "  \n",
    "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
    "  seqs = []\n",
    "  template_hit_list = []\n",
    "  \n",
    "  n_used = 0\n",
    "  for i,filepath in enumerate(cif_files):\n",
    "    if not str(filepath).endswith(\".cif\"): continue\n",
    "    if n_used >= maximum_templates_from_pdb + number_of_supplied_templates:\n",
    "      continue\n",
    "    n_used += 1\n",
    "    print(\"CIF file included:\",i+1,str(filepath))\n",
    "    with filepath.open(\"r\") as fh:\n",
    "      filestr = fh.read()\n",
    "      mmcif_obj = mmcif_parsing.parse(file_id=filepath.stem,mmcif_string=filestr)\n",
    "      mmcif = mmcif_obj.mmcif_object\n",
    "      if not mmcif: continue\n",
    "  \n",
    "      for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
    "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
    "        seq_name = filepath.stem.upper()+\"_\"+chain_id\n",
    "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
    "        seqs.append(seq)\n",
    "  \n",
    "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
    "          SeqIO.write([seq], fh, \"fasta\")\n",
    "  \n",
    "        \"\"\"\n",
    "        At this stage, we have a template sequence.\n",
    "        and a query sequence. \n",
    "        There are two options to generate template features:\n",
    "          1. Write new code to manually generate template features\n",
    "          2. Get an hhr alignment string, and pass that\n",
    "            to the existing template featurizer. \n",
    "            \n",
    "        I chose the second, implemented in hh_process_seq()\n",
    "        \"\"\"\n",
    "        SeqIO.write([seq], sys.stdout, \"fasta\")\n",
    "        SeqIO.write([query_seq], sys.stdout, \"fasta\")\n",
    "        try:\n",
    "          hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
    "        except Exception as e:\n",
    "          hit = None\n",
    "        if hit is not None:\n",
    "          template_hit_list.append(hit)\n",
    "  \n",
    "  if template_hit_list:\n",
    "    #process hits into template features\n",
    "    template_hit_list = [replace(hit,**{\"index\":i+1}) for i,hit in enumerate(template_hit_list)]\n",
    "  \n",
    "  if (len(manual_templates_uploaded) > 0) and upload_manual_templates and (not template_hit_list):\n",
    "    # check to make sure we got something\n",
    "    # need template and did not get any\n",
    "      print(\"\\n\",80*\"-\")\n",
    "      print(\"\\nNo templates obtained...please be sure to use a .cif file\")\n",
    "      print(\"Use this converter: https://mmcif.pdbj.org/converter/\")\n",
    "      print(\"\\nYou can hit the red run button and load a new file\")\n",
    "      print(\"\\nYou can then hit all the remaining red run buttons one by one\")\n",
    "      print(\"\\n ... or you can go up to Runtime and hit 'Run all' again to start over\")\n",
    "      print(\"\\n\",80*\"-\")\n",
    "      raise AssertionError(\"Failed to read template file\")\n",
    "  elif use_templates and template_hit_list:\n",
    "    # have new templates to work with\n",
    "  \n",
    "    template_features = {}\n",
    "    for template_feature_name in TEMPLATE_FEATURES:\n",
    "      template_features[template_feature_name] = []\n",
    "  \n",
    "    for i,hit in enumerate(sorted(template_hit_list, key=lambda x: x.sum_probs, reverse=True)):\n",
    "      # modifications to alphafold/data/templates.py _process_single_hit\n",
    "      hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
    "      mapping = _build_query_to_hit_index_mapping(\n",
    "      hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
    "      query_sequence)\n",
    "      template_sequence = hit.hit_sequence.replace('-', '')\n",
    "  \n",
    "      try:\n",
    "        features, realign_warning = _extract_template_features(\n",
    "          mmcif_object=mmcif,\n",
    "          pdb_id=hit_pdb_code,\n",
    "          mapping=mapping,\n",
    "          template_sequence=template_sequence,\n",
    "          query_sequence=query_sequence,\n",
    "          template_chain_id=hit_chain_id,\n",
    "          kalign_binary_path=\"kalign\")\n",
    "      except Exception as e:\n",
    "        continue\n",
    "      features['template_sum_probs'] = [hit.sum_probs]\n",
    "  \n",
    "      single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
    "      for k in template_features:\n",
    "        template_features[k].append(features[k])\n",
    "  \n",
    "    for name in template_features:\n",
    "      template_features[name] = np.stack(\n",
    "          template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
    "      \n",
    "    #overwrite template data\n",
    "    template_paths = cif_dir.as_posix()\n",
    "\n",
    "\n",
    "    # Select only one chain from any cif file\n",
    "    unique_template_hits = []\n",
    "    pdb_text_list = []\n",
    "    for hit in template_hit_list:\n",
    "      pdb_text = hit.name.split()[0].split(\"_\")[0]\n",
    "      if not pdb_text in pdb_text_list:\n",
    "        pdb_text_list.append(pdb_text)\n",
    "        unique_template_hits.append(hit)\n",
    "    template_hit_list = unique_template_hits\n",
    "    template_hits = template_hit_list\n",
    "\n",
    "    print(\"\\nIncluding templates:\")\n",
    "    for hit in template_hit_list:\n",
    "      print(\"\\t\",hit.name.split()[0])\n",
    "    if len(template_hit_list) == 0:\n",
    "      print(\"No templates found...quitting\")\n",
    "      raise AssertionError(\"No templates found...quitting\")\n",
    "    os.chdir(\"/content/\")\n",
    "  \n",
    "    for key,value in template_features.items():\n",
    "      if np.all(value==0):\n",
    "        print(\"ERROR: Some template features are empty\")\n",
    "  else:  # no templates\n",
    "    print(\"Not using any templates\")\n",
    "  \n",
    "  print(\"\\nPREDICTING STRUCTURE\")\n",
    "\n",
    "  \n",
    "  # collect model weights\n",
    "  use_model = {}\n",
    "  model_params = {}\n",
    "  model_runner_1 = None\n",
    "  model_runner_3 = None\n",
    "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
    "    use_model[model_name] = True\n",
    "    if model_name not in list(model_params.keys()):\n",
    "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
    "      if model_name == \"model_1\":\n",
    "        model_config = config.model_config(model_name+\"_ptm\")\n",
    "        model_config.data.eval.num_ensemble = 1\n",
    "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
    "      if model_name == \"model_3\":\n",
    "        model_config = config.model_config(model_name+\"_ptm\")\n",
    "        model_config.data.eval.num_ensemble = 1\n",
    "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
    "  if homooligomer == 1:\n",
    "    msas = [msa]\n",
    "    deletion_matrices = [deletion_matrix]\n",
    "  else:\n",
    "    # make multiple copies of msa for each copy\n",
    "    # AAA------\n",
    "    # ---AAA---\n",
    "    # ------AAA\n",
    "    #\n",
    "    # note: if you concat the sequences (as below), it does NOT work\n",
    "    # AAAAAAAAA\n",
    "    msas = []\n",
    "    deletion_matrices = []\n",
    "    Ln = len(query_sequence)\n",
    "    for o in range(homooligomer):\n",
    "      L = Ln * o\n",
    "      R = Ln * (homooligomer-(o+1))\n",
    "      msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
    "      deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
    "  \n",
    "  # gather features\n",
    "  feature_dict = {\n",
    "      **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
    "                                        description=\"none\",\n",
    "                                        num_res=len(query_sequence)*homooligomer),\n",
    "      **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
    "      **template_features\n",
    "  }\n",
    "  outs = predict_structure(jobname, feature_dict,\n",
    "                           Ls=[len(query_sequence)]*homooligomer,\n",
    "                           model_params=model_params, use_model=use_model,\n",
    "                           model_runner_1=model_runner_1,\n",
    "                           model_runner_3=model_runner_3,\n",
    "                           do_relax=use_amber)\n",
    "  print(\"DONE WITH STRUCTURE\")\n",
    "  \n",
    "  #@title Making plots...\n",
    "  \n",
    "  # gather MSA info\n",
    "  deduped_full_msa = list(dict.fromkeys(msa))\n",
    "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
    "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
    "  seqid_sort = seqid.argsort() #[::-1]\n",
    "  non_gaps = (msa_arr != \"-\").astype(float)\n",
    "  non_gaps[non_gaps == 0] = np.nan\n",
    "  \n",
    "  ##################################################################\n",
    "  plt.figure(figsize=(14,4),dpi=100)\n",
    "  ##################################################################\n",
    "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
    "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
    "             interpolation='nearest', aspect='auto',\n",
    "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
    "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
    "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
    "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
    "  plt.colorbar(label=\"Sequence identity to query\",)\n",
    "  plt.xlabel(\"Positions\")\n",
    "  plt.ylabel(\"Sequences\")\n",
    "  \n",
    "  ##################################################################\n",
    "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
    "  for model_name,value in outs.items():\n",
    "    plt.plot(value[\"plddt\"],label=model_name)\n",
    "  if homooligomer > 0:\n",
    "    for n in range(homooligomer+1):\n",
    "      x = n*(len(query_sequence)-1)\n",
    "      plt.plot([x,x],[0,100],color=\"black\")\n",
    "  plt.legend()\n",
    "  plt.ylim(0,100)\n",
    "  plt.ylabel(\"Predicted lDDT\")\n",
    "  plt.xlabel(\"Positions\")\n",
    "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
    "  ##################################################################\n",
    "  plt.show()\n",
    "  \n",
    "  print(\"Predicted Alignment Error\")\n",
    "  ##################################################################\n",
    "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
    "  for n,(model_name,value) in enumerate(outs.items()):\n",
    "    plt.subplot(1,num_models,n+1)\n",
    "    plt.title(model_name)\n",
    "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
    "    plt.colorbar()\n",
    "  plt.savefig(jobname+\"_PAE.png\")\n",
    "  plt.show()\n",
    "  ##################################################################\n",
    "  #@title Displaying 3D structure... {run: \"auto\"}\n",
    "  model_num = 1 \n",
    "  color = \"lDDT\" \n",
    "  show_sidechains = False \n",
    "  show_mainchains = False \n",
    "  \n",
    "  \n",
    "  \n",
    "  show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
    "  if color == \"lDDT\": plot_plddt_legend().show()  \n",
    "  plot_confidence(outs, model_num).show()\n",
    "  #@title Packaging and downloading results...\n",
    "  \n",
    "  #@markdown When modeling is complete .zip files with results will be downloaded automatically.\n",
    "  \n",
    "  citations = {\n",
    "  \"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
    "  author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
    "  doi = {10.1101/2021.08.15.456425},\n",
    "  journal = {bioRxiv},\n",
    "  title = {{ColabFold - Making Protein folding accessible to all}},\n",
    "  year = {2021},\n",
    "  comment = {ColabFold including MMseqs2 MSA server}\n",
    "  }\"\"\",\n",
    "    \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
    "  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
    "  doi = {10.1093/nar/gkz1035},\n",
    "  journal = {Nucleic Acids Res.},\n",
    "  title = {{MGnify: the microbiome analysis resource in 2020}},\n",
    "  year = {2019},\n",
    "  comment = {MGnify database}\n",
    "  }\"\"\",\n",
    "    \"Eastman2017\": \"\"\"@article{Eastman2017,\n",
    "  author = {Eastman, Peter and Swails, Jason and Chodera, John D. and McGibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and Wiewiora, Rafal P. and Brooks, Bernard R. and Pande, Vijay S.},\n",
    "  doi = {10.1371/journal.pcbi.1005659},\n",
    "  journal = {PLOS Comput. Biol.},\n",
    "  number = {7},\n",
    "  title = {{OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}},\n",
    "  volume = {13},\n",
    "  year = {2017},\n",
    "  comment = {Amber relaxation}\n",
    "  }\"\"\",\n",
    "    \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
    "  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
    "  doi = {10.1038/s41586-021-03819-2},\n",
    "  journal = {Nature},\n",
    "  pmid = {34265844},\n",
    "  title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
    "  year = {2021},\n",
    "  comment = {AlphaFold2 + BFD Database}\n",
    "  }\"\"\",\n",
    "    \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
    "  author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
    "  doi = {10.1093/bioinformatics/bty1057},\n",
    "  journal = {Bioinformatics},\n",
    "  number = {16},\n",
    "  pages = {2856--2858},\n",
    "  pmid = {30615063},\n",
    "  title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
    "  volume = {35},\n",
    "  year = {2019},\n",
    "  comment = {MMseqs2 search server}\n",
    "  }\"\"\",\n",
    "    \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
    "  author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
    "  doi = {10.1186/s12859-019-3019-7},\n",
    "  journal = {BMC Bioinform.},\n",
    "  number = {1},\n",
    "  pages = {473},\n",
    "  pmid = {31521110},\n",
    "  title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
    "  volume = {20},\n",
    "  year = {2019},\n",
    "  comment = {PDB70 database}\n",
    "  }\"\"\",\n",
    "    \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
    "  author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
    "  doi = {10.1093/nar/gkw1081},\n",
    "  journal = {Nucleic Acids Res.},\n",
    "  number = {D1},\n",
    "  pages = {D170--D176},\n",
    "  pmid = {27899574},\n",
    "  title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
    "  volume = {45},\n",
    "  year = {2017},\n",
    "  comment = {Uniclust30/UniRef30 database},\n",
    "  }\"\"\",\n",
    "    \"Berman2003\": \"\"\"@misc{Berman2003,\n",
    "  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
    "  booktitle = {Nat. Struct. Biol.},\n",
    "  doi = {10.1038/nsb1203-980},\n",
    "  number = {12},\n",
    "  pages = {980},\n",
    "  pmid = {14634627},\n",
    "  title = {{Announcing the worldwide Protein Data Bank}},\n",
    "  volume = {10},\n",
    "  year = {2003},\n",
    "  comment = {templates downloaded from wwPDB server}\n",
    "  }\"\"\",\n",
    "  }\n",
    "  \n",
    "  to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
    "  if use_msa:       to_cite += [\"Mirdita2019\"]\n",
    "  if use_msa:       to_cite += [\"Mirdita2017\"]\n",
    "  if use_env:       to_cite += [\"Mitchell2019\"]\n",
    "  if use_templates: to_cite += [\"Steinegger2019\"]\n",
    "  if use_templates: to_cite += [\"Berman2003\"]\n",
    "  if use_amber:     to_cite += [\"Eastman2017\"]\n",
    "  \n",
    "  with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
    "    for i in to_cite:\n",
    "      writer.write(citations[i])\n",
    "      writer.write(\"\\n\")\n",
    "  \n",
    "  print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
    "  if use_custom_msa:\n",
    "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
    "  \n",
    "  !echo 'FILES TO PACKAGE: $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\" '\n",
    "  try:\n",
    "    print(\"zipping files...\")\n",
    "    !zip -FSr $jobname\".result.zip\" $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\"\n",
    "  except Exception as e:\n",
    "    print(\"unable to zip files\")\n",
    "\n",
    "  filename = f\"{jobname}.result.zip\"\n",
    "  if os.path.isfile(filename):\n",
    "    print(\"About to download %s\" %(filename))\n",
    "  \n",
    "    try:\n",
    "      print(\"Downloading zip file %s\" %(filename))\n",
    "      files.download(filename)\n",
    "      print(\"Start of download successful (NOTE: if the download symbol does not go away it did not work. Download it manually using the folder icon to the left)\")\n",
    "      return filename\n",
    "    except Exception as e:\n",
    "      print(\"Unable to download zip file %s\" %(filename))\n",
    "      return None\n",
    "  else:\n",
    "    print(\"No .zip file %s created\" %(filename))\n",
    "    return None\n",
    "\n",
    "# RUN THE JOBS HERE\n",
    "\n",
    "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
    "  print(\"\\n\",\"****************************************\",\"\\n\",\n",
    "         \"RUNNING JOB %s with sequence %s\\n\" %(\n",
    "    jobname, query_sequence),\n",
    "    \"****************************************\",\"\\n\")\n",
    "  # GET TEMPLATES AND SET UP FILES\n",
    "\n",
    "\n",
    "\n",
    "  # User input of manual templates\n",
    "  manual_templates_uploaded = cif_filename_dict.get(\n",
    "      jobname,[])\n",
    "  if manual_templates_uploaded:\n",
    "    print(\"Using uploaded templates %s for this run\" %(\n",
    "        manual_templates_uploaded))\n",
    "\n",
    "  try:\n",
    "    filename = run_job(query_sequence,\n",
    "        jobname,\n",
    "        upload_manual_templates,\n",
    "        manual_templates_uploaded,\n",
    "        maximum_templates_from_pdb,\n",
    "        num_models,\n",
    "        homooligomer,\n",
    "        use_msa,\n",
    "        use_env,\n",
    "        use_custom_msa,\n",
    "        use_amber,\n",
    "        use_templates,\n",
    "        include_templates_from_pdb)\n",
    "    if filename:\n",
    "      print(\"FINISHED JOB (%s) %s with sequence %s\\n\" %(\n",
    "        filename, jobname, query_sequence),\n",
    "        \"****************************************\",\"\\n\")\n",
    "    else:\n",
    "      print(\"NO RESULT FOR JOB %s with sequence %s\\n\" %(\n",
    "    jobname, query_sequence),\n",
    "    \"****************************************\",\"\\n\")\n",
    "\n",
    "  except Exception as e:\n",
    "    print(\"FAILED: JOB %s with sequence %s\\n\\n%s\\n\" %(\n",
    "    jobname, query_sequence, str(e)),\n",
    "    \"****************************************\",\"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nDOWNLOADING FILES NOW:\\n\")\n",
    "for query_sequence, jobname in zip(query_sequences, jobnames):\n",
    "  filename = f\"{jobname}.result.zip\"\n",
    "  if os.path.isfile(filename):\n",
    "    print(filename)\n",
    "\n",
    "print(\"\\nALL DONE\\n\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGUBLzB3C6WN"
   },
   "source": [
    "**Helpful hints**\n",
    "\n",
    "* To upload\n",
    "a file with a jobname and a sequence on each line, \n",
    "check ***upload_file_with_jobname_space_sequence_lines*** and hit\n",
    "the ***Run*** button to the left of the first cell\n",
    "\n",
    "* To start over, check ***clear_saved_sequences_and_jobnames***.\n",
    "\n",
    "* You can step through this notebook one part at a time\n",
    "by hitting the ***Run*** buttons to the left one at a time. \n",
    "\n",
    "* The cell that is active is indicated by a ***Run*** button that has turned into a black circle with a moving black arc\n",
    "\n",
    "* When execution is done, the ***Run*** button will go back \n",
    "to its original white triangle inside a black circle\n",
    "\n",
    "* You can stop execution of the active cell by hitting its ***Run*** button. It will turn red to indicate it has stopped.\n",
    "\n",
    "* You can rerun any cell any time that nothing is running.  That means you can go all the way through, then go back to the first cell and enter another sequence and redo the procedure.\n",
    "\n",
    "* If something goes wrong, the Colab Notebook will print out\n",
    "an error message.  Usually this will be something telling you\n",
    "how to change your inputs.  You enter your new inputs and\n",
    "hit the ***Run*** button again to carry on.\n",
    "\n",
    "* The automatic download may not always work. Normally the\n",
    "file download starts when the .zip files are created,\n",
    "but the actual download happens when all the AlphaFold\n",
    "models are completed.\n",
    "You can click on the \n",
    "folder icon to the left of the window and download your\n",
    "jobname.zip file manually.  Open and close the file\n",
    "broswer to show recently-added files.\n",
    "\n",
    "* Your Colab connection may time out if you go away and\n",
    "leave it, or if you run for a long time (12 hr).\n",
    "If your connection times out you lose everything that\n",
    "is not yet downloaded. So you might want to download as you go\n",
    "if you are running multiple sequences, or else run in batches\n",
    "that are not too large (perhaps up to 10 at a time).\n",
    "\n",
    "\n",
    "* The zip file or files will usually not be automatically downloaded until the very end of the job. If you are running multiple sequences you might want to manually download results as they appear so they are safe.\n",
    "* Your sequence should contain only the 1-letter code of one protein chain. It can contain spaces if you want.\n",
    "* If you upload a file with multiple sequences, each line of the file should have exactly one job name, a space, and a sequence, like this:\n",
    "\n",
    "7n8i_24237 VIWMTQSPSSLSASVGDRVTITCQASQDIRFYLNWYQQKPGKAPKLLISDASNMETGVPSRFSGS\n",
    "\n",
    "7lvr_23541 MRECISIHVGQAGVQIGNACWELYCLEHGIQPDGQMPSDKTIGGGDDSFNTFFSETG\n",
    "\n",
    "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
    "* Your browser can block the pop-up for downloading the result file. You can instead manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
    "\n",
    "**Result zip file contents**\n",
    "\n",
    "1. PDB formatted structure\n",
    "2. Plot of the model quality (IDDT).\n",
    "3. Plots of the MSA coverage.\n",
    "4. A3M formatted input MSA.\n",
    "5. BibTeX file with citations for all used tools and databases.\n",
    "\n",
    "At the end of the job the `jobname.result.zip` file or files will be downloaded automatically.\n",
    "\n",
    "\n",
    "**Colab limitations**\n",
    "* While Colab is free, it is designed for interactive work and not-unlimited memory and GPU usage. It will time-out after a few hours and it may check that you are not a robot at random times.  On a time-out you may lose your work. You can increase your allowed time with Colab+\n",
    "\n",
    "* AlphaFold can crash if it requires too much memory. On a crash you may lose all your work that is not yet downloaded. You can have more memory accessible if you have Colab+. If you are familiar with Colab scripts you can try this [hack](https://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-efa636e646ff ) to increase your memory allowance.\n",
    "\n",
    "\n",
    "**Description of the plots**\n",
    "\n",
    "*   **Number of sequences per position** - Look for at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
    "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
    "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
    "\n",
    "\n",
    "**Acknowledgments**\n",
    "\n",
    "- <b> <font color='green'>This notebook is based on the very nice notebook from ColabFold ([Mirdita et al., *bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1), https://github.com/sokrypton/ColabFold)</font></b> \n",
    "\n",
    "- <b><font color='green'>ColabFold is based on AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
    "</font></b>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
