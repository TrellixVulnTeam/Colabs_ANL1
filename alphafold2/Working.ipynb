{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/Working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1r2dn6P2uq"
      },
      "source": [
        "### <center> <b> <font color='black'>  AlphaFold with a density map (development version) </font></b> </center>\n",
        "\n",
        "<font color='green'>This notebook integrates Phenix model rebuilding with AlphaFold to improve AlphaFold modeling.  You upload a sequence and a density map (ccp4/mrc format) and it carries out cycles of AlphaFold modeling, rebuilding with the density map, and AlphaFold modeling with the rebuilt model as a template. In each cycle you get a new AlphaFold model and a rebuilt model.\n",
        "\n",
        "This notebook is derived from [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) and the DeepMind [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb).\n",
        "</font>\n",
        "\n",
        "-----------------\n",
        "<b> <font color='black'> <center>Instructions for a simple run:</center>\n",
        "</font></b> \n",
        "\n",
        "1.  Load condacolab with the first cell. Hit  the <b><i>Run</i></b> button next to it (little white triangle in black circle) to install it. You will get crash messages because the kernel is restarted. Click to close all the messages.\n",
        "\n",
        "2.  Type in a sequence, resolution and jobname in the second cell and hit the <b><i>Run</i></b> button to load this info. You can choose whether and how you want to supply a starting template (this is useful for running additional cycles). Upload your maps and any templates when the upload button appears at the bottom of the cell.\n",
        "\n",
        "3. Type in your Phenix download password in the third cell and load Phenix and AlphaFold with the <b><i>Run</i></b> button in the next cell down...takes 10 minutes. <b>HINT:</b> You can run everything from cell 3 on down by selecting cell 3 (click on it) and going up to the <b><i>Runtime</i></b> pulldown menu and selecting <b><i>Run after</i></b>.\n",
        "\n",
        "4. Run cell 4 to update the locations of some files and run a Phenix regression test.  Takes 1 minute.\n",
        "\n",
        "5. Run cell 5 to get an AlphaFold/Phenix model...takes 1 to 10 hours..\n",
        "</font>\n",
        "\n",
        "\n",
        "Your model information should download automatically as a zip file at the very end of the last step (when all jobs are done).\n",
        "</font>\n",
        "-----------------\n",
        "<b> <font color='black'> <center>Please cite the ColabFold and AlphaFold2 papers if you use this notebook:</center>\n",
        "</font></b> \n",
        "\n",
        "- <font color='green'>[Mirdita, M.,  Ovchinnikov, S., Steinegger, M.(2021). ColabFold - Making protein folding accessible to all *bioRxiv*, 2021.08.15.456425](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1)</font> \n",
        "\n",
        "- <font color='green'> [Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583â€“589 (2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font>\n",
        "-----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ1Aun5T6are",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Install condacolab with the <b><i>Run</i></b> button to the left\n",
        "#@markdown The kernel is automatically restarted and you may receive a message about a crash. The message can be safely ignored.\n",
        "# https://github.com/conda-incubator/condacolab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!touch CONDA_READY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCr54KqwlmA",
        "cellView": "form"
      },
      "source": [
        "\n",
        "# set up bioconda and mount drive to patch in pdb_to_cif\n",
        "import os\n",
        "if not os.path.isfile(\"HH_READY\"):\n",
        "  !conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  ! touch HH_READY\n",
        "\n",
        "#@markdown <b><i><font color=green>Check box if you have maxit on your google drive (not likely)</i></b>\n",
        "local_maxit = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "def run_pdb_to_cif(f):\n",
        "    output_file = f.as_posix().replace(\".pdb\",\".cif\")\n",
        "    import shutil\n",
        "    shutil.copyfile(f,'/content/pdb.pdb')\n",
        "    ! chmod +x /content/maxit-v11.100-prod-src/bin/process_entry; RCSBROOT=/content/maxit-v11.100-prod-src; export RCSBROOT; echo $RCSBROOT;  /content/maxit-v11.100-prod-src/bin/process_entry -input /content/pdb.pdb -input_format pdb -output /content/pdb.cif -output_format cif\n",
        "    shutil.copyfile('/content/pdb.cif',output_file)\n",
        "    return Path(output_file)\n",
        "def pdb_to_cif_not_available(f):\n",
        "    print(\"\\nSorry, pdb_to_cif is not available. Please use a cif file\")\n",
        "    raise AssertionError(\"pdb_to_cif is not available\")\n",
        "\n",
        "if (local_maxit):\n",
        "  from google.colab import drive\n",
        "  if not os.path.isdir('/content/drive'): \n",
        "    drive.mount('/content/drive')\n",
        "  if not os.path.exists('/content/maxit-v11.100-prod-src'):\n",
        "    !ln -s /content/drive/MyDrive/maxit-v11.100-prod-src /content/\n",
        "  pdb_to_cif = run_pdb_to_cif\n",
        "  print(\"PDB or CIF files can be used\")\n",
        "else:\n",
        "  pdb_to_cif = pdb_to_cif_not_available\n",
        "\n",
        "# USER INPUT SECTION\n",
        "\n",
        "# IMPORTS, STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "import os, sys\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from string import ascii_uppercase\n",
        "\n",
        "# Local methods\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "def clear_directories(all_dirs):\n",
        "\n",
        "  for d in all_dirs:\n",
        "    if d.exists():\n",
        "      shutil.rmtree(d)\n",
        "    d.mkdir(parents=True)\n",
        "\n",
        "\n",
        "def clean_query(query_sequence):\n",
        "  query_sequence = \"\".join(query_sequence.split())\n",
        "  query_sequence = re.sub(r'[^a-zA-Z]','', query_sequence).upper()\n",
        "  return query_sequence\n",
        "\n",
        "def clean_jobname(jobname):\n",
        "  jobname = \"\".join(jobname.split())\n",
        "  jobname = re.sub(r'\\W+', '', jobname)\n",
        "  if len(jobname.split(\"_\")) == 1:\n",
        "    jobname = add_hash(jobname, query_sequence)\n",
        "  return jobname\n",
        "\n",
        "def save_sequence(jobname, query_sequence):\n",
        "  # save sequence as text file\n",
        "  filename = f\"{jobname}.fasta\"\n",
        "  with open(filename, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)\n",
        "  print(\"Saved sequence in %s: %s\" %(filename, query_sequence))\n",
        "\n",
        "def upload_templates(cif_dir, upload_maps = False, \n",
        "     upload_manual_templates = None):\n",
        "  manual_templates_uploaded = []\n",
        "  maps_uploaded = []\n",
        "\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    uploaded = files.upload()\n",
        "    for filename,contents in uploaded.items():\n",
        "\n",
        "      if upload_maps and \\\n",
        "           (str(filename).lower().endswith(\".ccp4\") or\n",
        "           str(filename).lower().endswith(\".map\") or\n",
        "           str(filename).lower().endswith(\".mrc\")):\n",
        "\n",
        "        filepath = Path(cif_dir,filename)\n",
        "        ff = open(filepath, 'wb')\n",
        "        ff.write(contents)\n",
        "        maps_uploaded.append(filepath)\n",
        "\n",
        "      elif upload_manual_templates and \\\n",
        "           str(filename).endswith(\".cif\"):\n",
        "\n",
        "        filepath = Path(cif_dir,filename)\n",
        "        with filepath.open(\"w\") as fh:\n",
        "          fh.write(contents.decode(\"UTF-8\"))\n",
        "          manual_templates_uploaded.append(filepath)\n",
        "\n",
        "      elif upload_manual_templates and \\\n",
        "           str(filename).endswith(\".pdb\"):\n",
        "\n",
        "        pdb_filepath = Path(cif_dir,filename)\n",
        "        with filepath.open(\"w\") as fh:\n",
        "          fh.write(contents.decode(\"UTF-8\"))\n",
        "        cif_filepath = pdb_to_cif(pdb_filepath)\n",
        "        manual_templates_uploaded.append(cif_filepath)\n",
        "\n",
        "  if upload_maps:\n",
        "    print(\"Maps uploaded: %s\" %(maps_uploaded))\n",
        "    \n",
        "  print(\"Templates uploaded: %s\" %(manual_templates_uploaded))\n",
        "  if (not upload_maps) and (not manual_templates_uploaded):\n",
        "    print(\"\\n*** WARNING: no templates uploaded...Please use only .cif files ***\\n\")\n",
        "  return manual_templates_uploaded, maps_uploaded\n",
        "\n",
        "def select_matching_template_files(uploaded_template_files,\n",
        "    jobname):\n",
        "  matching_files = []\n",
        "  for file_name in uploaded_template_files:\n",
        "    if file_name.parts[-1].startswith(jobname):\n",
        "      matching_files.append(file_name)\n",
        "  return matching_files\n",
        "\n",
        "def get_jobnames_sequences_from_file(\n",
        "    upload_manual_templates = None,\n",
        "    all_templates_at_once = None,\n",
        "    upload_maps = False,\n",
        "    cif_dir = None):\n",
        "  from io import StringIO\n",
        "  from google.colab import files\n",
        "  print(\"Upload file with one jobname, a space and one sequence on each line\")\n",
        "  uploaded_job_file = files.upload()\n",
        "  if upload_manual_templates and all_templates_at_once:\n",
        "    print(\"\\nUpload your templates now, all at once. \" +\\\n",
        "           \"NOTE: template file names must start with your job names\")\n",
        "    uploaded_template_files, uploaded_maps = upload_templates(\n",
        "        cif_dir, upload_maps = upload_maps,\n",
        "        upload_manual_templates= upload_manual_templates)\n",
        "    print(\"Total of %s template files uploaded: %s\" %(\n",
        "          len(uploaded_template_files), uploaded_template_files))\n",
        "    \n",
        "  else:\n",
        "    uploaded_template_files = []\n",
        "\n",
        "  s = StringIO()\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  resolutions = []\n",
        "  cif_filename_dict = {}\n",
        "  map_filename_dict = {}\n",
        "  for filename,contents in uploaded_job_file.items():\n",
        "    print(contents.decode(\"UTF-8\"), file = s)\n",
        "    text = s.getvalue()\n",
        "    for line in text.splitlines():\n",
        "      spl = line.split()\n",
        "      if len(spl) < 2:\n",
        "        pass # empty line\n",
        "      else: # usual\n",
        "        jobname = spl[0]\n",
        "        if upload_maps:\n",
        "          resolution = float(spl[1])\n",
        "          query_sequence = \"\".join(spl[2:])\n",
        "        else:\n",
        "          resolution = 3\n",
        "          query_sequence = \"\".join(spl[1:])\n",
        "        jobname = clean_jobname(jobname)\n",
        "        query_sequence = clean_query(query_sequence)\n",
        "\n",
        "        if jobname in jobnames:\n",
        "          pass # already there\n",
        "        else:\n",
        "          query_sequences.append(query_sequence)\n",
        "          jobnames.append(jobname)\n",
        "          resolutions.append(resolution)\n",
        "          if upload_manual_templates:\n",
        "            if uploaded_template_files:\n",
        "              cif_filename_dict[jobname] = \\\n",
        "                select_matching_template_files(\n",
        "                    uploaded_template_files,\n",
        "                     jobname)\n",
        "            \n",
        "            else:\n",
        "              if upload_maps:\n",
        "                print(\"\\nPlease upload CIF template and map (at same time) for %s\" %(jobname))\n",
        "\n",
        "              else:\n",
        "                print(\"\\nPlease upload CIF template for %s\" %(jobname))\n",
        "              sys.stdout.flush()\n",
        "              cif_filename_dict[jobname], map_filename_dict[jobname] = \\\n",
        "                 upload_templates(\n",
        "                  cif_dir, upload_maps = upload_maps,\n",
        "                  upload_manual_templates = upload_manual_templates)\n",
        "  return jobnames, resolutions, \\\n",
        "     query_sequences, cif_filename_dict, map_filename_dict\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "# Clear out directories\n",
        "parent_dir = Path(\"/content/manual_templates\")\n",
        "cif_dir = Path(parent_dir,\"mmcif\")\n",
        "\n",
        "# GET INPUTS\n",
        "\n",
        "#@title 2. Enter sequence (at least 20 residues), resolution and jobname. Load with <i><b>Run</b></i> button to left. Upload map and optional model with Upload button that appears. Repeat to enter any additional sequences.\n",
        "\n",
        "#@markdown <b><i><font color=green>Protein sequence and job name</font></i></b>\n",
        "\n",
        "query_sequence = 'KPHRYRPGTVALREIRRYQKSTELLIRRQPFARVVREICLLFTRGVDYRWQAMALLALQEAAEAFLVHLLEDAYLCSLHA RRVTLYPKDLQLARRLRGLQGEG' #@param {type:\"string\"}\n",
        "resolution = 3 #@param {type:\"number\"}\n",
        "\n",
        "jobname = '7bxt' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <b><i><font color=green>Choose whether to use templates from PDB or upload templates (PDB or mmCIF format), both or neither. Upload button will show up below</font></i></b>\n",
        "templates_to_use = 'Upload templates (templates for all jobs at once)' #@param [\"From PDB\", \"Upload templates\", \"Upload templates (templates for all jobs at once)\", \"From PDB and upload templates\", \"From PDB and upload templates (templates for all jobs at once)\", \"None\"]\n",
        "if templates_to_use == \"From PDB\":\n",
        "  upload_manual_templates = False \n",
        "  include_templates_from_pdb = True\n",
        "  all_templates_at_once = None\n",
        "elif templates_to_use == \"Upload templates\":\n",
        "  include_templates_from_pdb = False\n",
        "  upload_manual_templates = True\n",
        "  all_templates_at_once = False\n",
        "elif templates_to_use == \"Upload templates (templates for all jobs at once)\":\n",
        "  include_templates_from_pdb = False\n",
        "  upload_manual_templates = True\n",
        "  all_templates_at_once = True\n",
        "elif templates_to_use == \"From PDB and upload templates\":\n",
        "  include_templates_from_pdb = True\n",
        "  upload_manual_templates = True\n",
        "  all_templates_at_once = False\n",
        "elif templates_to_use == \"From PDB and upload templates (templates for all jobs at once)\":\n",
        "  include_templates_from_pdb = True\n",
        "  upload_manual_templates = True\n",
        "  all_templates_at_once = True\n",
        "else:\n",
        "  upload_manual_templates = False \n",
        "  include_templates_from_pdb = False\n",
        "  all_templates_at_once = None\n",
        "\n",
        "upload_maps = True  # This version expects a map\n",
        "\n",
        "#@markdown <b><i><font color=green>You can quickly set up multiple jobs by uploading a file with one jobname and sequence per line</i></b>\n",
        "upload_file_with_jobname_space_sequence_lines = False #@param {type:\"boolean\"}\n",
        "#@markdown <b><i><font color=green>You can choose how many templates from the PDB to consider</i></b>\n",
        "maximum_templates_from_pdb =  20#@param {type:\"integer\"}\n",
        "#@markdown <b><i><font color=green>You can choose how many iterations of modeling and rebuilding</i></b>\n",
        "maximum_cycles =  3#@param {type:\"integer\"}\n",
        "#@markdown <b><i><font color=green>You can skip all multiple sequence alignment information</i></b>\n",
        "skip_all_msa = False #@param {type:\"boolean\"}\n",
        "#@markdown <b><i><font color=green>You can remove all the information already entered and load what you have in the form now</i></b>\n",
        "clear_saved_sequences_and_jobnames = True #@param {type:\"boolean\"}\n",
        "\n",
        "if upload_manual_templates:\n",
        "  print(\"Templates will be uploaded\")\n",
        "  if all_templates_at_once and upload_file_with_jobname_space_sequence_lines:\n",
        "    print(\"All templates for all jobs will be uploaded at once\")\n",
        "  else:\n",
        "    all_templates_at_once = False\n",
        "else:\n",
        "  all_templates_at_once = False\n",
        "\n",
        "if include_templates_from_pdb:\n",
        "  print(\"Templates from the PDB will be included\")\n",
        "\n",
        "if skip_all_msa:\n",
        "  use_custom_msa = False\n",
        "\n",
        "# Initialize query_sequences so we can loop through input\n",
        "if clear_saved_sequences_and_jobnames or (\n",
        "     not locals().get('query_sequences', None)):\n",
        "  query_sequences = []\n",
        "  jobnames = []\n",
        "  resolutions = []\n",
        "  cif_filename_dict = {}\n",
        "  map_filename_dict = {}\n",
        "  clear_directories([parent_dir,cif_dir])\n",
        "\n",
        "del locals()['clear_saved_sequences_and_jobnames'] # so it updates\n",
        "\n",
        "if upload_file_with_jobname_space_sequence_lines:\n",
        "  del locals()['upload_file_with_jobname_space_sequence_lines'] # so it updates\n",
        "  jobnames, resolutions, query_sequences, cif_filename_dict, map_filename_dict = \\\n",
        "    get_jobnames_sequences_from_file(\n",
        "        upload_manual_templates = upload_manual_templates,\n",
        "        all_templates_at_once = all_templates_at_once,\n",
        "        upload_maps = True,\n",
        "        cif_dir = cif_dir)\n",
        "else: # usual\n",
        "  jobname = clean_jobname(jobname)\n",
        "  query_sequence = clean_query(query_sequence)\n",
        "  if query_sequence and not jobname:\n",
        "    print(\"Please enter a job name and rerun\")\n",
        "    raise AssertionError(\"Please enter a job name and rerun\")\n",
        "\n",
        "  if jobname and not query_sequence:\n",
        "    print(\"Please enter a query_sequence and rerun\")\n",
        "    raise AssertionError(\"Please enter a query_sequence rerun\")\n",
        "  \n",
        "  # Add sequence and jobname if new\n",
        "  if (jobname and query_sequence) and (\n",
        "       not query_sequence in query_sequences) and (\n",
        "       not jobname in jobnames):\n",
        "      query_sequences.append(query_sequence)\n",
        "      jobnames.append(jobname)\n",
        "      resolutions.append(resolution)\n",
        "      if upload_manual_templates or upload_maps:\n",
        "        print(\"\\nPlease upload %s for %s\" %(\n",
        "            \"template and map\" if upload_manual_templates and upload_maps \n",
        "            else \"template\" if upload_manual_templates\n",
        "            else \"map\",\n",
        "            jobname))\n",
        "        sys.stdout.flush()\n",
        "        cif_filename_dict[jobname], map_filename_dict[jobname] = \\\n",
        "          upload_templates(cif_dir, upload_maps = upload_maps,\n",
        "             upload_manual_templates = upload_manual_templates )\n",
        "      \n",
        "\n",
        "# Save sequence\n",
        "for i in range(len(query_sequences)):\n",
        "  # save the sequence as a file with name jobname.fasta\n",
        "  save_sequence(jobnames[i], query_sequences[i])\n",
        "  \n",
        "print(\"\\nCurrent jobs, sequences, templates, and maps:\")\n",
        "\n",
        "for qs,jn,res in zip(query_sequences, jobnames, resolutions):\n",
        "  template_list = []\n",
        "  for t in cif_filename_dict.get(jn,[]):\n",
        "    template_list.append(os.path.split(str(t))[-1])\n",
        "  map_list = []\n",
        "  for t in map_filename_dict.get(jn,[]):\n",
        "    map_list.append(os.path.split(str(t))[-1])\n",
        "  print(jn, res, qs, template_list, map_list)\n",
        "\n",
        "  if len(qs) < 20:\n",
        "    print(\"\\n\\nMinimum sequence length is 20 residues...\\n\\n\",\n",
        "          \"Please enter a longer sequence and select \\n\",\n",
        "          \"clear_saved_sequences_and_jobnames and run again\\n\\n\")\n",
        "    sys.stdout.flush()\n",
        "    raise AssertionError(\"Sequence must be 20 residues or more\")\n",
        "  if not map_list:\n",
        "    print(\"\\n\\nNeed a map for each sequence...\\n\\n\",\n",
        "          \"Please select \\n\",\n",
        "          \"clear_saved_sequences_and_jobnames and run again\\n\\n\")\n",
        "    sys.stdout.flush()\n",
        "    raise AssertionError(\"Map file needed for job %s\" %(jn))\n",
        "\n",
        "sys.stdout.flush()  # seems to overwrite otherwise\n",
        "\n",
        "\n",
        "if not query_sequences:\n",
        "  print(\"Please supply a query sequence and run again\")\n",
        "  raise AssertionError(\"Need a query sequence\")\n",
        "\n",
        "# STANDARD PARAMETERS AND METHODS\n",
        "\n",
        "#standard values of parameters\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
        "num_models = 1 \n",
        "homooligomer = 1\n",
        "use_msa = (not skip_all_msa)\n",
        "use_env = True\n",
        "use_custom_msa = False\n",
        "use_amber = False \n",
        "use_templates = True\n",
        "\n",
        "!touch READY_WITH_SEQUENCES\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8nak9KTUzZr",
        "cellView": "form"
      },
      "source": [
        "#@title 3. Type in Phenix password (same as your download password). Install Phenix and AlphaFold with the <b><i>Run</i></b> button to the left\n",
        "#@markdown This step will take approximately 8 minutes.\n",
        "%%bash\n",
        "if [ ! -f CONDA_READY ]; then\n",
        "  echo \"Please run the first cell to install conda.\"\n",
        "fi\n",
        "\n",
        "if [ ! -f PHENIX_DOWNLOADED ]; then\n",
        "  #@markdown <b><i><font color=green>The  password (permanent or weekly) for downloading Phenix</i></b>\n",
        "  PASSWORD='' #@param {type:\"string\"}\n",
        "  #@markdown <b><i><font color=green>The version of Phenix</i></b>  \n",
        "  VERSION='1.20-4445' #@param {type:\"string\"}\n",
        "\n",
        "  wget -q --user user --password ${PASSWORD} -r -l1 https://phenix-online.org/download/installers/${VERSION}/linux-64/ -A \"phenix*.tar.bz2\"\n",
        "  if [ ! -d phenix-online.org ]; then\n",
        "     # try with user is trusted\n",
        "     wget -q --user trusted --password ${PASSWORD} -r -l1 https://phenix-online.org/download/installers/${VERSION}/linux-64/ -A \"phenix*.tar.bz2\"\n",
        "  fi\n",
        "  if [ ! -d phenix-online.org ]; then\n",
        "    echo \"Unable to download...please check your password?\"\n",
        "  else\n",
        "    mv phenix-online.org/download/installers/${VERSION}/linux-64/* .\n",
        "    rm -fr phenix-online.org\n",
        "    touch PHENIX_DOWNLOADED\n",
        "    echo \"Phenix has been downloaded.\"\n",
        "    touch PHENIX_DOWNLOADED\n",
        "    mamba install -q -y phenix*.tar.bz2\n",
        "    mamba install -q -c conda-forge -y boost=1.74 boost-cpp mrcfile numpy=1.20 scipy >& /dev/null\n",
        "    cp -a /usr/local/share/cctbx /usr/share\n",
        "    pip install psutil\n",
        "    touch PHENIX_READY\n",
        "    echo \"Phenix has been installed.\"\n",
        "  fi\n",
        "fi\n",
        "\n",
        "echo \"Installing dependencies...\"\n",
        "\n",
        "if [ ! -f AF2_READY ]; then\n",
        "  # install dependencies\n",
        "  echo \"Installing biopython and colabfold...\"\n",
        "  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "  wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "  # download model\n",
        "  if [ ! -d \"alphafold/\" ]; then\n",
        "    echo \"Installing AlphaFold...\"\n",
        "    git clone https://github.com/deepmind/alphafold.git --quiet\n",
        "    (cd alphafold; git checkout 0bab1bf84d9d887aba5cfb6d09af1e8c3ecbc408 --quiet)\n",
        "    mv alphafold alphafold_\n",
        "    mv alphafold_/alphafold .\n",
        "    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "  fi\n",
        "  # download model params (~1 min)\n",
        "  if [ ! -d \"params/\" ]; then\n",
        "    echo \"Installing AlphaFold parameters...\"\n",
        "    mkdir params\n",
        "    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \\\n",
        "    | tar x -C params\n",
        "  fi\n",
        "  touch AF2_READY\n",
        "fi\n",
        "# download libraries for interfacing with MMseqs2 API\n",
        "if [ ! -f MMSEQ2_READY ]; then\n",
        "    echo \"Installing mmseq2 ...\"\n",
        "    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "    touch MMSEQ2_READY\n",
        "fi\n",
        "\n",
        "echo \"Done with dependencies\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M7W1jb9o1py",
        "cellView": "form"
      },
      "source": [
        "#@title 4. Update some locations for Phenix and run a regression test with the <b><i>Run</i></b> button to the left\n",
        "import sys\n",
        "for d in ['/usr/local/lib', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages']:\n",
        "  if d not in sys.path:\n",
        "    sys.path.append(d)\n",
        "! cd /usr/local/lib/python3.7/site-packages/; tar czvf - phenix/refinement/*/*.params > phenix/tmp_phenix.tgz; cd phenix; tar xzvf tmp_phenix.tgz\n",
        "# simple test\n",
        "try:\n",
        "  from scitbx.array_family import flex\n",
        "  import libtbx.load_env\n",
        "  print('Phenix is working.')\n",
        "except Exception:\n",
        "  print('Phenix does not seem to be working. Please check that your password is correct. If the previous step completed quickly, the download and installation might have failed.')\n",
        "\n",
        "#@title 0.c. Run a Phenix regression test with the <b><i>Run</i></b> button to the left\n",
        "\n",
        "! echo \"Running quick regression test on phenix...\"\n",
        "! libtbx.python /usr/local/lib/python3.7/site-packages/phenix_regression/model_building/tst_fit_ligand_with_resolve.py|tail -5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFNCnjI9_DUG",
        "cellView": "form"
      },
      "source": [
        "#@title 5. Create AlphaFold models with the <b><i>Run</i></b> button to the left\n",
        "\n",
        "for x in ('mmcif_parsing','mmcif_parsing.parse'):\n",
        "  if x in locals():\n",
        "    del locals()[x]\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "import os, sys\n",
        "ready = True\n",
        "if not os.path.isfile('AF2_READY'):\n",
        "  print(\"Please load AlphaFold2 before running this cell...\")\n",
        "  ready = False\n",
        "if not os.path.isfile('READY_WITH_SEQUENCES'):\n",
        "  print(\"Please set up sequences before running this cell...\")\n",
        "  ready = False\n",
        "\n",
        "if ready:\n",
        "  from contextlib import redirect_stderr, redirect_stdout\n",
        "  from dataclasses import dataclass, replace\n",
        "  from Bio.Seq import Seq\n",
        "  from Bio.SeqRecord import SeqRecord\n",
        "  from Bio import SeqIO\n",
        "\n",
        "\n",
        "  print(\"Setting up methods...\", end = \"\")\n",
        "# setup the model\n",
        "if ready and \"model\" not in dir():\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  import colabfold as cf\n",
        "\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "if ready:\n",
        "  from alphafold.data import mmcif_parsing\n",
        "  from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
        "                                      _process_single_hit,\n",
        "                                      _assess_hhsearch_hit,\n",
        "                                      _build_query_to_hit_index_mapping,\n",
        "                                      _extract_template_features,\n",
        "                                      SingleHitResult,\n",
        "                                      TEMPLATE_FEATURES)\n",
        "\n",
        "def mk_mock_template(query_sequence):\n",
        "  # since alphafold's model requires a template input\n",
        "  # we create a blank example w/ zero input, confidence -1\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"-\"*ln\n",
        "  output_confidence_scores = np.full(ln,-1)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
        "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
        "                       'template_sequence': [f'none'.encode()],\n",
        "                       'template_aatype': np.array(templates_aatype)[None],\n",
        "                       'template_confidence_scores': output_confidence_scores[None],\n",
        "                       'template_domain_names': [f'none'.encode()],\n",
        "                       'template_release_date': [f'none'.encode()]}\n",
        "  return template_features\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, model_params, \n",
        "  use_model,\n",
        "  model_runner_1,\n",
        "  model_runner_3,\n",
        "  do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  L_prev = 0\n",
        "  # Ls: number of residues in each chain\n",
        "  for L_i in Ls[:-1]:\n",
        "      idx_res[L_prev+L_i:] += 200\n",
        "      L_prev += L_i  \n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "  feature_dict['residue_index'] = idx_res\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "      \n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      prediction_result = model_runner.predict(processed_feature_dict)\n",
        "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'])\n",
        "      paes.append(prediction_result['predicted_aligned_error'])\n",
        "\n",
        "    \n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  print(\"reranking models based on avg. predicted lDDT\")\n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    if do_relax:\n",
        "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
        "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
        "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
        "  \"\"\"\n",
        "  This is a hack to get hhsuite output strings to pass on\n",
        "  to the AlphaFold template featurizer. \n",
        "  \n",
        "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
        "  all the templates. Currently it builds a database with only one template at a time. Even \n",
        "  better would be to get an hhsuite alignment without using a database at all, just between\n",
        "  pairs of sequence files. However, I have not figured out how to do this.\n",
        "\n",
        "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
        "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
        "  \"\"\"\n",
        "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
        "  if hhDB_dir.exists():\n",
        "    shutil.rmtree(hhDB_dir)\n",
        "  \n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  msa_dir.mkdir(parents=True)\n",
        "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
        "  with template_seq_path.open(\"w\") as fh:\n",
        "    SeqIO.write([template_seq], fh, \"fasta\")\n",
        "  print(\"MSA DIR\",msa_dir)\n",
        "  # make hhsuite DB\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    os.chdir(msa_dir)\n",
        "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
        "    os.chdir(hhDB_dir)\n",
        "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
        "    %shell rm DB_msa.ff{data,index}\n",
        "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
        "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
        "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
        "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
        "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
        "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
        "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
        "    os.chdir(\"/content/\")\n",
        "\n",
        "  # run hhsearch\n",
        "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\",\n",
        "      databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
        "  with StringIO() as fh:\n",
        "    SeqIO.write([query_seq], fh, \"fasta\")\n",
        "    seq_fasta = fh.getvalue()\n",
        "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
        "\n",
        "  # process hits\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  if len(hhsearch_hits) >0:\n",
        "    hit = hhsearch_hits[0]\n",
        "    hit = replace(hit,**{\"name\":template_seq.id})\n",
        "  else:\n",
        "    hit = None\n",
        "  return hit\n",
        "\n",
        "def plot_plddt_legend():\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=100)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "def plot_confidence(outs, model_num=1):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  plt.figure(figsize=(10,3),dpi=100)\n",
        "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
        "  #########################################\n",
        "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
        "  plt.plot(outs[model_name][\"plddt\"])\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence))\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  #########################################\n",
        "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
        "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "  #########################################\n",
        "  return plt\n",
        "\n",
        "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "\n",
        "  pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
        "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "def write_pae_file(pae_matrix, file_name):\n",
        "  shape=tuple(pae_matrix.shape)\n",
        "  n,n = shape\n",
        "  # Write out array to text file as json\n",
        "  residues_1 = []\n",
        "  residues_2 = []\n",
        "  distances = []\n",
        "  for i in range(n):\n",
        "    ii = i + 1\n",
        "    for j in range(n):\n",
        "      jj= j + 1\n",
        "      residues_1.append(ii)\n",
        "      residues_2.append(jj)\n",
        "      distances.append(float(\"%.2f\" %(pae_matrix[i][j])))\n",
        "\n",
        "  residue_dict = {\"residue1\":residues_1,\n",
        "                   \"residue2\":residues_2,\n",
        "                  \"distance\":distances,\n",
        "                  \"max_predicted_aligned_error\":0}\n",
        "  values = [residue_dict]\n",
        "  text = str(values).replace(\" \",\"\").replace(\"'\",'\"')\n",
        "\n",
        "  f = open(file_name, 'w')\n",
        "  print(text, file = f)\n",
        "  f.close()\n",
        "  print(\"Wrote pae file to %s\" %(file_name))\n",
        "\n",
        "\n",
        "def get_msa(\n",
        "      query_sequence, jobname, use_env,\n",
        "      use_templates,\n",
        "      homooligomer,\n",
        "      use_msa):\n",
        "  template_paths = None # initialize\n",
        "\n",
        "  #@title Get MSA and templates\n",
        "  print(\"Getting MSA and templates...\")\n",
        "  if use_templates:\n",
        "    a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "  elif use_msa:\n",
        "    a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "\n",
        "\n",
        "\n",
        "  if (not use_msa):\n",
        "    a3m_lines = \">query sequence \\n%s\" %(query_sequence)\n",
        "    print(\"Not using any MSA information\")\n",
        "  \n",
        "  # File for a3m\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "      text_file.write(a3m_lines)\n",
        "\n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "      \n",
        "  print(\"Done with MSA and templates\")\n",
        "  return msa, deletion_matrix, template_paths\n",
        "\n",
        "def get_cif_file_list(include_pdb = None,\n",
        "    manual_templates_uploaded = None,\n",
        "    cif_dir = None):\n",
        "  cif_files = list(cif_dir.glob(\"*\"))\n",
        "  # Only include the cif_files in manual_templates_uploaded\n",
        "  manual_files_as_text = []\n",
        "  if not manual_templates_uploaded:\n",
        "    manual_templates_uploaded = []\n",
        "  for f in manual_templates_uploaded:\n",
        "    manual_files_as_text.append(\n",
        "        os.path.split(str(f))[-1])\n",
        "  cif_files_to_include = []\n",
        "  for cif_file in cif_files:\n",
        "    text = os.path.split(str(cif_file))[-1]\n",
        "    if text in manual_files_as_text:\n",
        "      cif_files_to_include.append(cif_file)\n",
        "  cif_files = cif_files_to_include\n",
        "      \n",
        "  if include_templates_from_pdb:\n",
        "    other_cif_files = []\n",
        "    for file_name in list(other_cif_dir.glob(\"*\")):\n",
        "      if file_name.endswith(\".cif\"):\n",
        "        other_cif_files.append(file_name)\n",
        "    cif_files += other_cif_files\n",
        "\n",
        "  return cif_files\n",
        "\n",
        "def get_template_hit_list(cif_files = None, fasta_dir = None,\n",
        "    query_seq = None,\n",
        "    hhDB_dir = None):\n",
        "  template_hit_list = []\n",
        "  for i,filepath in enumerate(cif_files):\n",
        "    if not str(filepath).endswith(\".cif\"): continue\n",
        "    print(\"CIF file included:\",i+1,str(filepath))\n",
        "    with filepath.open(\"r\") as fh:\n",
        "      filestr = fh.read()\n",
        "      mmcif_obj = mmcif_parsing.parse(file_id=filepath.stem,mmcif_string=filestr)\n",
        "      mmcif = mmcif_obj.mmcif_object\n",
        "      if not mmcif: \n",
        "        print(\"...No CIF object obtained...skipping...\")\n",
        "        continue\n",
        "  \n",
        "      for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
        "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
        "        seq_name = filepath.stem.upper()+\"_\"+chain_id\n",
        "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
        "  \n",
        "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
        "          SeqIO.write([seq], fh, \"fasta\")\n",
        "  \n",
        "        \"\"\"\n",
        "        At this stage, we have a template sequence.\n",
        "        and a query sequence. \n",
        "        There are two options to generate template features:\n",
        "          1. Write new code to manually generate template features\n",
        "          2. Get an hhr alignment string, and pass that\n",
        "            to the existing template featurizer. \n",
        "            \n",
        "        I chose the second, implemented in hh_process_seq()\n",
        "        \"\"\"\n",
        "        SeqIO.write([seq], sys.stdout, \"fasta\")\n",
        "        SeqIO.write([query_seq], sys.stdout, \"fasta\")\n",
        "        try:\n",
        "          hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
        "        except Exception as e:\n",
        "          hit = None\n",
        "        if hit is not None:\n",
        "          template_hit_list.append([hit,mmcif])\n",
        "\n",
        "  return template_hit_list\n",
        "  \n",
        "def run_one_cycle(cycle, template_hit_list,\n",
        "        query_sequence,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        num_models,\n",
        "        msa, deletion_matrix, template_paths):\n",
        "\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  if template_hit_list:\n",
        "    #process hits into template features\n",
        "    template_hit_list = [[replace(hit,**{\"index\":i+1}),mmcif] for i,[hit,mmcif] in enumerate(template_hit_list)]\n",
        "  \n",
        "    template_features = {}\n",
        "    for template_feature_name in TEMPLATE_FEATURES:\n",
        "      template_features[template_feature_name] = []\n",
        "  \n",
        "    for i,[hit,mmcif] in enumerate(sorted(template_hit_list, key=lambda xx: xx[0].sum_probs, reverse=True)):\n",
        "      # modifications to alphafold/data/templates.py _process_single_hit\n",
        "      hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
        "      mapping = _build_query_to_hit_index_mapping(\n",
        "      hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
        "      query_sequence)\n",
        "      template_sequence = hit.hit_sequence.replace('-', '')\n",
        "  \n",
        "      try:\n",
        "        features, realign_warning = _extract_template_features(\n",
        "          mmcif_object=mmcif,\n",
        "          pdb_id=hit_pdb_code,\n",
        "          mapping=mapping,\n",
        "          template_sequence=template_sequence,\n",
        "          query_sequence=query_sequence,\n",
        "          template_chain_id=hit_chain_id,\n",
        "          kalign_binary_path=\"kalign\")\n",
        "      except Exception as e:\n",
        "        continue\n",
        "      features['template_sum_probs'] = [hit.sum_probs]\n",
        "      single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
        "      for k in template_features:\n",
        "        template_features[k].append(features[k])\n",
        "\n",
        "    for name in template_features:\n",
        "      template_features[name] = np.stack(\n",
        "          template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
        "    #overwrite template data\n",
        "    template_paths = cif_dir.as_posix()\n",
        "\n",
        "\n",
        "    # Select only one chain from any cif file\n",
        "    unique_template_hits = []\n",
        "    pdb_text_list = []\n",
        "    for hit, mmcif in template_hit_list:\n",
        "      pdb_text = hit.name.split()[0].split(\"_\")[0]\n",
        "      if not pdb_text in pdb_text_list:\n",
        "        pdb_text_list.append(pdb_text)\n",
        "        unique_template_hits.append(hit)\n",
        "    template_hits = unique_template_hits\n",
        "\n",
        "    print(\"\\nIncluding templates:\")\n",
        "    for hit,mmcif in template_hit_list:\n",
        "      print(\"\\t\",hit.name.split()[0])\n",
        "    if len(template_hit_list) == 0:\n",
        "      print(\"No templates found...quitting\")\n",
        "      raise AssertionError(\"No templates found...quitting\")\n",
        "\n",
        "  \n",
        "    for key,value in template_features.items():\n",
        "      if np.all(value==0):\n",
        "        print(\"ERROR: Some template features are empty\")\n",
        "  else:  # no templates\n",
        "    print(\"Not using any templates\")\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "\n",
        "  print(\"\\nPREDICTING STRUCTURE\")\n",
        "  \n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  model_params = {}\n",
        "  model_runner_1 = None\n",
        "  model_runner_3 = None\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "    if model_name not in list(model_params.keys()):\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "  if homooligomer == 1:\n",
        "    msas = [msa]\n",
        "    deletion_matrices = [deletion_matrix]\n",
        "  else:\n",
        "    # make multiple copies of msa for each copy\n",
        "    # AAA------\n",
        "    # ---AAA---\n",
        "    # ------AAA\n",
        "    #\n",
        "    # note: if you concat the sequences (as below), it does NOT work\n",
        "    # AAAAAAAAA\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    Ln = len(query_sequence)\n",
        "    for o in range(homooligomer):\n",
        "      L = Ln * o\n",
        "      R = Ln * (homooligomer-(o+1))\n",
        "      msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
        "      deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
        "  \n",
        "  # gather features\n",
        "  feature_dict = {\n",
        "      **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
        "                                        description=\"none\",\n",
        "                                        num_res=len(query_sequence)*homooligomer),\n",
        "      **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "      **template_features\n",
        "  }\n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)]*homooligomer,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           model_runner_1=model_runner_1,\n",
        "                           model_runner_3=model_runner_3,\n",
        "                           do_relax=False)\n",
        "  print(\"DONE WITH STRUCTURE in\",os.getcwd())\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  print(os.listdir(\".\"))\n",
        "  model_file_name = \"%s_unrelaxed_model_1.pdb\" %(jobname)\n",
        "  if os.path.isfile(model_file_name):\n",
        "    print(\"Model file is in %s\" %(model_file_name))\n",
        "    cycle_model_file_name = \"%s_unrelaxed_model_1_%s.pdb\" %(jobname, cycle)\n",
        "    shutil.copyfile(model_file_name,cycle_model_file_name)\n",
        "  else:\n",
        "    print(\"No model file %s found for job %s\" %(model_file_name, jobname))\n",
        "    cycle_model_file_name = None\n",
        "  \n",
        "  #@title Making plots...\n",
        "  \n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "  \n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "  ##################################################################\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Predicted Alignment Error\")\n",
        "  ##################################################################\n",
        "  pae_file_list = []\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "    # Write pae file\n",
        "    pae_file = jobname+\"_\"+model_name+\"_PAE.jsn\"\n",
        "    write_pae_file(value[\"pae\"], pae_file)\n",
        "    pae_file_list.append(pae_file)\n",
        "  plt.savefig(jobname+\"_PAE.png\")\n",
        "  plt.show()\n",
        "  ##################################################################\n",
        "  #@title Displaying 3D structure... {run: \"auto\"}\n",
        "  model_num = 1 \n",
        "  color = \"lDDT\" \n",
        "  show_sidechains = False \n",
        "  show_mainchains = False \n",
        "  \n",
        "  \n",
        "  \n",
        "  show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
        "  if color == \"lDDT\": plot_plddt_legend().show()  \n",
        "  plot_confidence(outs, model_num).show()\n",
        "  #@title Packaging and downloading results...\n",
        "  \n",
        "  #@markdown When modeling is complete .zip files with results will be downloaded automatically.\n",
        "  \n",
        "  citations = {\n",
        "  \"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "  author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "  doi = {10.1101/2021.08.15.456425},\n",
        "  journal = {bioRxiv},\n",
        "  title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "  year = {2021},\n",
        "  comment = {ColabFold including MMseqs2 MSA server}\n",
        "  }\"\"\",\n",
        "    \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "  doi = {10.1093/nar/gkz1035},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "  year = {2019},\n",
        "  comment = {MGnify database}\n",
        "  }\"\"\",\n",
        "    \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "  doi = {10.1038/s41586-021-03819-2},\n",
        "  journal = {Nature},\n",
        "  pmid = {34265844},\n",
        "  title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "  year = {2021},\n",
        "  comment = {AlphaFold2 + BFD Database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "  author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1093/bioinformatics/bty1057},\n",
        "  journal = {Bioinformatics},\n",
        "  number = {16},\n",
        "  pages = {2856--2858},\n",
        "  pmid = {30615063},\n",
        "  title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "  volume = {35},\n",
        "  year = {2019},\n",
        "  comment = {MMseqs2 search server}\n",
        "  }\"\"\",\n",
        "    \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "  author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "  doi = {10.1186/s12859-019-3019-7},\n",
        "  journal = {BMC Bioinform.},\n",
        "  number = {1},\n",
        "  pages = {473},\n",
        "  pmid = {31521110},\n",
        "  title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "  volume = {20},\n",
        "  year = {2019},\n",
        "  comment = {PDB70 database}\n",
        "  }\"\"\",\n",
        "    \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "  author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "  doi = {10.1093/nar/gkw1081},\n",
        "  journal = {Nucleic Acids Res.},\n",
        "  number = {D1},\n",
        "  pages = {D170--D176},\n",
        "  pmid = {27899574},\n",
        "  title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "  volume = {45},\n",
        "  year = {2017},\n",
        "  comment = {Uniclust30/UniRef30 database},\n",
        "  }\"\"\",\n",
        "    \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "  author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "  booktitle = {Nat. Struct. Biol.},\n",
        "  doi = {10.1038/nsb1203-980},\n",
        "  number = {12},\n",
        "  pages = {980},\n",
        "  pmid = {14634627},\n",
        "  title = {{Announcing the worldwide Protein Data Bank}},\n",
        "  volume = {10},\n",
        "  year = {2003},\n",
        "  comment = {templates downloaded from wwPDB server}\n",
        "  }\"\"\",\n",
        "  }\n",
        "  \n",
        "  to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "  if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "  if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "  if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "  if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "  if use_templates: to_cite += [\"Berman2003\"]\n",
        "  \n",
        "  with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
        "    for i in to_cite:\n",
        "      writer.write(citations[i])\n",
        "      writer.write(\"\\n\")\n",
        "  \n",
        "  print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
        "  if use_custom_msa:\n",
        "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "  \n",
        "\n",
        "\n",
        "  return cycle_model_file_name\n",
        "\n",
        "def run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb):\n",
        "\n",
        "  os.chdir(\"/content/\")\n",
        "  \n",
        "  #Get the MSA\n",
        "  msa, deletion_matrix, template_paths = get_msa(\n",
        "      query_sequence, jobname, use_env,\n",
        "      use_templates,\n",
        "      homooligomer,\n",
        "      use_msa)\n",
        "  \n",
        "  #Process templates\n",
        "  print(\"PROCESSING TEMPLATES\")\n",
        "  \n",
        "  other_cif_dir = Path(\"/content/%s\" %(template_paths))\n",
        "  parent_dir = Path(\"/content/manual_templates\")\n",
        "  cif_dir = Path(parent_dir,\"mmcif\")\n",
        "  fasta_dir = Path(parent_dir,\"fasta\")\n",
        "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  clear_directories([fasta_dir,hhDB_dir,msa_dir])\n",
        "  \n",
        "  pdb_cif_file_list = get_cif_file_list(\n",
        "    include_pdb = True,\n",
        "    manual_templates_uploaded = None,\n",
        "    cif_dir = cif_dir)\n",
        "  print(\"CIF files from PDB to include:\",pdb_cif_file_list)\n",
        "  \n",
        "  manual_cif_file_list = get_cif_file_list(\n",
        "    include_pdb = False,\n",
        "    manual_templates_uploaded = manual_templates_uploaded,\n",
        "    cif_dir = cif_dir)\n",
        "  print(\"Uploaded CIF files to include:\",manual_cif_file_list)\n",
        "    \n",
        "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",\n",
        "    name=\"\",description=\"\")\n",
        "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
        "  with query_seq_path.open(\"w\") as fh:\n",
        "      SeqIO.write([query_seq], fh, \"fasta\")\n",
        "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
        "\n",
        "  previous_final_model_name = manual_templates_uploaded[0] if \\\n",
        "      manual_templates_uploaded else None\n",
        "\n",
        "  # Run first cycle\n",
        "\n",
        "  for cycle in range(1, maximum_cycles + 1):\n",
        "    print(\"\\nStarting cycle %s\" %(cycle))\n",
        "    working_cif_file_list = \\\n",
        "     list(manual_cif_file_list) + \\\n",
        "     list(pdb_cif_file_list)[:maximum_templates_from_pdb]\n",
        "\n",
        "    print(\"Templates used in this cycle: %s\" %(\n",
        "        \" \".join([w.as_posix() for w in working_cif_file_list])))\n",
        "\n",
        "    template_hit_list = get_template_hit_list(\n",
        "      cif_files = working_cif_file_list,\n",
        "      fasta_dir = fasta_dir,\n",
        "      query_seq = query_seq,\n",
        "      hhDB_dir = hhDB_dir)  \n",
        "\n",
        "    os.chdir(\"/content/\")\n",
        "    \n",
        "    cycle_model_file_name = run_one_cycle(\n",
        "        cycle, template_hit_list,\n",
        "        query_sequence,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        num_models,\n",
        "        msa, deletion_matrix, template_paths)\n",
        "\n",
        "    cycle_model_file_name = Path(cycle_model_file_name)\n",
        "\n",
        "   \n",
        "    print(\"\\nFinished with cycle %s of AlphaFold model generation\" %(cycle))\n",
        "    if not os.path.isfile(cycle_model_file_name.as_posix()):\n",
        "      print(\"No AlphaFold model obtained...quitting\")\n",
        "      return None\n",
        "    print(\"Current AlphaFold model is in %s\" %(\n",
        "        cycle_model_file_name.as_posix()))\n",
        "\n",
        "    print(\"\\nGetting a new rebuilt model at a resolution of %.2f A\" %(\n",
        "        resolution))\n",
        "    # Now get a new rebuilt model\n",
        "    final_model_file_name = rebuild_model(\n",
        "        cycle_model_file_name,\n",
        "        previous_final_model_name,\n",
        "        cycle,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        resolution)\n",
        "    \n",
        "    try:\n",
        "      !zip -FSr $jobname\".result.zip\"  $jobname*.pdb $jobname*.j* $jobname*.png $jobname*.bibtex $jobname*.jsn\n",
        "      zip_file_name = f\"{jobname}.result.zip\"\n",
        "    except Exception as e:\n",
        "      zip_file_name = None\n",
        "\n",
        "    if not final_model_file_name:\n",
        "      print(\"\\nEnding cycles as no rebuilt model obtained\")\n",
        "      break\n",
        "\n",
        "    # now update template_hit_list\n",
        "    final_model_file_name = Path(final_model_file_name)\n",
        "\n",
        "    final_model_file_name_in_cif_dir = Path(\n",
        "        os.path.join(cif_dir,final_model_file_name.name))\n",
        "    shutil.copyfile(\n",
        "      final_model_file_name,\n",
        "      final_model_file_name_in_cif_dir)\n",
        "    manual_cif_file_list = get_cif_file_list(\n",
        "      include_pdb = False,\n",
        "      manual_templates_uploaded = [final_model_file_name.name],\n",
        "      cif_dir = cif_dir)\n",
        "    \n",
        "\n",
        "  filename = zip_file_name\n",
        "  if filename and os.path.isfile(filename):\n",
        "    print(\"About to download %s\" %(filename))\n",
        "  \n",
        "    try:\n",
        "      print(\"Downloading zip file %s\" %(filename))\n",
        "      files.download(filename)\n",
        "      print(\"Start of download successful (NOTE: if the download symbol does not go away it did not work. Download it manually using the folder icon to the left)\")\n",
        "      return filename\n",
        "    except Exception as e:\n",
        "      print(\"Unable to download zip file %s\" %(filename))\n",
        "      return None\n",
        "  else:\n",
        "    print(\"No .zip file %s created\" %(filename))\n",
        "    return None\n",
        "\n",
        "def rebuild_model(\n",
        "        cycle_model_file_name,\n",
        "        previous_final_model_name,\n",
        "        cycle,\n",
        "        jobname,\n",
        "        maps_uploaded,\n",
        "        resolution,\n",
        "        nproc = 4):\n",
        "  assert len(maps_uploaded) == 1  # just one map\n",
        "  map_file_name = maps_uploaded[0]\n",
        "  af_model_file = os.path.abspath(\n",
        "      cycle_model_file_name.as_posix())\n",
        "  if previous_final_model_name:\n",
        "    previous_model_file = os.path.abspath(\n",
        "      previous_final_model_name.as_posix())\n",
        "  else:\n",
        "    previous_model_file = None\n",
        "  output_file_name = os.path.abspath(\n",
        "      \"%s_rebuilt_%s.pdb\" %(jobname,cycle))\n",
        "  print(\"Rebuilding %s %s with map in %s at resolution of %.2f\" %(\n",
        "       af_model_file,\n",
        "        \" with previous model of %s\" %(previous_model_file) \\\n",
        "        if previous_model_file else \"\",\n",
        "       map_file_name,\n",
        "      resolution,))\n",
        "\n",
        "  for ff in (map_file_name,af_model_file, previous_model_file):\n",
        "    if ff and not os.path.isfile(ff):\n",
        "      print(\"\\nMissing the file: %s\" %(ff))\n",
        "      return None\n",
        "\n",
        "  rebuilt_model_name = af_model_file.replace(\".pdb\",\"_rebuilt.pdb\")\n",
        "  rebuilt_model_stem = rebuilt_model_name.replace(\".pdb\",\"\")\n",
        "\n",
        "  # run phenix dock_and_rebuild here\n",
        "  !phenix.dock_and_rebuild nproc=$nproc resolution=$resolution previous_model_file=$previous_model_file model=$af_model_file full_map=$map_file_name output_model_prefix=$rebuilt_model_stem\n",
        "\n",
        "  if os.path.isfile(rebuilt_model_name):\n",
        "    print(\"Rebuilding successful\")\n",
        "    return rebuilt_model_name\n",
        "  else:\n",
        "    print(\"Rebuilding not successful\")\n",
        "    return None\n",
        "\n",
        "if ready:\n",
        "  # RUN THE JOBS HERE\n",
        "\n",
        "  for query_sequence, jobname, resolution in zip(query_sequences, jobnames, resolutions):\n",
        "    print(\"\\n\",\"****************************************\",\"\\n\",\n",
        "         \"RUNNING JOB %s with sequence %s at resolution of %s\\n\" %(\n",
        "      jobname, query_sequence, resolution),\n",
        "      \"****************************************\",\"\\n\")\n",
        "    # GET TEMPLATES AND SET UP FILES\n",
        "\n",
        "    # User input of manual templates\n",
        "    manual_templates_uploaded = cif_filename_dict.get(\n",
        "      jobname,[])\n",
        "    if manual_templates_uploaded:\n",
        "      print(\"Using uploaded templates %s for this run\" %(\n",
        "          manual_templates_uploaded))\n",
        "    maps_uploaded = map_filename_dict.get(\n",
        "      jobname,[])\n",
        "    if maps_uploaded:\n",
        "      print(\"Using uploaded maps %s for this run\" %(\n",
        "          maps_uploaded))\n",
        "\n",
        "    try:\n",
        "      filename = run_job(query_sequence,\n",
        "        jobname,\n",
        "        upload_manual_templates,\n",
        "        manual_templates_uploaded,\n",
        "        maps_uploaded,\n",
        "        maximum_cycles,\n",
        "        resolution,\n",
        "        maximum_templates_from_pdb,\n",
        "        num_models,\n",
        "        homooligomer,\n",
        "        use_msa,\n",
        "        use_env,\n",
        "        use_custom_msa,\n",
        "        use_templates,\n",
        "        include_templates_from_pdb)\n",
        "      if filename:\n",
        "        print(\"FINISHED JOB (%s) %s with sequence %s\\n\" %(\n",
        "        filename, jobname, query_sequence),\n",
        "        \"****************************************\",\"\\n\")\n",
        "      else:\n",
        "        print(\"NO RESULT FOR JOB %s with sequence %s\\n\" %(\n",
        "      jobname, query_sequence),\n",
        "      \"****************************************\",\"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"FAILED: JOB %s with sequence %s\\n\\n%s\\n\" %(\n",
        "      jobname, query_sequence, str(e)),\n",
        "      \"****************************************\",\"\\n\")\n",
        "\n",
        "\n",
        "  print(\"\\nDOWNLOADING FILES NOW:\\n\")\n",
        "  for query_sequence, jobname in zip(query_sequences, jobnames):\n",
        "    filename = f\"{jobname}.result.zip\"\n",
        "    if os.path.isfile(filename):\n",
        "      print(filename)\n",
        "\n",
        "  print(\"\\nALL DONE\\n\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "**Helpful hints**\n",
        "\n",
        "* To upload\n",
        "a file with a jobname and a sequence on each line, \n",
        "check ***upload_file_with_jobname_space_sequence_lines*** and hit\n",
        "the ***Run*** button to the left of the first cell. You will be asked to upload any template files one at a time or\n",
        "all at once (file names must start with jobnames if all at once)\n",
        "\n",
        "* To start over, check ***clear_saved_sequences_and_jobnames***.\n",
        "\n",
        "* You can encourage AlphaFold to use your uploaded template by specifying skip_all_msa. This will just use your template information and intrinsic structural information in AlphaFold.\n",
        "\n",
        "* You can step through this notebook one part at a time\n",
        "by hitting the ***Run*** buttons to the left one at a time. \n",
        "\n",
        "* The cell that is active is indicated by a ***Run*** button that has turned into a black circle with a moving black arc\n",
        "\n",
        "* When execution is done, the ***Run*** button will go back \n",
        "to its original white triangle inside a black circle\n",
        "\n",
        "* You can stop execution of the active cell by hitting its ***Run*** button. It will turn red to indicate it has stopped.\n",
        "\n",
        "* You can rerun any cell any time that nothing is running.  That means you can go all the way through, then go back to the first cell and enter another sequence and redo the procedure.\n",
        "\n",
        "* If something goes wrong, the Colab Notebook will print out\n",
        "an error message.  Usually this will be something telling you\n",
        "how to change your inputs.  You enter your new inputs and\n",
        "hit the ***Run*** button again to carry on.\n",
        "\n",
        "* The automatic download may not always work. Normally the\n",
        "file download starts when the .zip files are created,\n",
        "but the actual download happens when all the AlphaFold\n",
        "models are completed.\n",
        "You can click on the \n",
        "folder icon to the left of the window and download your\n",
        "jobname.zip file manually.  Open and close the file\n",
        "broswer to show recently-added files.\n",
        "\n",
        "* Your Colab connection may time out if you go away and\n",
        "leave it, or if you run for a long time (12 hr).\n",
        "If your connection times out you lose everything that\n",
        "is not yet downloaded. So you might want to download as you go\n",
        "if you are running multiple sequences, or else run in batches\n",
        "that are not too large (perhaps up to 10 at a time).\n",
        "\n",
        "\n",
        "* The zip file or files will usually not be automatically downloaded until the very end of the job. If you are running multiple sequences you might want to manually download results as they appear so they are safe.\n",
        "* Your sequence should contain only the 1-letter code of one protein chain. It can contain spaces if you want.\n",
        "* If you upload a file with multiple sequences, each line of the file should have exactly one job name, a space, and a sequence, like this:\n",
        "\n",
        "7n8i_24237 VIWMTQSPSSLSASVGDRVTITCQASQDIRFYLNWYQQKPGKAPKLLISDASNMETGVPSRFSGS\n",
        "\n",
        "7lvr_23541 MRECISIHVGQAGVQIGNACWELYCLEHGIQPDGQMPSDKTIGGGDDSFNTFFSETG\n",
        "\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.  You can use the button at the bottom of this notebook to deliberately crash the notebook which sometimes will result in Colab offering you up to double your initial memory allocation.\n",
        "* Your browser can block the pop-up for downloading the result file. You can instead manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structure\n",
        "2. Plot of the model quality (IDDT).\n",
        "3. Plots of the MSA coverage.\n",
        "4. A3M formatted input MSA.\n",
        "5. BibTeX file with citations for all used tools and databases.\n",
        "6. JSN file with predicted error matrix (PAE matrix)\n",
        "\n",
        "At the end of the job the `jobname.result.zip` file or files will be downloaded automatically.\n",
        "\n",
        "\n",
        "**Colab limitations**\n",
        "* While Colab is free, it is designed for interactive work and not-unlimited memory and GPU usage. It will time-out after a few hours and it may check that you are not a robot at random times.  On a time-out you may lose your work. You can increase your allowed time with Colab+\n",
        "\n",
        "* AlphaFold can crash if it requires too much memory. On a crash you may lose all your work that is not yet downloaded. You can have more memory accessible if you have Colab+. If you are familiar with Colab scripts you can try this [hack](https://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-efa636e646ff ) to increase your memory allowance.\n",
        "\n",
        "\n",
        "**Description of the plots**\n",
        "\n",
        "*   **Number of sequences per position** - Look for at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Updates**\n",
        "\n",
        "- <b> <font color='green'>2021-11-12 Clarified use of PDB templates and allow skipping MSA information\n",
        "\n",
        "\n",
        "**Acknowledgments**\n",
        "\n",
        "- <b> <font color='green'>This notebook is based on the very nice notebook from ColabFold ([Mirdita et al., *bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1), https://github.com/sokrypton/ColabFold)</font></b> \n",
        "\n",
        "- <b><font color='green'>ColabFold is based on AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font></b>"
      ]
    }
  ]
}