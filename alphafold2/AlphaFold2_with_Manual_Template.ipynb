{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold2_with_Manual_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFold2_with_Manual_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "#AlphaFold2 with a Template\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOblAo-xetgx",
        "cellView": "form"
      },
      "source": [
        "#@title Protein sequence\n",
        "from google.colab import files\n",
        "import os\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "query_sequence = '' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# remove whitespaces\n",
        "query_sequence = \"\".join(query_sequence.split())\n",
        "query_sequence = re.sub(r'[^a-zA-Z]','', query_sequence).upper()\n",
        "\n",
        "jobname = 'af_with_template' \n",
        "# remove whitespaces\n",
        "jobname = \"\".join(jobname.split())\n",
        "jobname = re.sub(r'\\W+', '', jobname)\n",
        "jobname = add_hash(jobname, query_sequence)\n",
        "\n",
        "\n",
        "with open(f\"{jobname}.fasta\", \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)\n",
        "\n",
        "\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" \n",
        "num_models = 1 \n",
        "use_msa = True if msa_mode.startswith(\"MMseqs2\") else False\n",
        "use_env = True if msa_mode == \"MMseqs2 (UniRef+Environmental)\" else False\n",
        "use_custom_msa = True if msa_mode == \"custom\" else False\n",
        "use_amber = False \n",
        "use_templates = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "homooligomer = 1\n",
        "save_to_google_drive = False\n",
        "\n",
        "\n",
        "if homooligomer > 1:\n",
        "  if use_amber:\n",
        "    print(\"amber disabled: amber is not currently supported for homooligomers\")\n",
        "    use_amber = False\n",
        "  if use_templates:\n",
        "    print(\"templates disabled: templates are not currently supported for homooligomers\")\n",
        "    use_templates = False\n",
        "\n",
        "with open(f\"{jobname}.log\", \"w\") as text_file:\n",
        "    text_file.write(\"num_models=%s\\n\" % num_models)\n",
        "    text_file.write(\"use_amber=%s\\n\" % use_amber)\n",
        "    text_file.write(\"use_msa=%s\\n\" % use_msa)\n",
        "    text_file.write(\"msa_mode=%s\\n\" % msa_mode)\n",
        "    text_file.write(\"use_templates=%s\\n\" % use_templates)\n",
        "    text_file.write(\"homooligomer=%s\\n\" % homooligomer)\n",
        "\n",
        "# decide which a3m to use\n",
        "if use_msa:\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "elif use_custom_msa:\n",
        "  a3m_file = f\"{jobname}.custom.a3m\"\n",
        "  if not os.path.isfile(a3m_file):\n",
        "    custom_msa_dict = files.upload()\n",
        "    custom_msa = list(custom_msa_dict.keys())[0]\n",
        "    header = 0\n",
        "    import fileinput\n",
        "    for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "      if line.startswith(\">\"):\n",
        "         header = header + 1 \n",
        "      if line.startswith(\"#\"):\n",
        "        continue\n",
        "      if line.rstrip() == False:\n",
        "        continue\n",
        "      if line.startswith(\">\") == False and header == 1:\n",
        "         query_sequence = line.rstrip() \n",
        "      print(line, end='')\n",
        "\n",
        "    os.rename(custom_msa, a3m_file)\n",
        "    print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "else:\n",
        "  a3m_file = f\"{jobname}.single_sequence.a3m\"\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)\n",
        "\n",
        "if save_to_google_drive == True:\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"You are logged into Google Drive and are good to go!\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn1r2dn6P2uq"
      },
      "source": [
        "-----------------\n",
        "<b> HOW TO GET YOUR ALPHAFOLD MODEL USING A TEMPLATE</b>\n",
        "- <b><font color='green'>\n",
        "Paste in your sequence above in the box labelled \"query_sequence\"\n",
        "</font></b>\n",
        "\n",
        "- <b><font color='green'>\n",
        "At the top of the window select \"Runtime\" and \"Run all\"\n",
        "</font></b>\n",
        "\n",
        "- <font color='black'>\n",
        "... Wait a minute or two for the \"Choose files\" button to appear below in a box labelled \"Upload your .cif format template model here when button appears\"\n",
        "</font>\n",
        "\n",
        "- <b><font color='green'>\n",
        "Upload your template (preliminary model). Must be an mmCIF file. To convert from pdb format use this converter: https://mmcif.pdbj.org/converter/\n",
        "</font></b>\n",
        "\n",
        "- <font color='black'>\n",
        "... Wait a few more minutes for your result\n",
        "</font>\n",
        "\n",
        "- <b><font color='green'>\n",
        "Your model information should download automatically as a zip file (it may ask for permission)\n",
        "\n",
        "</font></b>\n",
        "-----------------\n",
        "-----------------\n",
        "<b> <font color='black'> Please cite the following papers if you use this interface to AlphaFold2:\n",
        "</font></b> \n",
        "\n",
        "- <b> <font color='green'>The template is derived from ColabFold ([Mirdita et al., *bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1), https://github.com/sokrypton/ColabFold)</font></b> \n",
        "\n",
        "- <b><font color='green'>ColabFold is based on AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font></b>\n",
        "-----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "#@title Installing dependencies...\n",
        "%%bash -s $use_amber $use_msa $use_templates\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_MSA=$2\n",
        "USE_TEMPLATES=$3\n",
        "\n",
        "if [ ! -f AF2_READY ]; then\n",
        "  # install dependencies\n",
        "  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "  wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "\n",
        "  # download model\n",
        "  if [ ! -d \"alphafold/\" ]; then\n",
        "    git clone https://github.com/deepmind/alphafold.git --quiet\n",
        "    (cd alphafold; git checkout 0bab1bf84d9d887aba5cfb6d09af1e8c3ecbc408 --quiet)\n",
        "    mv alphafold alphafold_\n",
        "    mv alphafold_/alphafold .\n",
        "    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "  fi\n",
        "\n",
        "  # download model params (~1 min)\n",
        "  if [ ! -d \"params/\" ]; then\n",
        "    mkdir params\n",
        "    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \\\n",
        "    | tar x -C params\n",
        "  fi\n",
        "  touch AF2_READY\n",
        "fi\n",
        "# download libraries for interfacing with MMseqs2 API\n",
        "if [ ${USE_MSA} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f MMSEQ2_READY ]; then\n",
        "    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "    touch MMSEQ2_READY\n",
        "  fi\n",
        "fi\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
        "  (cd /usr/local/lib/python3.7/site-packages; patch -s -p0 < /content/alphafold_/docker/openmm.patch)\n",
        "  wget -qnc https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "  mv stereo_chemical_props.txt alphafold/common/\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFNCnjI9_DUG",
        "cellView": "form"
      },
      "source": [
        "#@title Importing libraries...\n",
        "# setup the model\n",
        "if \"model\" not in dir():\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  import colabfold as cf\n",
        "\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "if use_amber and \"relax\" not in dir():\n",
        "  sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
        "  from alphafold.relax import relax\n",
        "\n",
        "def mk_mock_template(query_sequence):\n",
        "  # since alphafold's model requires a template input\n",
        "  # we create a blank example w/ zero input, confidence -1\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"-\"*ln\n",
        "  output_confidence_scores = np.full(ln,-1)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': templates_all_atom_positions[None],\n",
        "                       'template_all_atom_masks': templates_all_atom_masks[None],\n",
        "                       'template_sequence': [f'none'.encode()],\n",
        "                       'template_aatype': np.array(templates_aatype)[None],\n",
        "                       'template_confidence_scores': output_confidence_scores[None],\n",
        "                       'template_domain_names': [f'none'.encode()],\n",
        "                       'template_release_date': [f'none'.encode()]}\n",
        "  return template_features\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, model_params, use_model, do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  L_prev = 0\n",
        "  # Ls: number of residues in each chain\n",
        "  for L_i in Ls[:-1]:\n",
        "      idx_res[L_prev+L_i:] += 200\n",
        "      L_prev += L_i  \n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "  feature_dict['residue_index'] = idx_res\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "      \n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      prediction_result = model_runner.predict(processed_feature_dict)\n",
        "      unrelaxed_protein = protein.from_prediction(processed_feature_dict,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'])\n",
        "      paes.append(prediction_result['predicted_aligned_error'])\n",
        "\n",
        "      if do_relax:\n",
        "        # Relax the prediction.\n",
        "        amber_relaxer = relax.AmberRelaxation(max_iterations=0,tolerance=2.39,\n",
        "                                              stiffness=10.0,exclude_residues=[],\n",
        "                                              max_outer_iterations=20)      \n",
        "        relaxed_pdb_str, _, _ = amber_relaxer.process(prot=unrelaxed_protein)\n",
        "        relaxed_pdb_lines.append(relaxed_pdb_str)\n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  print(\"reranking models based on avg. predicted lDDT\")\n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    if do_relax:\n",
        "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
        "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
        "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sztQyz29DIC",
        "cellView": "form"
      },
      "source": [
        "#@title Getting MSA/templates...\n",
        "if use_templates:\n",
        "  a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "  if template_paths is None:\n",
        "    template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "  else:\n",
        "    template_features = mk_template(a3m_lines, template_paths)\n",
        "elif use_msa:\n",
        "  a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "  template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "else:\n",
        "  template_features = mk_mock_template(query_sequence * homooligomer)\n",
        "\n",
        "if use_msa:\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(a3m_lines)\n",
        "else:\n",
        "  a3m_lines = \"\".join(open(a3m_file,\"r\").read())\n",
        "\n",
        "# parse MSA\n",
        "msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTBwpd8LmY8Y",
        "cellView": "form"
      },
      "source": [
        "\n",
        "#@title *Upload your .cif format template model here when button appears*\n",
        "supply_manual_templates = True\n",
        "import os\n",
        "from io import StringIO\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from dataclasses import dataclass, replace\n",
        "\n",
        "from alphafold.data import mmcif_parsing\n",
        "from alphafold.data.templates import (_get_pdb_id_and_chain,\n",
        "                                      _process_single_hit,\n",
        "                                      _assess_hhsearch_hit,\n",
        "                                      _build_query_to_hit_index_mapping,\n",
        "                                      _extract_template_features,\n",
        "                                      SingleHitResult,\n",
        "                                      TEMPLATE_FEATURES)\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def hh_process_seq(query_seq,template_seq,hhDB_dir,db_prefix=\"DB\"):\n",
        "  \"\"\"\n",
        "  This is a hack to get hhsuite output strings to pass on\n",
        "  to the AlphaFold template featurizer. \n",
        "  \n",
        "  Note: that in the case of multiple templates, this would be faster to build one database for\n",
        "  all the templates. Currently it builds a database with only one template at a time. Even \n",
        "  better would be to get an hhsuite alignment without using a database at all, just between\n",
        "  pairs of sequence files. However, I have not figured out how to do this.\n",
        "\n",
        "  Update: I think the hhsearch can be replaced completely, and we can just do a pairwise \n",
        "  alignment with biopython, or skip alignment if the seqs match. TODO\n",
        "  \"\"\"\n",
        "  # set up directory for hhsuite DB. Place one template fasta file to be the DB contents\n",
        "  if hhDB_dir.exists():\n",
        "    shutil.rmtree(hhDB_dir)\n",
        "  \n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  msa_dir.mkdir(parents=True)\n",
        "  template_seq_path = Path(msa_dir,\"template.fasta\")\n",
        "  with template_seq_path.open(\"w\") as fh:\n",
        "    SeqIO.write([template_seq], fh, \"fasta\")\n",
        "\n",
        "  # make hhsuite DB\n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    os.chdir(msa_dir)\n",
        "    %shell ffindex_build -s ../DB_msa.ff{data,index} .\n",
        "    os.chdir(hhDB_dir)\n",
        "    %shell ffindex_apply DB_msa.ff{data,index}  -i DB_a3m.ffindex -d DB_a3m.ffdata  -- hhconsensus -M 50 -maxres 65535 -i stdin -oa3m stdout -v 0\n",
        "    %shell rm DB_msa.ff{data,index}\n",
        "    %shell ffindex_apply DB_a3m.ff{data,index} -i DB_hhm.ffindex -d DB_hhm.ffdata -- hhmake -i stdin -o stdout -v 0\n",
        "    %shell cstranslate -f -x 0.3 -c 4 -I a3m -i DB_a3m -o DB_cs219 \n",
        "    %shell sort -k3 -n -r DB_cs219.ffindex | cut -f1 > sorting.dat\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_hhm.ff{data,index} DB_hhm_ordered.ff{data,index}\n",
        "    %shell mv DB_hhm_ordered.ffindex DB_hhm.ffindex\n",
        "    %shell mv DB_hhm_ordered.ffdata DB_hhm.ffdata\n",
        "\n",
        "    %shell ffindex_order sorting.dat DB_a3m.ff{data,index} DB_a3m_ordered.ff{data,index}\n",
        "    %shell mv DB_a3m_ordered.ffindex DB_a3m.ffindex\n",
        "    %shell mv DB_a3m_ordered.ffdata DB_a3m.ffdata\n",
        "\n",
        "  # run hhsearch\n",
        "  hhsearch_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[hhDB_dir.as_posix()+\"/\"+db_prefix])\n",
        "  with StringIO() as fh:\n",
        "    SeqIO.write([query_seq], fh, \"fasta\")\n",
        "    seq_fasta = fh.getvalue()\n",
        "  hhsearch_result = hhsearch_runner.query(seq_fasta)\n",
        "\n",
        "  # process hits\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  if len(hhsearch_hits) >0:\n",
        "    hit = hhsearch_hits[0]\n",
        "    hit = replace(hit,**{\"name\":template_seq.id})\n",
        "  else:\n",
        "    hit = None\n",
        "    print(\"ERROR: Rejected template: \",template_seq.id)\n",
        "  return hit\n",
        "\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "if not supply_manual_templates:\n",
        "  print(\"Not using manual templates.\")\n",
        "else:\n",
        "\n",
        "  parent_dir = Path(\"/content/manual_templates\")\n",
        "  cif_dir = Path(parent_dir,\"mmcif\")\n",
        "  fasta_dir = Path(parent_dir,\"fasta\")\n",
        "  hhDB_dir = Path(parent_dir,\"hhDB\")\n",
        "  msa_dir = Path(hhDB_dir,\"msa\")\n",
        "  all_dirs = [parent_dir,cif_dir,fasta_dir,hhDB_dir,msa_dir]\n",
        "  for d in all_dirs:\n",
        "    if d.exists():\n",
        "      shutil.rmtree(d)\n",
        "    d.mkdir(parents=True)\n",
        "  \n",
        "  with redirect_stdout(StringIO()) as out:\n",
        "    uploaded = files.upload()\n",
        "    for filename,contents in uploaded.items():\n",
        "      filepath = Path(cif_dir,filename)\n",
        "      with filepath.open(\"w\") as fh:\n",
        "        fh.write(contents.decode(\"UTF-8\"))\n",
        "\n",
        "\n",
        "\n",
        "  cif_files = list(cif_dir.glob(\"*\"))\n",
        "  query_seq = SeqRecord(Seq(query_sequence),id=\"query\",name=\"\",description=\"\")\n",
        "  query_seq_path = Path(fasta_dir,\"query.fasta\")\n",
        "  with query_seq_path.open(\"w\") as fh:\n",
        "      SeqIO.write([query_seq], fh, \"fasta\")\n",
        "\n",
        "  shutil.copyfile(query_seq_path,Path(msa_dir,\"query.fasta\"))\n",
        "  seqs = []\n",
        "  template_hit_list = []\n",
        "\n",
        "  print(\"\\nProcessing templates...\")\n",
        "  for i,filepath in enumerate(cif_files):\n",
        "    with filepath.open(\"r\") as fh:\n",
        "      filestr = fh.read()\n",
        "      mmcif_obj = mmcif_parsing.parse(file_id=filepath.stem,mmcif_string=filestr)\n",
        "      mmcif = mmcif_obj.mmcif_object\n",
        "\n",
        "      for chain_id,template_sequence in mmcif.chain_to_seqres.items():\n",
        "        template_sequence = mmcif.chain_to_seqres[chain_id]\n",
        "        seq_name = filepath.stem.upper()+\"_\"+chain_id\n",
        "        seq = SeqRecord(Seq(template_sequence),id=seq_name,name=\"\",description=\"\")\n",
        "        seqs.append(seq)\n",
        "\n",
        "        with  Path(fasta_dir,seq.id+\".fasta\").open(\"w\") as fh:\n",
        "          SeqIO.write([seq], fh, \"fasta\")\n",
        "\n",
        "        \"\"\"\n",
        "        At this stage, we have a template sequence.\n",
        "        and a query sequence. \n",
        "        There are two options to generate template features:\n",
        "          1. Write new code to manually generate template features\n",
        "          2. Get an hhr alignment string, and pass that\n",
        "            to the existing template featurizer. \n",
        "            \n",
        "        I chose the second, implemented in hh_process_seq()\n",
        "        \"\"\"\n",
        "\n",
        "        hit = hh_process_seq(query_seq,seq,hhDB_dir)\n",
        "        if hit is not None:\n",
        "          template_hit_list.append(hit)\n",
        "\n",
        "  #process hits into template features\n",
        "  template_hit_list = [replace(hit,**{\"index\":i+1}) for i,hit in enumerate(template_hit_list)]\n",
        "  template_features = {}\n",
        "  for template_feature_name in TEMPLATE_FEATURES:\n",
        "    template_features[template_feature_name] = []\n",
        "\n",
        "  for i,hit in enumerate(sorted(template_hit_list, key=lambda x: x.sum_probs, reverse=True)):\n",
        "    # modifications to alphafold/data/templates.py _process_single_hit\n",
        "    hit_pdb_code, hit_chain_id = _get_pdb_id_and_chain(hit)\n",
        "    mapping = _build_query_to_hit_index_mapping(\n",
        "    hit.query, hit.hit_sequence, hit.indices_hit, hit.indices_query,\n",
        "    query_sequence)\n",
        "    template_sequence = hit.hit_sequence.replace('-', '')\n",
        "\n",
        "    features, realign_warning = _extract_template_features(\n",
        "      mmcif_object=mmcif,\n",
        "      pdb_id=hit_pdb_code,\n",
        "      mapping=mapping,\n",
        "      template_sequence=template_sequence,\n",
        "      query_sequence=query_sequence,\n",
        "      template_chain_id=hit_chain_id,\n",
        "      kalign_binary_path=\"kalign\")\n",
        "    features['template_sum_probs'] = [hit.sum_probs]\n",
        "\n",
        "    single_hit_result = SingleHitResult(features=features, error=None, warning=None)\n",
        "    for k in template_features:\n",
        "      template_features[k].append(features[k])\n",
        "\n",
        "\n",
        "  for name in template_features:\n",
        "    template_features[name] = np.stack(\n",
        "        template_features[name], axis=0).astype(TEMPLATE_FEATURES[name])\n",
        "    \n",
        "    \n",
        "    \n",
        "  #overwrite template data\n",
        "  template_paths = cif_dir.as_posix()\n",
        "  template_hits = template_hit_list\n",
        "  print(\"\\nIncluding templates:\")\n",
        "  for hit in template_hit_list:\n",
        "    print(\"\\t\",hit.name.split()[0])\n",
        "  os.chdir(\"/content/\")\n",
        "\n",
        "  for key,value in template_features.items():\n",
        "    if np.all(value==0):\n",
        "      print(\"ERROR: Some template features are empty\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYApPElB30u",
        "cellView": "form"
      },
      "source": [
        "#@title Predicting structure...\n",
        "from string import ascii_uppercase\n",
        "\n",
        "# collect model weights\n",
        "use_model = {}\n",
        "if \"model_params\" not in dir(): model_params = {}\n",
        "for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "  use_model[model_name] = True\n",
        "  if model_name not in model_params:\n",
        "    model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "    if model_name == \"model_1\":\n",
        "      model_config = config.model_config(model_name+\"_ptm\")\n",
        "      model_config.data.eval.num_ensemble = 1\n",
        "      model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "    if model_name == \"model_3\":\n",
        "      model_config = config.model_config(model_name+\"_ptm\")\n",
        "      model_config.data.eval.num_ensemble = 1\n",
        "      model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "\n",
        "if homooligomer == 1:\n",
        "  msas = [msa]\n",
        "  deletion_matrices = [deletion_matrix]\n",
        "else:\n",
        "  # make multiple copies of msa for each copy\n",
        "  # AAA------\n",
        "  # ---AAA---\n",
        "  # ------AAA\n",
        "  #\n",
        "  # note: if you concat the sequences (as below), it does NOT work\n",
        "  # AAAAAAAAA\n",
        "  msas = []\n",
        "  deletion_matrices = []\n",
        "  Ln = len(query_sequence)\n",
        "  for o in range(homooligomer):\n",
        "    L = Ln * o\n",
        "    R = Ln * (homooligomer-(o+1))\n",
        "    msas.append([\"-\"*L+seq+\"-\"*R for seq in msa])\n",
        "    deletion_matrices.append([[0]*L+mtx+[0]*R for mtx in deletion_matrix])\n",
        "\n",
        "# gather features\n",
        "feature_dict = {\n",
        "    **pipeline.make_sequence_features(sequence=query_sequence*homooligomer,\n",
        "                                      description=\"none\",\n",
        "                                      num_res=len(query_sequence)*homooligomer),\n",
        "    **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "    **template_features\n",
        "}\n",
        "outs = predict_structure(jobname, feature_dict,\n",
        "                         Ls=[len(query_sequence)]*homooligomer,\n",
        "                         model_params=model_params, use_model=use_model,\n",
        "                         do_relax=use_amber)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xbvRNrwnJqj",
        "cellView": "form"
      },
      "source": [
        "#@title Making plots...\n",
        "\n",
        "# gather MSA info\n",
        "deduped_full_msa = list(dict.fromkeys(msa))\n",
        "msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "seqid_sort = seqid.argsort() #[::-1]\n",
        "non_gaps = (msa_arr != \"-\").astype(float)\n",
        "non_gaps[non_gaps == 0] = np.nan\n",
        "\n",
        "##################################################################\n",
        "plt.figure(figsize=(14,4),dpi=100)\n",
        "##################################################################\n",
        "plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "           interpolation='nearest', aspect='auto',\n",
        "           cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "plt.colorbar(label=\"Sequence identity to query\",)\n",
        "plt.xlabel(\"Positions\")\n",
        "plt.ylabel(\"Sequences\")\n",
        "\n",
        "##################################################################\n",
        "plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "for model_name,value in outs.items():\n",
        "  plt.plot(value[\"plddt\"],label=model_name)\n",
        "if homooligomer > 0:\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence)-1)\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "plt.legend()\n",
        "plt.ylim(0,100)\n",
        "plt.ylabel(\"Predicted lDDT\")\n",
        "plt.xlabel(\"Positions\")\n",
        "plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "##################################################################\n",
        "plt.show()\n",
        "\n",
        "print(\"Predicted Alignment Error\")\n",
        "##################################################################\n",
        "plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "for n,(model_name,value) in enumerate(outs.items()):\n",
        "  plt.subplot(1,num_models,n+1)\n",
        "  plt.title(model_name)\n",
        "  plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "plt.savefig(jobname+\"_PAE.png\")\n",
        "plt.show()\n",
        "##################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK7X9T44pWb7",
        "cellView": "form"
      },
      "source": [
        "#@title Displaying 3D structure... {run: \"auto\"}\n",
        "model_num = 1 \n",
        "color = \"lDDT\" \n",
        "show_sidechains = False \n",
        "show_mainchains = False \n",
        "\n",
        "def plot_plddt_legend():\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=100)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "def plot_confidence(model_num=1):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  plt.figure(figsize=(10,3),dpi=100)\n",
        "  \"\"\"Plots the legend for plDDT.\"\"\"\n",
        "  #########################################\n",
        "  plt.subplot(1,2,1); plt.title('Predicted lDDT')\n",
        "  plt.plot(outs[model_name][\"plddt\"])\n",
        "  for n in range(homooligomer+1):\n",
        "    x = n*(len(query_sequence))\n",
        "    plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  #########################################\n",
        "  plt.subplot(1,2,2);plt.title('Predicted Aligned Error')\n",
        "  plt.imshow(outs[model_name][\"pae\"], cmap=\"bwr\",vmin=0,vmax=30)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "  #########################################\n",
        "  return plt\n",
        "\n",
        "def show_pdb(model_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
        "  model_name = f\"model_{model_num}\"\n",
        "  if use_amber:\n",
        "    pdb_filename = f\"{jobname}_relaxed_{model_name}.pdb\"\n",
        "  else:\n",
        "    pdb_filename = f\"{jobname}_unrelaxed_{model_name}.pdb\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(pdb_filename,'r').read(),'pdb')\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(homooligomer),list(\"ABCDEFGH\"),\n",
        "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "show_pdb(model_num,show_sidechains, show_mainchains, color).show()\n",
        "if color == \"lDDT\": plot_plddt_legend().show()  \n",
        "plot_confidence(model_num).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33g5IIegij5R",
        "cellView": "form"
      },
      "source": [
        "#@title Packaging and downloading results...\n",
        "#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "citations = {\n",
        "\"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "doi = {10.1101/2021.08.15.456425},\n",
        "journal = {bioRxiv},\n",
        "title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "year = {2021},\n",
        "comment = {ColabFold including MMseqs2 MSA server}\n",
        "}\"\"\",\n",
        "  \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "doi = {10.1093/nar/gkz1035},\n",
        "journal = {Nucleic Acids Res.},\n",
        "title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "year = {2019},\n",
        "comment = {MGnify database}\n",
        "}\"\"\",\n",
        "  \"Eastman2017\": \"\"\"@article{Eastman2017,\n",
        "author = {Eastman, Peter and Swails, Jason and Chodera, John D. and McGibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and Wiewiora, Rafal P. and Brooks, Bernard R. and Pande, Vijay S.},\n",
        "doi = {10.1371/journal.pcbi.1005659},\n",
        "journal = {PLOS Comput. Biol.},\n",
        "number = {7},\n",
        "title = {{OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}},\n",
        "volume = {13},\n",
        "year = {2017},\n",
        "comment = {Amber relaxation}\n",
        "}\"\"\",\n",
        "  \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "doi = {10.1038/s41586-021-03819-2},\n",
        "journal = {Nature},\n",
        "pmid = {34265844},\n",
        "title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "year = {2021},\n",
        "comment = {AlphaFold2 + BFD Database}\n",
        "}\"\"\",\n",
        "  \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "doi = {10.1093/bioinformatics/bty1057},\n",
        "journal = {Bioinformatics},\n",
        "number = {16},\n",
        "pages = {2856--2858},\n",
        "pmid = {30615063},\n",
        "title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "volume = {35},\n",
        "year = {2019},\n",
        "comment = {MMseqs2 search server}\n",
        "}\"\"\",\n",
        "  \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "doi = {10.1186/s12859-019-3019-7},\n",
        "journal = {BMC Bioinform.},\n",
        "number = {1},\n",
        "pages = {473},\n",
        "pmid = {31521110},\n",
        "title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "volume = {20},\n",
        "year = {2019},\n",
        "comment = {PDB70 database}\n",
        "}\"\"\",\n",
        "  \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "doi = {10.1093/nar/gkw1081},\n",
        "journal = {Nucleic Acids Res.},\n",
        "number = {D1},\n",
        "pages = {D170--D176},\n",
        "pmid = {27899574},\n",
        "title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "volume = {45},\n",
        "year = {2017},\n",
        "comment = {Uniclust30/UniRef30 database},\n",
        "}\"\"\",\n",
        "  \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "booktitle = {Nat. Struct. Biol.},\n",
        "doi = {10.1038/nsb1203-980},\n",
        "number = {12},\n",
        "pages = {980},\n",
        "pmid = {14634627},\n",
        "title = {{Announcing the worldwide Protein Data Bank}},\n",
        "volume = {10},\n",
        "year = {2003},\n",
        "comment = {templates downloaded from wwPDB server}\n",
        "}\"\"\",\n",
        "}\n",
        "\n",
        "to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "if use_templates: to_cite += [\"Berman2003\"]\n",
        "if use_amber:     to_cite += [\"Eastman2017\"]\n",
        "\n",
        "with open(f\"{jobname}.bibtex\", 'w') as writer:\n",
        "  for i in to_cite:\n",
        "    writer.write(citations[i])\n",
        "    writer.write(\"\\n\")\n",
        "\n",
        "print(f\"Found {len(to_cite)} citation{'s' if len(to_cite) > 1 else ''} for tools or databases.\")\n",
        "if use_custom_msa:\n",
        "  print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "!zip -FSr $jobname\".result.zip\" $jobname\".log\" $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\".bibtex\" $jobname\"_PAE.png\"\n",
        "files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "if save_to_google_drive == True and drive != None:\n",
        "  uploaded = drive.CreateFile({'title': f\"{jobname}.result.zip\"})\n",
        "  uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "  uploaded.Upload()\n",
        "  print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq2HJBe6vdBU",
        "cellView": "form"
      },
      "source": [
        "#@title Downloading any templates...\n",
        "\n",
        "from alphafold.data.templates import _get_pdb_id_and_chain\n",
        "from pathlib import Path\n",
        "\n",
        "if not use_templates:\n",
        "  print(\"No templates used\")\n",
        "else:\n",
        "  cif_files = [f for f in Path(template_paths).glob(\"*\") if f.suffix in [\".cif\",\".mmcif\"]]\n",
        "  cif_files_used = []\n",
        "  for hit in template_hits:\n",
        "    code,chain_id = _get_pdb_id_and_chain(hit)\n",
        "    for f in cif_files:\n",
        "      if code in f.name or code.upper() in f.name:\n",
        "        cif_files_used.append(f)\n",
        "\n",
        "\n",
        "  zip_string = \" \".join([cif_file.as_posix() for cif_file in cif_files_used])\n",
        "\n",
        "  !zip -FSrj $jobname\".templates.zip\" $zip_string\n",
        "  files.download(f\"{jobname}.templates.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "# NOTES\n",
        "**Quick start**\n",
        "1. Paste your protein sequence in the input field.\n",
        "2. Press \"Runtime\" -> \"Run all\".\n",
        "3. Upload your template when the upload buttom appears\n",
        "3. The pipeline consists of 15 steps. The currently running steps is indicated by a grey  circle with a moving black arc and a white square inside.\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pIDDT. (unrelaxed and relaxed if `use_amber` is enabled).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "At the end of the job a download modal box will pop up with a `jobname.result.zip` file. Additionally, if the `save_to_google_drive` option was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
        "\n",
        "\n",
        "\n",
        "**Troubleshooting**\n",
        "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Description of the plots**\n",
        "\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "\n",
        "**Acknowledgments**\n",
        "\n",
        "- <b> <font color='green'>The template is derived from ColabFold ([Mirdita et al., *bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1), https://github.com/sokrypton/ColabFold)</font></b> \n",
        "\n",
        "- <b><font color='green'>ColabFold is based on AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2)\n",
        "</font></b>\n",
        "\n",
        "- <b> From the ColabFold site: </b>\n",
        "\n",
        "- [Söding Lab](https://www.mpibpc.mpg.de/soeding) for providing the computational resources for the MMseqs2 server\n",
        "\n",
        "- Minkyung Baek ([@minkbaek](https://twitter.com/minkbaek)) and Yoshitaka Moriwaki ([@Ag_smith](https://twitter.com/Ag_smith)) for protein-complex prediction proof-of-concept in AlphaFold2.\n",
        "\n",
        "- [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin, without whom these notebooks would be quite boring!\n",
        "\n",
        "- Do-Yoon Kim for creating the ColabFold logo.\n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
      ]
    }
  ]
}
